{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10章 隐马尔可夫模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1．隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态的序列，再由各个状态随机生成一个观测而产生观测的序列的过程。\n",
    "\n",
    "隐马尔可夫模型由初始状态概率向$\\pi$、状态转移概率矩阵$A$和观测概率矩阵$B$决定。因此，隐马尔可夫模型可以写成$\\lambda=(A, B, \\pi)$。\n",
    "\n",
    "隐马尔可夫模型是一个生成模型，表示状态序列和观测序列的联合分布，但是状态序列是隐藏的，不可观测的。\n",
    "\n",
    "隐马尔可夫模型可以用于标注，这时状态对应着标记。标注问题是给定观测序列预测其对应的标记序列。\n",
    "\n",
    "2．概率计算问题。给定模型$\\lambda=(A, B, \\pi)$和观测序列$O＝(o_1，o_2,…,o_T)$，计算在模型$\\lambda$下观测序列$O$出现的概率$P(O|\\lambda)$。前向-后向算法是通过递推地计算前向-后向概率可以高效地进行隐马尔可夫模型的概率计算。\n",
    " \n",
    "3．学习问题。已知观测序列$O＝(o_1，o_2,…,o_T)$，估计模型$\\lambda=(A, B, \\pi)$参数，使得在该模型下观测序列概率$P(O|\\lambda)$最大。即用极大似然估计的方法估计参数。Baum-Welch算法，也就是EM算法可以高效地对隐马尔可夫模型进行训练。它是一种非监督学习算法。\n",
    "\n",
    "4．预测问题。已知模型$\\lambda=(A, B, \\pi)$和观测序列$O＝(o_1，o_2,…,o_T)$，求对给定观测序列条件概率$P(I|O)$最大的状态序列$I＝(i_1，i_2,…,i_T)$。维特比算法应用动态规划高效地求解最优路径，即概率最大的状态序列。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 马尔可夫链\n",
    "## 马尔可夫(Markov)过程及其概率分布\n",
    "**马尔可夫性**或**无后效性**: 过程(或系统)在时刻$t_0$所处状态为已知条件下, 过程在时刻$t>t_0$所处状态的条件分布与过程在时刻$t_0$之前所处的状态无关. 通俗地说, 就是在已知过程\"现在\"的条件下, 其\"将来\"不依赖于\"过去\".\n",
    "\n",
    "设随机过程$\\{X(t), t \\in T\\}$的状态空间为$I$. 如果对时间$t$的任意$n$个数值$t_1<t_2< \\cdots< t_n, n\\geq3, t_i \\in T $, 在条件$X(t_i)=x_i, x_i \\in I, i=1,2, \\cdots, n-1$下, $X(t_n)$的条件分布函数恰等于在条件$X(t_{n-1})=x_{n-1}$下$X(t_n)$的条件分布函数, 即\n",
    "$$\n",
    "P\\{X(t_n) \\leq x_n| X(t_1)=x_1, X(t_2)=x_2, \\cdots, X(t_{n-1})=x_{n-1}\\} \\\\\n",
    "= P\\{X(t_{n}) \\leq x_{n}|X(t_{n-1})=x_{n-1}\\}, x_n \\in \\mathcal R\n",
    "$$\n",
    "或写成\n",
    "$$F_{t_n|t_1\\cdots t_{n-1}}(x_n,t_n|x_1, x_2, \\cdots, x_{n-1};t_1, t_2, \\cdots, t_{n-1}) \n",
    "= F_{t_n|t_{n-1}}(x_n, t_n|x_{n-1},t_{n-1})\n",
    "$$\n",
    "则称过程$\\{X(t), t\\in T\\}$具有马尔可夫性或无后效性, 并称此过程为**马尔可夫过程**\n",
    "\n",
    "时间和状态都是**离散**的马尔可夫过程称为**马尔可夫链**, 简称马氏链, 记为$\\{X_n=X(n), n=0, 1, 2, \\cdots\\}$, 它可以看作在时间集$T_1 = \\{0, 1, 2, \\cdots\\}$上对离散状态的马尔可夫过程相继观察的结果.记链的状态空间为$I=\\{a_1, a_2, \\cdots\\}, a_i \\in R$, 马尔可夫性常用条件分布律来表示, 即对任意的正整数$n,r$和$0\\leq t_1 < t_2<\\cdots<t_r<t_m; t_i,m,m+n \\in T_1$, 有\n",
    "$$P\\{X_{m+n}= a_j|X_{t_1}=a_{1}, X_{t_2}=a_{2}, \\cdots X_{t_r}=a_{r}, X_{m}=a_{i}\\} \\\\\n",
    "= P\\{X_{m+n}=a_j |X_m=a_i\\}\n",
    "$$\n",
    "记上式右端为$P_{ij}(m, m+n)$, 称条件概率\n",
    "$$P_{ij}(m, m+n) = P\\{X_{m+n}=a_j | X_m=a_i\\}$$\n",
    "为马尔科夫链在时刻$m$处于状态$a_i$条件下, 在时刻$m+n$转移到状态$a_j$的**转移概率**.  \n",
    "由于链在时刻$m$从任何一个状态$a_i$出发, 到另一时刻$m+n$, 必然转移到$a_1, a_2, \\cdots$诸状态中的某一个,所以\n",
    "$$\\sum_{j}P_{ij}(m, m+n) = 1, \\quad i=1, 2, \\cdots.$$\n",
    "由转移概率组成的矩阵$P(m, m+n) = (P_{ij}(m, m+n))$称为马氏链的**转移概率矩阵**, 每一行之和等于1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当转移概率$P_{ij}(m, m+n)$只与$i,j$及时间间距$n$有关时, 把它记为$P_{ij}(n)$, 即\n",
    "$$P_{ij}(m, m+n) = P_{ij}(n)$$\n",
    "并称此转移概率具有**平稳性**. 同时也称此链是**齐次**的或**齐时**的. 在齐次情形下, 上述定义的转移概率可以写成\n",
    "$$P_{ij}(n) = P\\{X_{m+n}=a_j|X_m=a_i\\}$$\n",
    "称为马氏链的**n步转移概率**, $P(n)=(P_{ij}(n))$为**n步转移概率矩阵**. 其中特别重要的就是一步转移概率\n",
    "$$p_{ij}=P_{ij}(1)=P\\{X_{m+1}=a_j|X_m=a_i\\}$$\n",
    "和他们组成的一步转移概率矩阵\n",
    "$$\\begin{bmatrix}p_{11} & p_{12} & \\cdots & p_{1j} & \\cdots \\\\\n",
    "p_{21} & p_{22} & \\cdots & p_{2j} & \\cdots \\\\\n",
    "\\vdots & \\vdots& &\\vdots \\\\\n",
    "p_{i1} & p_{i2} & \\cdots & p_{ij} & \\cdots \\\\\n",
    "\\vdots & \\vdots& &\\vdots \\\\\n",
    "\\end{bmatrix} = P(1)$$\n",
    "记作$P.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例1 股市模型\n",
    "def markov_market(init, transfer, n):\n",
    "    # 股市 bull 0 , bear 1 , stagnant 2 \n",
    "    # 初始状态\n",
    "    init_array = init\n",
    "    # 一步状态转移矩阵\n",
    "    transfer_array = transfer\n",
    "    temp = init_array\n",
    "    for i in range(n):\n",
    "        res = np.dot(temp, transfer_array)\n",
    "        print(f'{i}\\t{res}')\n",
    "        temp = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer = np.array([[0.9, 0.075, 0.025],\n",
    "                   [0.15, 0.8, 0.05],\n",
    "                   [0.25, 0.25, 0.5]])\n",
    "init = np.array([0.1, 0.2, 0.7])\n",
    "markov_market(init, transfer, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_market(np.array([0.3, 0.3, 0.4]), transfer, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到初始状态$a_1$发生变化, 但两者都在第18次时候, 收敛到$[0.624, 0.312, 0.0625]$. 不管初始状态是什么样子, 只要状态转移矩阵不变, 当$n\\rightarrow \\infty$时, 最终状态始终会收敛到一个固定值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 状态转移矩阵的n次幂\n",
    "def matrix_power(matrix, n):\n",
    "    temp = matrix\n",
    "    for i in range(n):\n",
    "        res = np.dot(temp, matrix)\n",
    "        print(f'{i}\\t{res}')\n",
    "        temp = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_power(transfer, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随着幂的次数的增加, 结果开始收敛, 每一行都为$[0.624, 0.312, 0.0625]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隐马尔可夫模型(hidden Markov model, HMM)\n",
    "隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态的序列(状态序列, state sequence)，再由各个状态随机生成一个观测而产生观测的序列(观测序列, observation sequence)的过程。序列的每一个位置又可以看作是一个时刻.\n",
    "设$Q$为所有可能的状态集合, $V$是所有可能的观测集合:\n",
    "$$Q=\\{q_1, q_2, \\cdots, q_N\\}, \\quad V=\\{v_1, v_2, \\cdots, v_M\\}$$\n",
    "其中, $N$是可能的状态数, $M$是可能的观测数.  \n",
    "$I$是长度$T$的状态序列, $O$是对应的观测序列:\n",
    "$$I=(i_1, i_2, \\cdots, i_T), \\quad O=(o_1, o_2, \\cdots, o_T)$$\n",
    "$A$是(一步)状态转移概率矩阵:\n",
    "$$A=[a_{ij}]_{N\\times N}$$\n",
    "$$a_{ij}=P(i_{t+1}=q_j|i_t=q_i), \\quad i=1, 2, \\cdots, N; \\quad j=1, 2, \\cdots, N$$\n",
    "与上述$P_{ij}(1)$含义相同.\n",
    "$B$是观测概率矩阵:\n",
    "$$B=[b_j(k)]_{N\\times M}$$\n",
    "其中,\n",
    "$$b_j(k)=P(o_t=v_k|i_t=q_j), \\quad k=1, 2, \\cdots, M; \\quad j=1, 2, \\cdots, N$$\n",
    "是在时刻$t$处于状态$q_i$条件下生成观测$v_k$的概率.\n",
    "$\\pi$是初始状态概率向量:\n",
    "$$\\pi = (\\pi_i)$$\n",
    "其中,\n",
    "$$\\pi_i = P(i_1 = q_i), \\quad i=1, 2, \\cdots, N$$\n",
    "是初始时刻$t=1$处于状态$q_i$的概率.\n",
    "\n",
    "隐马尔可夫模型由初始状态概率向$\\pi$、状态转移概率矩阵$A$和观测概率矩阵$B$决定。因此，隐马尔可夫模型可以写成\n",
    "$$\\lambda=(A, B, \\pi)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概率计算问题-评估观察序列概率\n",
    "给定模型$\\lambda=(A, B, \\pi)$和观测序列$O＝(o_1，o_2,…,o_T)$，计算在模型$\\lambda$下观测序列$O$出现的概率$P(O|\\lambda)$。前向-后向算法是通过递推地计算前向-后向概率可以高效地进行隐马尔可夫模型的概率计算。  \n",
    "### 前向算法\n",
    "**前向概率** 给定隐马尔可夫模型$\\lambda$, 定义到时刻$t$部分观测序列为$o_1, o_2, \\cdots, o_t$且状态为$q_i$的概率为前向概率, 记作\n",
    "$$\\alpha_t(i) = P(o_1, o_2, \\cdots, o_t, i_t=q_i|\\lambda)$$\n",
    "\n",
    "1. 初值 $$\\alpha_1 = P(o_1,i_1=q_i) = P(o_1|i_1=q_i)P(i_1=q_i) = b_i(o_1)\\pi_i$$\n",
    "\n",
    "2. 递推 对$t=1, 2, \\cdots, T-1$\n",
    "   $$\\begin{align}\\alpha_{t+1}(i) &= P(o_1, o_2, \\cdots, o_t, o_{t+1}, i_{t+1}=q_i) \\\\\n",
    "   &= P(o_1, o_2, \\cdots, o_t, i_{t+1} = q_i)P(o_{t+1}|i_{t+1}=q_i, o_1, o_2, \\cdots, o_t) \\\\\n",
    "   &= \\left[\\sum_{j=1}^N P(o_1, o_2, \\cdots, o_t, i_{t} = q_j, i_{t+1}=q_i)\\right]b_i(o_{t+1}) \\\\\n",
    "   &= \\left[\\sum_{j=1}^N P(o_1, o_2, \\cdots, o_t, i_{t} = q_j)P(i_{t+1}=q_i|i_{t}= q_j)\\right]b_i(o_{t+1}) \\\\\n",
    "   &= \\left[\\sum_{j=1}^N \\alpha_t(j)a_{ji}\\right]b_i(o_{t+1})\n",
    "   \\end{align}$$\n",
    "   按定义$\\alpha_{t+1}(i)$可以表示为$t$时刻观测到$o_1, o_2, \\cdots, o_t$且在$t+1$时刻处于状态$q_i$的联合概率与观测概率$b_i(o_{t+1})$乘积;而前半部分又可以写成全概率公式的模式;再由马尔可夫链的齐次性质写成条件概率形式\n",
    "3. 终止\n",
    "   $$P(O|\\lambda) = \\sum_{i=1}^N P(o_1, o_2, \\cdots, o_T, i_{T} = q_i) = \\sum_{j=1}^N \\alpha_T(i)$$\n",
    "   \n",
    "复杂度分析: 每一次计算直接引用前一个时刻的计算结果, 避免重复计算, 时间复杂度$O(TN^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**习题10.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#习题10.1\n",
    "Q = [1, 2, 3]\n",
    "V = ['红', '白']\n",
    "A = np.array([[0.5, 0.2, 0.3], [0.3, 0.5, 0.2], [0.2, 0.3, 0.5]])\n",
    "B = np.array([[0.5, 0.5], [0.4, 0.6], [0.7, 0.3]])\n",
    "# O = ['红', '白', '红', '红', '白', '红', '白', '白']\n",
    "O = ['红', '白', '红', '白']    #习题10.1的例子\n",
    "PI = np.array([0.2, 0.4, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(A, B, Q, O, V, PI):\n",
    "    N = len(Q)  # 可能的状态数量\n",
    "    T = len(O)  # 观测序列长度=时刻\n",
    "    for t in range(0, T):\n",
    "        k = V.index(O[t])  # O_t对应的观测状态序号k\n",
    "        if t == 0 :\n",
    "            alpha = PI * B[:, k]  # PI[i] * b_i(o_1)  (N, )\n",
    "        else:\n",
    "            for i in range(N):\n",
    "                # alpha_{t+1}(i) = (\\sum_j alpha_{t}(j) A[j, i] ) B[i, k]\n",
    "                alpha[i] = np.sum(alpha_last * A[:, i]) * B[i, k]\n",
    "        alpha_last = alpha.copy()\n",
    "        print(f\"alpha[{t}]: {alpha}\")\n",
    "    return np.sum(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward(A, B, Q, O, V, PI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 后向算法\n",
    "**后向概率**: 给定隐马尔可夫模型$\\lambda$, 定义到时刻$t$状态为$q_i$的条件下, 从$t+1$到$T$的部分观测序列为$o_{t+1}, o_{t+2}, \\cdots, o_T$的概率为后向概率, 记作\n",
    "$$\\beta_t(i) = P(o_{t+1}, o_{t+2}, \\cdots, o_T|i_t=q_i, \\lambda)$$\n",
    "用递推的方法求后向概率$\\beta_t{i}$以及观测序列概率$P(O|\\lambda)$\n",
    "\n",
    "后向概率的动态规划递推公式和前向概率是相反的。现在我们假设我们已经找到了在时刻$t+1$时各个隐藏状态的后向概率$\\beta_{t+1}(j)$，现在我们需要递推出时刻$t$时各个隐藏状态的后向概率。我们可以计算出观测状态的序列为$o_{t+2},o_{t+3},...o_T$， $t$时隐藏状态为$q_i$, 时刻$t+1$隐藏状态为$q_j$的概率为$a_{ij}\\beta_{t+1}(j)$, 接着可以得到观测状态的序列为$o_{t+1},o_{t+2},...o_T$， $t$时隐藏状态为$q_i$, 时刻$t+1$隐藏状态为$q_j$的概率为$a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)$, 则把所有对应的概率加起来，我们可以得到观测状态的序列为$o_{t+1},o_{t+2},...o_T$， $t$时隐藏状态为$q_i$的概率为$\\sum\\limits_{j=1}^{N}a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)$，这个概率即为时刻$t$的后向概率。\n",
    "\n",
    "输入：HMM模型$\\lambda = (A, B, \\pi)$，观测序列$O=(o_1,o_2,...o_T)$\n",
    "\n",
    "输出：观测序列概率$P(O|\\lambda)$\n",
    "\n",
    "1) 初始化时刻$T$的各个隐藏状态后向概率：$$\\beta_T(i) = 1,\\; i=1,2,...N$$\n",
    "\n",
    "2) 递推时刻$T-1,T-2,...1$时刻的后向概率：$$\\beta_{t}(i) = \\sum\\limits_{j=1}^{N}a_{ij}b_j(o_{t+1})\\beta_{t+1}(j),\\; i=1,2,...N$$\n",
    "\n",
    "3) 计算最终结果：$$P(O|\\lambda) = \\sum\\limits_{i=1}^N\\pi_ib_i(o_1)\\beta_1(i)$$\n",
    "\n",
    "此时我们的算法时间复杂度仍然是$O(TN^2)$。\n",
    "\n",
    "利用前向概率和后向概率的定义可以将观测序列概率$P(O|\\lambda)$统一写成:\n",
    "$$P(O|\\lambda) = \\sum_{j=1}^N \\sum_{i=1}^N \\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j), \\quad t=1, 2, \\cdots, T-1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backforwad(A, B, Q, O, V, PI):\n",
    "    N = len(Q)\n",
    "    T = len(O)\n",
    "    for t in range(T-1, -1, -1):\n",
    "        if t == T-1:\n",
    "            beta = np.ones(N)\n",
    "        else:\n",
    "            k = V.index(O[t + 1])  # t时刻观测值o_t对应在观测集合中索引k \n",
    "            for i in range(N):\n",
    "                beta[i] = np.sum(A[i, :] * B[:, k] * beta_last)\n",
    "        print(f'beta[{t}]:, {beta}')\n",
    "        beta_last = beta.copy()\n",
    "    return np.sum(PI * B[:, V.index(O[0])] * beta_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backforwad(A, B, Q, O, V, PI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一些概率与期望值的计算\n",
    "利用前向概率和后向概率，我们可以计算出HMM中单个状态和两个状态的概率公式。\n",
    "\n",
    "1）给定模型$\\lambda$和观测序列$O$,在时刻$t$处于状态$q_i$的概率记为:$$\\gamma_t(i) = P(i_t = q_i | O,\\lambda) = \\frac{P(i_t = q_i ,O|\\lambda)}{P(O|\\lambda)} $$\n",
    "\n",
    "利用前向概率和后向概率的定义可知：$$P(i_t = q_i ,O|\\lambda) = \\alpha_t(i)\\beta_t(i)$$\n",
    "\n",
    "于是我们得到：$$\\gamma_t(i) = \\frac{ \\alpha_t(i)\\beta_t(i)}{\\sum\\limits_{j=1}^N \\alpha_t(j)\\beta_t(j)}$$\n",
    "\n",
    "2）给定模型$\\lambda$和观测序列$O$,在时刻$t$处于状态$q_i$，且时刻$t+1$处于状态$q_j$的概率记为:$$\\xi_t(i,j) = P(i_t = q_i, i_{t+1}=q_j | O,\\lambda) = \\frac{ P(i_t = q_i, i_{t+1}=q_j , O|\\lambda)}{P(O|\\lambda)} $$\n",
    "\n",
    "而$P(i_t = q_i, i_{t+1}=q_j , O|\\lambda)$可以由前向后向概率来表示为:$$P(i_t = q_i, i_{t+1}=q_j , O|\\lambda) = \\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)$$\n",
    "\n",
    "从而最终我们得到$\\xi_t(i,j)$的表达式如下：$$\\xi_t(i,j) = \\frac{\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}{\\sum\\limits_{r=1}^N\\sum\\limits_{s=1}^N\\alpha_t(r)a_{rs}b_s(o_{t+1})\\beta_{t+1}(s)}$$\n",
    "\n",
    " 3) 将$\\gamma_t(i)$和$\\xi_t(i,j)$在各个时刻$t$求和，可以得到：\n",
    "\n",
    "在观测序列$O$下状态$i$出现的期望值$\\sum\\limits_{t=1}^T\\gamma_t(i)$\n",
    "\n",
    "在观测序列$O$下由状态$i$转移的期望值$\\sum\\limits_{t=1}^{T-1}\\gamma_t(i)$\n",
    "\n",
    "在观测序列$O$下由状态$i$转移到状态$j$的期望值$\\sum\\limits_{t=1}^{T-1}\\xi_t(i,j)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**习题10.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = [1, 2, 3]\n",
    "V = ['红', '白']\n",
    "A = np.array([[0.5, 0.1, 0.4], [0.3, 0.5, 0.2], [0.2, 0.2, 0.6]])\n",
    "B = np.array([[0.5, 0.5], [0.4, 0.6], [0.7, 0.3]])\n",
    "O = ['红', '白', '红', '红', '白', '红', '白', '白']\n",
    "PI = np.array([0.2, 0.3, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forword_backword(A, B, Q, O, V, PI):\n",
    "    # 前向-后向概率算法\n",
    "    N = len(Q)\n",
    "    T = len(O)\n",
    "    alphas = np.zeros((T, N))\n",
    "    betas = np.zeros((T, N))\n",
    "    gammas = np.zeros((T, N))\n",
    "    for t in range(0, T):\n",
    "        k = V.index(O[t])  # O_t对应的观测状态序号k\n",
    "        t_reverse = T-1-t\n",
    "        if t == 0 :\n",
    "            alphas[0, :] = PI * B[:, k]  # PI[i] * b_i(o_1)  (N, )\n",
    "            betas[t_reverse, :] = np.ones(N)\n",
    "        else:\n",
    "            for i in range(N):\n",
    "                # alpha_{t+1}(i) = (\\sum_j alpha_{t}(j) A[j, i] ) B[i, k]\n",
    "                alphas[t, i] = np.sum(alphas[t-1, :] * A[:, i]) * B[i, k]\n",
    "                betas[t_reverse, i] = np.sum(A[i, :] * B[:, k] * betas[t_reverse+1, :])\n",
    "    for t in range(0, T):\n",
    "        gammas[t, :] = alphas[t, :] * betas[t, :] / np.sum(alphas[t, :] * betas[t, :])\n",
    "    print(f\"alphas:\\n {alphas}\")\n",
    "    print(f\"betas:\\n {betas}\")\n",
    "    print(f\"gammas:\\n {gammas}\")\n",
    "    print(\"P(O|\\lambda):\", np.sum(alphas[-1]), np.sum(PI * B[:, V.index(O[0])] * betas[0,:]))\n",
    "    return gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = forword_backword(A, B, Q, O, V, PI)\n",
    "# P(i_4=q_3|O)\n",
    "gammas[3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习算法-参数求解\n",
    "### 监督学习方法\n",
    "已知$D$个长度为$T$的观测序列和对应的隐藏状态序列，即$\\{(O_1, I_1), (O_2, I_2), ...(O_D, I_D)\\}$是已知的，此时我们可以很容易的用**极大似然法**来求解模型参数。\n",
    "\n",
    "1. 转移概率$a_{ij}$估计  \n",
    "假设样本从隐藏状态$q_i$转移到$q_j$的频率计数是$A_{ij}$,那么状态转移矩阵求得为：$$A = \\Big[\\hat a_{ij}\\Big], \\;其中\\hat a_{ij} = \\frac{A_{ij}}{\\sum\\limits_{s=1}^{N}A_{is}}$$\n",
    "2. 观测概率$b_j(k)$  \n",
    "假设样本隐藏状态为$q_j$且观测状态为$v_k$的频率计数是$B_{jk}$,那么观测状态概率矩阵为：$$B= \\Big[\\hat b_{j}(k)\\Big], \\;其中\\hat b_{j}(k) = \\frac{B_{jk}}{\\sum\\limits_{s=1}^{M}B_{js}}$$\n",
    "3. 初始状态概率估计  \n",
    "假设所有样本中初始隐藏状态为$q_i$的频率计数为$C(i)$,那么初始概率分布为：$$\\Pi = \\pi(i) = \\frac{C(i)}{\\sum\\limits_{s=1}^{N}C(s)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baum-Wecle 算法\n",
    "在很多时候，我们无法得到HMM样本观察序列对应的隐藏序列，只有$D$个长度为$T$的观测序列$\\{(O_1), (O_2), ...(O_D)\\}$而没有对应的状态序列$I_1, I_2, \\cdots, I_D$, 目标是学习隐马尔可夫模型参数$\\lambda=(A, B, \\pi)$, 将可观测的序列数据看做$O$, 状态序列数据看做不可观测的隐数据$I$, 那么隐马尔可夫模型事实上式一个含有隐变量的概率模型\n",
    "$$P(O|\\lambda) = \\sum_I P(O|I, \\lambda)P(I|\\lambda)$$\n",
    "它的参数学习可以由EM算法实现.\n",
    "鲍姆-韦尔奇算法原理既然使用的就是EM算法的原理，那么我们需要在E步求出联合分布$P(O,I|\\lambda)$基于条件概率$P(I|O,\\overline{\\lambda})$的期望，其中$\\overline{\\lambda}$为当前的模型参数，然后再M步最大化这个期望，得到更新的模型参数$\\lambda$。接着不停的进行EM迭代，直到模型参数的值收敛为止。\n",
    "\n",
    "1. E步，当前模型参数为$\\overline{\\lambda}$, 联合分布$P(O,I|\\lambda)$基于条件概率$P(I|O,\\overline{\\lambda})$的期望表达式为：$$L(\\lambda, \\overline{\\lambda}) = \\sum\\limits_{I}P(I|O,\\overline{\\lambda})logP(O,I|\\lambda)$$\n",
    "\n",
    "2. M步，我们极大化上式，然后得到更新后的模型参数如下：　$$\\overline{\\lambda} = arg\\;\\max_{\\lambda}\\sum\\limits_{I}P(I|O,\\overline{\\lambda})logP(O,I|\\lambda)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkov:\n",
    "    def forward(self, Q, V, A, B, O, PI):  # 使用前向算法\n",
    "        N = len(Q)  #可能存在的状态数量\n",
    "        M = len(O)  # 观测序列的大小\n",
    "        alphas = np.zeros((N, M))  # alpha值\n",
    "        T = M  # 有几个时刻，有几个观测序列，就有几个时刻\n",
    "        for t in range(T):  # 遍历每一时刻，算出alpha值\n",
    "            indexOfO = V.index(O[t])  # 找出序列对应的索引\n",
    "            for i in range(N):\n",
    "                if t == 0:  # 计算初值\n",
    "                    alphas[i][t] = PI[t][i] * B[i][indexOfO]  # P176（10.15）\n",
    "                    print(\n",
    "                        'alpha1(%d)=p%db%db(o1)=%f' % (i, i, i, alphas[i][t]))\n",
    "                else:\n",
    "                    alphas[i][t] = np.dot(\n",
    "                        [alpha[t - 1] for alpha in alphas],\n",
    "                        [a[i] for a in A]) * B[i][indexOfO]  # 对应P176（10.16）\n",
    "                    print('alpha%d(%d)=[sigma alpha%d(i)ai%d]b%d(o%d)=%f' %\n",
    "                          (t, i, t - 1, i, i, t, alphas[i][t]))\n",
    "                    # print(alphas)\n",
    "        P = np.sum([alpha[M - 1] for alpha in alphas])  # P176(10.17)\n",
    "        # alpha11 = pi[0][0] * B[0][0]    #代表a1(1)\n",
    "        # alpha12 = pi[0][1] * B[1][0]    #代表a1(2)\n",
    "        # alpha13 = pi[0][2] * B[2][0]    #代表a1(3)\n",
    "\n",
    "    def backward(self, Q, V, A, B, O, PI):  # 后向算法\n",
    "        N = len(Q)  # 可能存在的状态数量\n",
    "        M = len(O)  # 观测序列的大小\n",
    "        betas = np.ones((N, M))  # beta\n",
    "        for i in range(N):\n",
    "            print('beta%d(%d)=1' % (M, i))\n",
    "        for t in range(M - 2, -1, -1):\n",
    "            indexOfO = V.index(O[t + 1])  # 找出序列对应的索引\n",
    "            for i in range(N):\n",
    "                betas[i][t] = np.dot(\n",
    "                    np.multiply(A[i], [b[indexOfO] for b in B]),\n",
    "                    [beta[t + 1] for beta in betas])\n",
    "                realT = t + 1\n",
    "                realI = i + 1\n",
    "                print(\n",
    "                    'beta%d(%d)=[sigma a%djbj(o%d)]beta%d(j)=(' %\n",
    "                    (realT, realI, realI, realT + 1, realT + 1),\n",
    "                    end='')\n",
    "                for j in range(N):\n",
    "                    print(\n",
    "                        \"%.2f*%.2f*%.2f+\" % (A[i][j], B[j][indexOfO],\n",
    "                                             betas[j][t + 1]),\n",
    "                        end='')\n",
    "                print(\"0)=%.3f\" % betas[i][t])\n",
    "        # print(betas)\n",
    "        indexOfO = V.index(O[0])\n",
    "        P = np.dot(\n",
    "            np.multiply(PI, [b[indexOfO] for b in B]),\n",
    "            [beta[0] for beta in betas])\n",
    "        print(\"P(O|lambda)=\", end=\"\")\n",
    "        for i in range(N):\n",
    "            print(\n",
    "                \"%.1f*%.1f*%.5f+\" % (PI[0][i], B[i][indexOfO], betas[i][0]),\n",
    "                end=\"\")\n",
    "        print(\"0=%f\" % P)\n",
    "\n",
    "    def viterbi(self, Q, V, A, B, O, PI):\n",
    "        N = len(Q)  #可能存在的状态数量\n",
    "        M = len(O)  # 观测序列的大小\n",
    "        deltas = np.zeros((N, M))\n",
    "        psis = np.zeros((N, M))\n",
    "        I = np.zeros((1, M))\n",
    "        for t in range(M):\n",
    "            realT = t + 1\n",
    "            indexOfO = V.index(O[t])  # 找出序列对应的索引\n",
    "            for i in range(N):\n",
    "                realI = i + 1\n",
    "                if t == 0:\n",
    "                    deltas[i][t] = PI[0][i] * B[i][indexOfO]\n",
    "                    psis[i][t] = 0\n",
    "                    print('delta1(%d)=pi%d * b%d(o1)=%.2f * %.2f=%.2f' %\n",
    "                          (realI, realI, realI, PI[0][i], B[i][indexOfO],\n",
    "                           deltas[i][t]))\n",
    "                    print('psis1(%d)=0' % (realI))\n",
    "                else:\n",
    "                    deltas[i][t] = np.max(\n",
    "                        np.multiply([delta[t - 1] for delta in deltas],\n",
    "                                    [a[i] for a in A])) * B[i][indexOfO]\n",
    "                    print(\n",
    "                        'delta%d(%d)=max[delta%d(j)aj%d]b%d(o%d)=%.2f*%.2f=%.5f'\n",
    "                        % (realT, realI, realT - 1, realI, realI, realT,\n",
    "                           np.max(\n",
    "                               np.multiply([delta[t - 1] for delta in deltas],\n",
    "                                           [a[i] for a in A])), B[i][indexOfO],\n",
    "                           deltas[i][t]))\n",
    "                    psis[i][t] = np.argmax(\n",
    "                        np.multiply(\n",
    "                            [delta[t - 1] for delta in deltas],\n",
    "                            [a[i]\n",
    "                             for a in A])) + 1  #由于其返回的是索引，因此应+1才能和正常的下标值相符合。\n",
    "                    print('psis%d(%d)=argmax[delta%d(j)aj%d]=%d' %\n",
    "                          (realT, realI, realT - 1, realI, psis[i][t]))\n",
    "        print(deltas)\n",
    "        print(psis)\n",
    "        I[0][M - 1] = np.argmax([delta[M - 1] for delta in deltas\n",
    "                                 ]) + 1  #由于其返回的是索引，因此应+1才能和正常的下标值相符合。\n",
    "        print('i%d=argmax[deltaT(i)]=%d' % (M, I[0][M - 1]))\n",
    "        for t in range(M - 2, -1, -1):\n",
    "            I[0][t] = psis[int(I[0][t + 1]) - 1][t + 1]\n",
    "            print('i%d=psis%d(i%d)=%d' % (t + 1, t + 2, t + 2, I[0][t]))\n",
    "        print(\"状态序列I：\", I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考: [隐马尔科夫模型HMM](https://www.cnblogs.com/pinard/p/6945257.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "参考代码：https://blog.csdn.net/tudaodiaozhale\n",
    "\n",
    "中文注释制作：机器学习初学者\n",
    "\n",
    "微信公众号：ID:ai-start-com\n",
    "\n",
    "配置环境：python 3.5+\n",
    "\n",
    "代码全部测试通过。\n",
    "![gongzhong](../gongzhong.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
