{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10章 隐马尔可夫模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1．隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态的序列，再由各个状态随机生成一个观测而产生观测的序列的过程。\n",
    "\n",
    "隐马尔可夫模型由初始状态概率向$\\pi$、状态转移概率矩阵$A$和观测概率矩阵$B$决定。因此，隐马尔可夫模型可以写成$\\lambda=(A, B, \\pi)$。\n",
    "\n",
    "隐马尔可夫模型是一个生成模型，表示状态序列和观测序列的联合分布，但是状态序列是隐藏的，不可观测的。\n",
    "\n",
    "隐马尔可夫模型可以用于标注，这时状态对应着标记。标注问题是给定观测序列预测其对应的标记序列。\n",
    "\n",
    "2．概率计算问题。给定模型$\\lambda=(A, B, \\pi)$和观测序列$O＝(o_1，o_2,…,o_T)$，计算在模型$\\lambda$下观测序列$O$出现的概率$P(O|\\lambda)$。前向-后向算法是通过递推地计算前向-后向概率可以高效地进行隐马尔可夫模型的概率计算。\n",
    " \n",
    "3．学习问题。已知观测序列$O＝(o_1，o_2,…,o_T)$，估计模型$\\lambda=(A, B, \\pi)$参数，使得在该模型下观测序列概率$P(O|\\lambda)$最大。即用极大似然估计的方法估计参数。Baum-Welch算法，也就是EM算法可以高效地对隐马尔可夫模型进行训练。它是一种非监督学习算法。\n",
    "\n",
    "4．预测问题。已知模型$\\lambda=(A, B, \\pi)$和观测序列$O＝(o_1，o_2,…,o_T)$，求对给定观测序列条件概率$P(I|O)$最大的状态序列$I＝(i_1，i_2,…,i_T)$。维特比算法应用动态规划高效地求解最优路径，即概率最大的状态序列。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 马尔可夫链\n",
    "## 马尔可夫(Markov)过程及其概率分布\n",
    "**马尔可夫性**或**无后效性**: 过程(或系统)在时刻$t_0$所处状态为已知条件下, 过程在时刻$t>t_0$所处状态的条件分布与过程在时刻$t_0$之前所处的状态无关. 通俗地说, 就是在已知过程\"现在\"的条件下, 其\"将来\"不依赖于\"过去\".\n",
    "\n",
    "设随机过程$\\{X(t), t \\in T\\}$的状态空间为$I$. 如果对时间$t$的任意$n$个数值$t_1<t_2< \\cdots< t_n, n\\geq3, t_i \\in T $, 在条件$X(t_i)=x_i, x_i \\in I, i=1,2, \\cdots, n-1$下, $X(t_n)$的条件分布函数恰等于在条件$X(t_{n-1})=x_{n-1}$下$X(t_n)$的条件分布函数, 即\n",
    "$$\n",
    "P\\{X(t_n) \\leq x_n| X(t_1)=x_1, X(t_2)=x_2, \\cdots, X(t_{n-1})=x_{n-1}\\} \\\\\n",
    "= P\\{X(t_{n}) \\leq x_{n}|X(t_{n-1})=x_{n-1}\\}, x_n \\in \\mathcal R\n",
    "$$\n",
    "或写成\n",
    "$$F_{t_n|t_1\\cdots t_{n-1}}(x_n,t_n|x_1, x_2, \\cdots, x_{n-1};t_1, t_2, \\cdots, t_{n-1}) \n",
    "= F_{t_n|t_{n-1}}(x_n, t_n|x_{n-1},t_{n-1})\n",
    "$$\n",
    "则称过程$\\{X(t), t\\in T\\}$具有马尔可夫性或无后效性, 并称此过程为**马尔可夫过程**\n",
    "\n",
    "时间和状态都是**离散**的马尔可夫过程称为**马尔可夫链**, 简称马氏链, 记为$\\{X_n=X(n), n=0, 1, 2, \\cdots\\}$, 它可以看作在时间集$T_1 = \\{0, 1, 2, \\cdots\\}$上对离散状态的马尔可夫过程相继观察的结果.记链的状态空间为$I=\\{a_1, a_2, \\cdots\\}, a_i \\in R$, 马尔可夫性常用条件分布律来表示, 即对任意的正整数$n,r$和$0\\leq t_1 < t_2<\\cdots<t_r<t_m; t_i,m,m+n \\in T_1$, 有\n",
    "$$P\\{X_{m+n}= a_j|X_{t_1}=a_{1}, X_{t_2}=a_{2}, \\cdots X_{t_r}=a_{r}, X_{m}=a_{i}\\} \\\\\n",
    "= P\\{X_{m+n}=a_j |X_m=a_i\\}\n",
    "$$\n",
    "记上式右端为$P_{ij}(m, m+n)$, 称条件概率\n",
    "$$P_{ij}(m, m+n) = P\\{X_{m+n}=a_j | X_m=a_i\\}$$\n",
    "为马尔科夫链在时刻$m$处于状态$a_i$条件下, 在时刻$m+n$转移到状态$a_j$的**转移概率**.  \n",
    "由于链在时刻$m$从任何一个状态$a_i$出发, 到另一时刻$m+n$, 必然转移到$a_1, a_2, \\cdots$诸状态中的某一个,所以\n",
    "$$\\sum_{j}P_{ij}(m, m+n) = 1, \\quad i=1, 2, \\cdots.$$\n",
    "由转移概率组成的矩阵$P(m, m+n) = (P_{ij}(m, m+n))$称为马氏链的**转移概率矩阵**, 每一行之和等于1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当转移概率$P_{ij}(m, m+n)$只与$i,j$及时间间距$n$有关时, 把它记为$P_{ij}(n)$, 即\n",
    "$$P_{ij}(m, m+n) = P_{ij}(n)$$\n",
    "并称此转移概率具有**平稳性**. 同时也称此链是**齐次**的或**齐时**的. 在齐次情形下, 上述定义的转移概率可以写成\n",
    "$$P_{ij}(n) = P\\{X_{m+n}=a_j|X_m=a_i\\}$$\n",
    "称为马氏链的**n步转移概率**, $P(n)=(P_{ij}(n))$为**n步转移概率矩阵**. 其中特别重要的就是一步转移概率\n",
    "$$p_{ij}=P_{ij}(1)=P\\{X_{m+1}=a_j|X_m=a_i\\}$$\n",
    "和他们组成的一步转移概率矩阵\n",
    "$$\n",
    "\\begin{bmatrix}p_{11} & p_{12} & \\cdots & p_{1j} & \\cdots \\\\\n",
    "p_{21} & p_{22} & \\cdots & p_{2j} & \\cdots \\\\\n",
    "\\vdots & \\vdots& &\\vdots \\\\\n",
    "p_{i1} & p_{i2} & \\cdots & p_{ij} & \\cdots \\\\\n",
    "\\vdots & \\vdots& &\\vdots \\\\\n",
    "\\end{bmatrix} = P(1)\n",
    "$$\n",
    "记作$P.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=5, suppress=True, threshold=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例1 股市模型\n",
    "def markov_market(init, transfer, n):\n",
    "    # 股市 bull 0 , bear 1 , stagnant 2 \n",
    "    # 初始状态\n",
    "    init_array = init\n",
    "    # 一步状态转移矩阵\n",
    "    transfer_array = transfer\n",
    "    temp = init_array\n",
    "    for i in range(n):\n",
    "        res = temp @ transfer_array\n",
    "        print(f'{i}\\t{res}')\n",
    "        temp = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t[[0.295  0.3425 0.3625]]\n",
      "1\t[[0.4075  0.38675 0.20575]]\n",
      "2\t[[0.4762 0.3914 0.1324]]\n",
      "3\t[[0.52039 0.38194 0.09767]]\n",
      "4\t[[0.55006 0.369   0.08094]]\n",
      "5\t[[0.57064 0.35669 0.07267]]\n",
      "6\t[[0.58525 0.34632 0.06844]]\n",
      "7\t[[0.59578 0.33806 0.06617]]\n",
      "8\t[[0.60345 0.33167 0.06488]]\n",
      "9\t[[0.60908 0.32681 0.06411]]\n",
      "10\t[[0.61322 0.32316 0.06362]]\n",
      "11\t[[0.61628 0.32042 0.0633 ]]\n",
      "12\t[[0.61854 0.31839 0.06308]]\n",
      "13\t[[0.62021 0.31687 0.06292]]\n",
      "14\t[[0.62145 0.31574 0.06281]]\n",
      "15\t[[0.62237 0.3149  0.06273]]\n",
      "16\t[[0.62305 0.31428 0.06267]]\n",
      "17\t[[0.62355 0.31382 0.06262]]\n",
      "18\t[[0.62393 0.31348 0.06259]]\n",
      "19\t[[0.62421 0.31323 0.06257]]\n",
      "20\t[[0.62441 0.31304 0.06255]]\n",
      "21\t[[0.62456 0.3129  0.06254]]\n",
      "22\t[[0.62468 0.3128  0.06253]]\n",
      "23\t[[0.62476 0.31272 0.06252]]\n",
      "24\t[[0.62482 0.31266 0.06252]]\n",
      "25\t[[0.62487 0.31262 0.06251]]\n",
      "26\t[[0.6249  0.31259 0.06251]]\n",
      "27\t[[0.62493 0.31257 0.06251]]\n",
      "28\t[[0.62495 0.31255 0.0625 ]]\n",
      "29\t[[0.62496 0.31254 0.0625 ]]\n"
     ]
    }
   ],
   "source": [
    "transfer = np.array([[0.9, 0.075, 0.025],\n",
    "                   [0.15, 0.8, 0.05],\n",
    "                   [0.25, 0.25, 0.5]])\n",
    "init = np.array([0.1, 0.2, 0.7]).reshape(1, 3)\n",
    "markov_market(init, transfer, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t[0.415  0.3625 0.2225]\n",
      "1\t[0.4835  0.37675 0.13975]\n",
      "2\t[0.5266 0.3726 0.1008]\n",
      "3\t[0.55503 0.36278 0.0822 ]\n",
      "4\t[0.57449 0.3524  0.07311]\n",
      "5\t[0.58818 0.34328 0.06854]\n",
      "6\t[0.59799 0.33587 0.06614]\n",
      "7\t[0.60511 0.33008 0.06481]\n",
      "8\t[0.61031 0.32565 0.06404]\n",
      "9\t[0.61414 0.3223  0.06356]\n",
      "10\t[0.61696 0.31979 0.06325]\n",
      "11\t[0.61904 0.31792 0.06304]\n",
      "12\t[0.62059 0.31652 0.06289]\n",
      "13\t[0.62173 0.31548 0.06279]\n",
      "14\t[0.62258 0.31471 0.06271]\n",
      "15\t[0.6232  0.31414 0.06266]\n",
      "16\t[0.62367 0.31372 0.06261]\n",
      "17\t[0.62401 0.3134  0.06258]\n",
      "18\t[0.62427 0.31317 0.06256]\n",
      "19\t[0.62446 0.313   0.06255]\n",
      "20\t[0.6246  0.31287 0.06253]\n",
      "21\t[0.6247  0.31277 0.06253]\n",
      "22\t[0.62478 0.3127  0.06252]\n",
      "23\t[0.62484 0.31265 0.06251]\n",
      "24\t[0.62488 0.31261 0.06251]\n",
      "25\t[0.62491 0.31258 0.06251]\n",
      "26\t[0.62493 0.31256 0.06251]\n",
      "27\t[0.62495 0.31255 0.0625 ]\n",
      "28\t[0.62496 0.31253 0.0625 ]\n",
      "29\t[0.62497 0.31252 0.0625 ]\n"
     ]
    }
   ],
   "source": [
    "markov_market(np.array([0.3, 0.3, 0.4]), transfer, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.57735, -0.44372, -0.034  ],\n",
       "       [-0.57735,  0.81131, -0.13018],\n",
       "       [-0.57735,  0.38065,  0.99091]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_, u = np.linalg.eig(transfer)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.     , 0.     , 0.     ],\n",
       "       [0.     , 0.74142, 0.     ],\n",
       "       [0.     , 0.     , 0.45858]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_ = np.diag(lambda_)\n",
    "lambda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.     , 0.     , 0.     ],\n",
       "       [0.     , 0.00252, 0.     ],\n",
       "       [0.     , 0.     , 0.     ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_ ** 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.57735, -0.44372, -0.034  ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u1 = u[0, :]\n",
    "u1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62592, 0.31166, 0.06242],\n",
       "       [0.62332, 0.31403, 0.06264],\n",
       "       [0.62421, 0.31322, 0.06257]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u @ lambda_ ** 20 @ np.linalg.inv(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到初始状态$a_1$发生变化, 但两者都在第18次时候, 收敛到$[0.624, 0.312, 0.0625]$. 不管初始状态是什么样子, 只要状态转移矩阵不变, 当$n\\rightarrow \\infty$时, 最终状态始终会收敛到一个固定值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 状态转移矩阵的n次幂\n",
    "def matrix_power(matrix, n):\n",
    "    temp = matrix\n",
    "    for i in range(n):\n",
    "        res = np.dot(temp, matrix)\n",
    "        print(f'{i}\\t{res}')\n",
    "        temp = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_power(transfer, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随着幂的次数的增加, 结果开始收敛, 每一行都为$[0.624, 0.312, 0.0625]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隐马尔可夫模型(hidden Markov model, HMM)\n",
    "隐马尔可夫模型是关于时序的概率模型，描述由一个隐藏的马尔可夫链随机生成不可观测的状态的序列(状态序列, state sequence)，再由各个状态随机生成一个观测而产生观测的序列(观测序列, observation sequence)的过程。序列的每一个位置又可以看作是一个时刻.\n",
    "设$Q$为所有可能的状态集合, $V$是所有可能的观测集合:\n",
    "$$Q=\\{q_1, q_2, \\cdots, q_N\\}, \\quad V=\\{v_1, v_2, \\cdots, v_M\\}$$\n",
    "其中, $N$是可能的状态数, $M$是可能的观测数.  \n",
    "$I$是长度$T$的状态序列, $O$是对应的观测序列:\n",
    "$$I=(i_1, i_2, \\cdots, i_T), \\quad O=(o_1, o_2, \\cdots, o_T)$$\n",
    "$A$是(一步)状态转移概率矩阵:\n",
    "$$A=[a_{ij}]_{N\\times N}$$\n",
    "$$a_{ij}=P(i_{t+1}=q_j|i_t=q_i), \\quad i=1, 2, \\cdots, N; \\quad j=1, 2, \\cdots, N$$\n",
    "与上述$P_{ij}(1)$含义相同.\n",
    "$B$是观测概率矩阵:\n",
    "$$B=[b_j(k)]_{N\\times M}$$\n",
    "其中,\n",
    "$$b_j(k)=P(o_t=v_k|i_t=q_j), \\quad k=1, 2, \\cdots, M; \\quad j=1, 2, \\cdots, N$$\n",
    "是在时刻$t$处于状态$q_i$条件下生成观测$v_k$的概率.\n",
    "$\\pi$是初始状态概率向量:\n",
    "$$\\pi = (\\pi_i)$$\n",
    "其中,\n",
    "$$\\pi_i = P(i_1 = q_i), \\quad i=1, 2, \\cdots, N$$\n",
    "是初始时刻$t=1$处于状态$q_i$的概率.\n",
    "\n",
    "隐马尔可夫模型由初始状态概率向$\\pi$、状态转移概率矩阵$A$和观测概率矩阵$B$决定。因此，隐马尔可夫模型可以写成\n",
    "$$\\lambda=(A, B, \\pi)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概率计算问题-评估观察序列概率\n",
    "给定模型$\\lambda=(A, B, \\pi)$和观测序列$O＝(o_1，o_2,…,o_T)$，计算在模型$\\lambda$下观测序列$O$出现的概率$P(O|\\lambda)$。前向-后向算法是通过递推地计算前向-后向概率可以高效地进行隐马尔可夫模型的概率计算。  \n",
    "### 前向算法\n",
    "**前向概率** 给定隐马尔可夫模型$\\lambda$, 定义到时刻$t$部分观测序列为$o_1, o_2, \\cdots, o_t$且状态为$q_i$的概率为前向概率, 记作\n",
    "$$\\alpha_t(i) = P(o_1, o_2, \\cdots, o_t, i_t=q_i|\\lambda)$$\n",
    "\n",
    "1. 初值 $$\\alpha_1 = P(o_1,i_1=q_i) = P(o_1|i_1=q_i)P(i_1=q_i) = b_i(o_1)\\pi_i$$\n",
    "\n",
    "2. 递推 对$t=1, 2, \\cdots, T-1$\n",
    "   $$\n",
    "   \\begin{align}\\alpha_{t+1}(i) &= P(o_1, o_2, \\cdots, o_t, o_{t+1}, i_{t+1}=q_i) \\\\\n",
    "   &= P(o_1, o_2, \\cdots, o_t, i_{t+1} = q_i)P(o_{t+1}|i_{t+1}=q_i, o_1, o_2, \\cdots, o_t) \\\\\n",
    "   &= \\left[\\sum_{j=1}^N P(o_1, o_2, \\cdots, o_t, i_{t} = q_j, i_{t+1}=q_i)\\right]b_i(o_{t+1}) \\\\\n",
    "   &= \\left[\\sum_{j=1}^N P(o_1, o_2, \\cdots, o_t, i_{t} = q_j)P(i_{t+1}=q_i|i_{t}= q_j)\\right]b_i(o_{t+1}) \\\\\n",
    "   &= \\left[\\sum_{j=1}^N \\alpha_t(j)a_{ji}\\right]b_i(o_{t+1})\n",
    "   \\end{align}\n",
    "   $$\n",
    "   按定义$\\alpha_{t+1}(i)$可以表示为$t$时刻观测到$o_1, o_2, \\cdots, o_t$且在$t+1$时刻处于状态$q_i$的联合概率与观测概率$b_i(o_{t+1})$乘积;而前半部分又可以写成全概率公式的模式;再由马尔可夫链的齐次性质写成条件概率形式\n",
    "3. 终止\n",
    "   $$\n",
    "   P(O|\\lambda) = \\sum_{i=1}^N P(o_1, o_2, \\cdots, o_T, i_{T} = q_i) = \\sum_{j=1}^N \\alpha_T(i)\n",
    "   $$\n",
    "   \n",
    "复杂度分析: 每一次计算直接引用前一个时刻的计算结果, 避免重复计算, 时间复杂度$O(TN^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**习题10.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#习题10.1\n",
    "Q = [1, 2, 3]\n",
    "V = ['红', '白']\n",
    "A = np.array([[0.5, 0.2, 0.3], [0.3, 0.5, 0.2], [0.2, 0.3, 0.5]])\n",
    "B = np.array([[0.5, 0.5], [0.4, 0.6], [0.7, 0.3]])\n",
    "# O = ['红', '白', '红', '红', '白', '红', '白', '白']\n",
    "O = ['红', '白', '红', '白']    #习题10.1的例子\n",
    "PI = np.array([0.2, 0.4, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(A, B, Q, O, V, PI):\n",
    "    N = len(Q)  # 可能的状态数量\n",
    "    T = len(O)  # 观测序列长度=时刻\n",
    "    for t in range(0, T):\n",
    "        k = V.index(O[t])  # O_t对应的观测状态序号k\n",
    "        if t == 0 :\n",
    "            alpha = PI * B[:, k]  # PI[i] * b_i(o_1)  (N, )\n",
    "        else:\n",
    "            for i in range(N):\n",
    "                # alpha_{t+1}(i) = (\\sum_j alpha_{t}(j) A[j, i] ) B[i, k]\n",
    "                alpha[i] = np.sum(alpha_last * A[:, i]) * B[i, k]\n",
    "        alpha_last = alpha.copy()\n",
    "        print(f\"alpha[{t}]: {alpha}\")\n",
    "    return np.sum(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward(A, B, Q, O, V, PI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 后向算法\n",
    "**后向概率**: 给定隐马尔可夫模型$\\lambda$, 定义到时刻$t$状态为$q_i$的条件下, 从$t+1$到$T$的部分观测序列为$o_{t+1}, o_{t+2}, \\cdots, o_T$的概率为后向概率, 记作\n",
    "$$\\beta_t(i) = P(o_{t+1}, o_{t+2}, \\cdots, o_T|i_t=q_i, \\lambda)$$\n",
    "用递推的方法求后向概率$\\beta_t{i}$以及观测序列概率$P(O|\\lambda)$\n",
    "\n",
    "后向概率的动态规划递推公式和前向概率是相反的。现在我们假设我们已经找到了在时刻$t+1$时各个隐藏状态的后向概率$\\beta_{t+1}(j)$，现在我们需要递推出时刻$t$时各个隐藏状态的后向概率。我们可以计算出观测状态的序列为$o_{t+2},o_{t+3},...o_T$， $t$时隐藏状态为$q_i$, 时刻$t+1$隐藏状态为$q_j$的概率为$a_{ij}\\beta_{t+1}(j)$, 接着可以得到观测状态的序列为$o_{t+1},o_{t+2},...o_T$， $t$时隐藏状态为$q_i$, 时刻$t+1$隐藏状态为$q_j$的概率为$a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)$, 则把所有对应的概率加起来，我们可以得到观测状态的序列为$o_{t+1},o_{t+2},...o_T$， $t$时隐藏状态为$q_i$的概率为$\\sum\\limits_{j=1}^{N}a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)$，这个概率即为时刻$t$的后向概率。\n",
    "\n",
    "输入：HMM模型$\\lambda = (A, B, \\pi)$，观测序列$O=(o_1,o_2,...o_T)$\n",
    "\n",
    "输出：观测序列概率$P(O|\\lambda)$\n",
    "\n",
    "1) 初始化时刻$T$的各个隐藏状态后向概率：$$\\beta_T(i) = 1,\\; i=1,2,...N$$\n",
    "\n",
    "2) 递推时刻$T-1,T-2,...1$时刻的后向概率：$$\\beta_{t}(i) = \\sum\\limits_{j=1}^{N}a_{ij}b_j(o_{t+1})\\beta_{t+1}(j),\\; i=1,2,...N$$\n",
    "\n",
    "3) 计算最终结果：$$P(O|\\lambda) = \\sum\\limits_{i=1}^N\\pi_ib_i(o_1)\\beta_1(i)$$\n",
    "\n",
    "此时我们的算法时间复杂度仍然是$O(TN^2)$。\n",
    "\n",
    "利用前向概率和后向概率的定义可以将观测序列概率$P(O|\\lambda)$统一写成:\n",
    "$$P(O|\\lambda) = \\sum_{j=1}^N \\sum_{i=1}^N \\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j), \\quad t=1, 2, \\cdots, T-1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backforwad(A, B, Q, O, V, PI):\n",
    "    N = len(Q)\n",
    "    T = len(O)\n",
    "    for t in range(T-1, -1, -1):\n",
    "        if t == T-1:\n",
    "            beta = np.ones(N)\n",
    "        else:\n",
    "            k = V.index(O[t + 1])  # t时刻观测值o_t对应在观测集合中索引k \n",
    "            for i in range(N):\n",
    "                beta[i] = np.sum(A[i, :] * B[:, k] * beta_last)\n",
    "        print(f'beta[{t}]:, {beta}')\n",
    "        beta_last = beta.copy()\n",
    "    return np.sum(PI * B[:, V.index(O[0])] * beta_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backforwad(A, B, Q, O, V, PI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一些概率与期望值的计算\n",
    "利用前向概率和后向概率，我们可以计算出HMM中单个状态和两个状态的概率公式。\n",
    "\n",
    "1）给定模型$\\lambda$和观测序列$O$,在时刻$t$处于状态$q_i$的概率记为:$$\\gamma_t(i) = P(i_t = q_i | O,\\lambda) = \\frac{P(i_t = q_i ,O|\\lambda)}{P(O|\\lambda)} $$\n",
    "\n",
    "利用前向概率和后向概率的定义可知：$$P(i_t = q_i ,O|\\lambda) = \\alpha_t(i)\\beta_t(i)$$\n",
    "\n",
    "于是我们得到：$$\\gamma_t(i) = \\frac{ \\alpha_t(i)\\beta_t(i)}{\\sum\\limits_{j=1}^N \\alpha_t(j)\\beta_t(j)}$$\n",
    "\n",
    "2）给定模型$\\lambda$和观测序列$O$,在时刻$t$处于状态$q_i$，且时刻$t+1$处于状态$q_j$的概率记为:$$\\xi_t(i,j) = P(i_t = q_i, i_{t+1}=q_j | O,\\lambda) = \\frac{ P(i_t = q_i, i_{t+1}=q_j , O|\\lambda)}{P(O|\\lambda)} $$\n",
    "\n",
    "而$P(i_t = q_i, i_{t+1}=q_j , O|\\lambda)$可以由前向后向概率来表示为:$$P(i_t = q_i, i_{t+1}=q_j , O|\\lambda) = \\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)$$\n",
    "\n",
    "从而最终我们得到$\\xi_t(i,j)$的表达式如下：$$\\xi_t(i,j) = \\frac{\\alpha_t(i)a_{ij}b_j(o_{t+1})\\beta_{t+1}(j)}{\\sum\\limits_{r=1}^N\\sum\\limits_{s=1}^N\\alpha_t(r)a_{rs}b_s(o_{t+1})\\beta_{t+1}(s)}$$\n",
    "\n",
    " 3) 将$\\gamma_t(i)$和$\\xi_t(i,j)$在各个时刻$t$求和，可以得到：\n",
    "\n",
    "在观测序列$O$下状态$i$出现的期望值$\\sum\\limits_{t=1}^T\\gamma_t(i)$\n",
    "\n",
    "在观测序列$O$下由状态$i$转移的期望值$\\sum\\limits_{t=1}^{T-1}\\gamma_t(i)$\n",
    "\n",
    "在观测序列$O$下由状态$i$转移到状态$j$的期望值$\\sum\\limits_{t=1}^{T-1}\\xi_t(i,j)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**习题10.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = [1, 2, 3]\n",
    "V = ['红', '白']\n",
    "A = np.array([[0.5, 0.1, 0.4], [0.3, 0.5, 0.2], [0.2, 0.2, 0.6]])\n",
    "B = np.array([[0.5, 0.5], [0.4, 0.6], [0.7, 0.3]])\n",
    "O = ['红', '白', '红', '红', '白', '红', '白', '白']\n",
    "PI = np.array([0.2, 0.3, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forword_backword(A, B, Q, O, V, PI):\n",
    "    # 前向-后向概率算法\n",
    "    N = len(Q)\n",
    "    T = len(O)\n",
    "    alphas = np.zeros((T, N))\n",
    "    betas = np.zeros((T, N))\n",
    "    gammas = np.zeros((T, N))\n",
    "    for t in range(0, T):\n",
    "        k = V.index(O[t])  # O_t对应的观测状态序号k\n",
    "        t_reverse = T-1-t\n",
    "        if t == 0 :\n",
    "            alphas[0, :] = PI * B[:, k]  # PI[i] * b_i(o_1)  (N, )\n",
    "            betas[t_reverse, :] = np.ones(N)\n",
    "        else:\n",
    "            for i in range(N):\n",
    "                # alpha_{t+1}(i) = (\\sum_j alpha_{t}(j) A[j, i] ) B[i, k]\n",
    "                alphas[t, i] = np.sum(alphas[t-1, :] * A[:, i]) * B[i, k]\n",
    "                betas[t_reverse, i] = np.sum(A[i, :] * B[:, k] * betas[t_reverse+1, :])\n",
    "    for t in range(0, T):\n",
    "        gammas[t, :] = alphas[t, :] * betas[t, :] / np.sum(alphas[t, :] * betas[t, :])\n",
    "    print(f\"alphas:\\n {alphas}\")\n",
    "    print(f\"betas:\\n {betas}\")\n",
    "    print(f\"gammas:\\n {gammas}\")\n",
    "    print(\"P(O|\\lambda):\", np.sum(alphas[-1]), np.sum(PI * B[:, V.index(O[0])] * betas[0,:]))\n",
    "    return gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas = forword_backword(A, B, Q, O, V, PI)\n",
    "# P(i_4=q_3|O)\n",
    "gammas[3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习算法-参数求解\n",
    "### 监督学习方法\n",
    "已知$D$个长度为$T$的观测序列和对应的隐藏状态序列，即$\\{(O_1, I_1), (O_2, I_2), ...(O_D, I_D)\\}$是已知的，此时我们可以很容易的用**极大似然法**来求解模型参数。\n",
    "\n",
    "1. 转移概率$a_{ij}$估计  \n",
    "假设样本从隐藏状态$q_i$转移到$q_j$的频率计数是$A_{ij}$,那么状态转移矩阵求得为：$$A = \\Big[\\hat a_{ij}\\Big], \\;其中\\hat a_{ij} = \\frac{A_{ij}}{\\sum\\limits_{s=1}^{N}A_{is}}$$\n",
    "2. 观测概率$b_j(k)$  \n",
    "假设样本隐藏状态为$q_j$且观测状态为$v_k$的频率计数是$B_{jk}$,那么观测状态概率矩阵为：$$B= \\Big[\\hat b_{j}(k)\\Big], \\;其中\\hat b_{j}(k) = \\frac{B_{jk}}{\\sum\\limits_{s=1}^{M}B_{js}}$$\n",
    "3. 初始状态概率估计  \n",
    "假设所有样本中初始隐藏状态为$q_i$的频率计数为$C(i)$,那么初始概率分布为：$$\\Pi = \\pi(i) = \\frac{C(i)}{\\sum\\limits_{s=1}^{N}C(s)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baum-Wecle 算法\n",
    "在很多时候，我们无法得到HMM样本观察序列对应的隐藏序列，只有$D$个长度为$T$的观测序列$\\{(O_1), (O_2), ...(O_D)\\}$而没有对应的状态序列$\\{(I_1), (I_2), \\cdots, (I_D)\\}$, 目标是学习隐马尔可夫模型参数$\\lambda=(A, B, \\pi)$, 将可观测的序列数据看做$O$, 状态序列数据看做不可观测的隐数据$I$, 那么隐马尔可夫模型事实上式一个含有隐变量的概率模型\n",
    "$$P(O|\\lambda) = \\sum_I P(O|I, \\lambda)P(I|\\lambda)$$\n",
    "它的参数学习可以由EM算法实现.\n",
    "鲍姆-韦尔奇算法原理既然使用的就是EM算法的原理，那么我们需要在E步求出联合分布$P(O,I|\\lambda)$基于条件概率$P(I|O,\\overline{\\lambda})$的期望，其中$\\overline{\\lambda}$为当前的模型参数，然后再M步最大化这个期望，得到更新的模型参数$\\lambda$。接着不停的进行EM迭代，直到模型参数的值收敛为止。\n",
    "\n",
    "1 E步，当前模型参数为$\\overline{\\lambda}$, 联合分布$P(O,I|\\lambda)$基于条件概率$P(I|O,\\overline{\\lambda})$的期望表达式为：$$Q(\\lambda, \\overline{\\lambda}) = \\sum\\limits_{I}P(I|O,\\overline{\\lambda})logP(O,I|\\lambda)$$\n",
    "训练数据为$\\{(O_1, I_1), (O_2, I_2), ...(O_D, I_D)\\}$, 其中任意一个观测序列$O_d = \\{o_1^{(d)}, o_2^{(d)}, ... o_T^{(d)}\\}$对应的隐藏的状态序列$I_d = \\{i_1^{(d)}, i_2^{(d)}, ... i_T^{(d)}\\}$,完全数据的联合概率分布可以表示成\n",
    "$$P(O,I|\\lambda) = \\prod_{d=1}^D\\pi_{i_1^{(d)}}b_{i_1^{(d)}}(o_1^{(d)})a_{i_1^{(d)}i_2^{(d)}}b_{i_2^{(d)}}(o_2^{(d)})...a_{i_{T-1}^{(d)}i_T^{(d)}}b_{i_T^{(d)}}(o_T^{(d)})$$\n",
    "那么$Q$函数可以写成\n",
    "$$Q(\\lambda, \\overline\\lambda) =\\sum\\limits_{d=1}^D\\sum\\limits_{I}P(O,I|\\overline{\\lambda})(log\\pi_{i_1^{(d)}} + \\sum\\limits_{t=1}^{T-1}log\\;a_{i_t^{(d)},i_{t+1}^{(d)}} +  \\sum\\limits_{t=1}^Tlog b_{i_t^{(d)}}(o_t^{(d)}))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 M步，我们极大化$Q$函数，然后得到更新后的模型参数如下：　$$\\overline{\\lambda} = arg\\;\\max_{\\lambda}\\sum\\limits_{d=1}^D\\sum\\limits_{I}P(O,I|\\overline{\\lambda})(log\\pi_{i_1^{(d)}} + \\sum\\limits_{t=1}^{T-1}log\\;a_{i_t^{(d)},i_{t+1}^{(d)}} +  \\sum\\limits_{t=1}^Tlog b_{i_t^{(d)}}(o_t^{(d)}))$$\n",
    "\n",
    "(1) 首先求参数模型中的$\\pi$, $Q$函数对$\\pi_i$求导,得到$$\\sum\\limits_{d=1}^D\\sum\\limits_{I}P(O,I|\\overline{\\lambda})log\\pi_{i_1^{(d)}}$$\n",
    "使用全概率公式进行化简:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{d=1}^D\\sum_{I}P(O,I|\\overline\\lambda)log\\pi_{i_1^{(d)}} &= \\sum_{d=1}^D\\sum_{i_1^{(d)}}\\sum_{i_2^{(d)}}\\cdots\\sum_{i_T^{(d)}}P(O,i_1^{(d)},i_2^{(d)},\\cdots,i_T^{(d)}|\\overline\\lambda)log\\pi_{i_1^{(d)}}  \\\\ \n",
    "&= \\sum_{d=1}^D\\sum_{i_1^{(d)}}P(O, i_1^{(d)}|\\overline\\lambda)log\\pi_{i_1^{(d)}} \\\\\n",
    "& 因为i_1有N种可能取值\\{q_1, q_2, \\cdots, q_N\\}, 所以 \\\\\n",
    "&= \\sum_{d=1}^D\\sum_{j=1}^NP(O, i_1^{(d)}=q_j|\\overline\\lambda)log\\pi_{i_1^{(d)} = q_j} \\\\ \n",
    "&= \\sum_{d=1}^D\\sum_{j=1}^NP(O, i_1^{(d)}=q_j|\\overline\\lambda)log\\pi_j\n",
    "\\end{align}\n",
    "$$\n",
    "所以\n",
    "$$\\overline{\\pi_i} = arg\\;\\max_{\\pi_{i}} \\sum\\limits_{d=1}^D\\sum\\limits_{j=1}^NP(O,i_1^{(d)} =j|\\overline{\\lambda})log\\pi_{i}$$\n",
    "由于$\\pi_i$还满足约束条件$\\sum_{i=1}^N \\pi_i = 1$, 使用拉格朗日乘子法:\n",
    "$$arg\\;\\max_{\\pi_{i}}\\sum\\limits_{d=1}^D\\sum\\limits_{i=1}^NP(O,i_1^{(d)} =i|\\overline{\\lambda})log\\pi_{i} + \\gamma(\\sum\\limits_{i=1}^N\\pi_i -1)$$\n",
    "对$\\pi_i$求导, 再令其为零, 得到\n",
    "$$\\sum\\limits_{d=1}^DP(O,i_1^{(d)} =i|\\overline{\\lambda}) + \\gamma\\pi_i = 0$$\n",
    "令i分别等于从1到N，从上式可以得到N个式子，对这N个式子求和可得：\n",
    "$$\\sum\\limits_{d=1}^DP(O|\\overline{\\lambda}) + \\gamma = 0$$\n",
    "从上两式中消去$\\gamma$得到$\\pi_i$的表达式为:\n",
    "$$\\pi_i =\\frac{\\sum\\limits_{d=1}^DP(O,i_1^{(d)} =i|\\overline{\\lambda})}{\\sum\\limits_{d=1}^DP(O|\\overline{\\lambda})} = \\frac{\\sum\\limits_{d=1}^DP(O,i_1^{(d)} =i|\\overline{\\lambda})}{DP(O|\\overline{\\lambda})} = \\frac{\\sum\\limits_{d=1}^DP(i_1^{(d)} =i|O, \\overline{\\lambda})}{D} =  \\frac{\\sum\\limits_{d=1}^DP(i_1^{(d)} =i|O^{(d)}, \\overline{\\lambda})}{D}$$\n",
    "根据前面的定义\n",
    "$$\\gamma_t(i) = P(i_t = q_i | O,\\lambda) = \\frac{P(i_t = q_i ,O|\\lambda)}{P(O|\\lambda)} $$\n",
    "最终我们在M步$\\pi_i$的迭代公式为\n",
    "$$\\pi_i =  \\frac{\\sum\\limits_{d=1}^D\\gamma_1^{(d)}(i)}{D}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) 根据$Q$函数第二项求状态转移矩阵$A$, 与$\\pi_i$的推导类似, 有\n",
    "$$\\sum\\limits_{d=1}^D\\sum\\limits_{I}\\sum\\limits_{t=1}^{T-1}P(O,I|\\overline{\\lambda})log\\;a_{i_t^{(d)},i_{t+1}^{(d)}} = \\sum\\limits_{d=1}^D\\sum\\limits_{i=1}^N\\sum\\limits_{j=1}^N\\sum\\limits_{t=1}^{T-1}P(O,i_t^{(d)} = i, i_{t+1}^{(d)} = j|\\overline{\\lambda})log\\;a_{ij}$$\n",
    "约束条件为$\\sum_{j=1}^Na_{ij}=1$, 可以用拉格朗日子乘法并对$a_{ij}$求导，并令结果为0，可以得到$a_{ij}$的迭代表达式为：\n",
    "$$a_{ij} = \\frac{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T-1}P(O^{(d)}, i_t^{(d)} = i, i_{t+1}^{(d)} = j|\\overline{\\lambda})}{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T-1}P(O^{(d)}, i_t^{(d)} = i|\\overline{\\lambda})}$$\n",
    "再由前面定义的\n",
    "$$\\xi_t(i,j) = P(i_t = q_i, i_{t+1}=q_j | O,\\lambda) = \\frac{ P(i_t = q_i, i_{t+1}=q_j , O|\\lambda)}{P(O|\\lambda)} $$\n",
    "最终\n",
    "$$a_{ij} = \\frac{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T-1}\\xi_t^{(d)}(i,j)}{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T-1}\\gamma_t^{(d)}(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3)同样地, 第三项:\n",
    "$$\\sum\\limits_{d=1}^D\\sum\\limits_{I}\\sum\\limits_{t=1}^{T}P(O,I|\\overline{\\lambda})log\\;b_{i_t}(o_t) = \\sum\\limits_{d=1}^D\\sum\\limits_{j=1}^N\\sum\\limits_{t=1}^{T}P(O,i_t^{(d)} = j|\\overline{\\lambda})log\\;b_{j}(o_t)$$\n",
    "约束条件为$\\sum_{k=1}^Mb_{j}(k)=1$, 构造拉格朗日函数, 对$b_{j}(k)$求导, 并令导数为零.求得:\n",
    "$$b_{j}(k) = \\frac{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T}P(O,i_t^{(d)} = j|\\overline{\\lambda})I(o_t^{(d)}=v_k)}{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T}P(O,i_t^{(d)} = j|\\overline{\\lambda})}$$\n",
    "其中$I(o_t^{(d)}=v_k)$表示当且仅当$o_t^{(d)}=v_k$时为1, 否则0. 再由前面的定义,可以写成\n",
    "$$b_{j}(k) = \\frac{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1, o_t^{(d)}=v_k}^{T}\\gamma_t^{(d)}(j)}{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T}\\gamma_t^{(d)}(j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**算法流程**\n",
    "\n",
    "输入: $D$个观测序列样本$\\{O_1, O_2, \\cdots, O_D\\}, \\quad O_i=(o_1, o_2, \\cdots, o_T)$\n",
    "\n",
    "输出: HMM模型参数\n",
    "\n",
    "(1) 初始化, 可以随机初始化参数$a_{ij}^{(0)}, b_j(k)^{(0)},\\pi_i^{(0)}$\n",
    "\n",
    "(2) 对$n=1, 2, \\cdots$, 对每个样本$d=1, 2, \\cdots$, 用前向后向算法计算$\\gamma_t^{(d)}(i)，\\xi_t^{(d)}(i,j), t =1,2...T$\n",
    "\n",
    "(3) 递推, 更新模型参数:\n",
    "$$\\pi_i =  \\frac{\\sum\\limits_{d=1}^D\\gamma_1^{(d)}(i)}{D}$$\n",
    "$$a_{ij} = \\frac{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T-1}\\xi_t^{(d)}(i,j)}{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T-1}\\gamma_t^{(d)}(i)}$$\n",
    "$$b_{j}(k) = \\frac{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1, o_t^{(d)}=v_k}^{T}\\gamma_t^{(d)}(j)}{\\sum\\limits_{d=1}^D\\sum\\limits_{t=1}^{T}\\gamma_t^{(d)}(j)}$$\n",
    "(4) 如果参数都已经收敛, 则算法结束, 否则返回到第(2)步继续迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baum_welch(O, V, Q, epsilon=0.0001):\n",
    "    # O: (D, T)含D个长度为T的观测序列\n",
    "    # Q: 状态集合\n",
    "    # V: 观测值集合\n",
    "    N = len(Q)\n",
    "    M = len(V)\n",
    "    D = len(O)\n",
    "    T = len(O[0])\n",
    "    \n",
    "    # 初始化参数\n",
    "    A = np.random.rand(N, N)\n",
    "    B = np.random.rand(N, M)\n",
    "    A = A / np.sum(A, axis=-1)[:, np.newaxis]\n",
    "    B = B / np.sum(B, axis=-1)[:, np.newaxis]\n",
    "    PI = np.ones(N) / N\n",
    "    alphas = np.zeros((D, T, N))\n",
    "    betas = np.zeros((D, T, N))\n",
    "    gammas = np.zeros((D, T, N))\n",
    "    xis = np.zeros((D, T-1, N, N))\n",
    "    print(\"A: \\n\", A)\n",
    "    print(\"B: \\n\", B)\n",
    "    print(\"PI: \\n\", PI)\n",
    "    for n in range(1, 1000):\n",
    "        A_last = A \n",
    "        B_last = B\n",
    "        PI_last = PI\n",
    "        for d in range(D):\n",
    "            for t in range(T):\n",
    "                k = V.index(O[d][t])  # o_t对应的观测状态序号k\n",
    "                t_reverse = T-1-t\n",
    "                if t == 0 :\n",
    "                    alphas[d, 0, :] = PI_last * B_last[:, k]  # PI[i] * b_i(o_1)  (N, )\n",
    "                    betas[d, t_reverse, :] = np.ones(N)\n",
    "                else:\n",
    "                    for i in range(N):\n",
    "                        # alpha_{t+1}(i) = (\\sum_j alpha_{t}(j) A[j, i] ) B[i, k]\n",
    "#                         print(d, t, i, k)\n",
    "#                         print(alphas.shape, A_last.shape, B_last.shape)\n",
    "                        alphas[d, t, i] = np.sum(alphas[d, t-1, :] * A_last[:, i]) * B_last[i, k]\n",
    "                        betas[d, t_reverse, i] = np.sum(A_last[i, :] * B_last[:, k] * betas[d, t_reverse+1, :])\n",
    "            for t in range(T):\n",
    "                temp = alphas[d, t, :] * betas[d, t, :]\n",
    "                gammas[d, t, :] = temp / np.sum(temp)\n",
    "            for t in range(T-1):\n",
    "                for i in range(N):\n",
    "                    for j in range(N):\n",
    "                        k = V.index(O[d][t+1])\n",
    "                        xis[d, t, i, j] = alphas[d, t, i]*A_last[i, j]*B_last[j, k]*betas[d, t+1, j]\n",
    "                xis[d, t] = xis[d, t] / np.sum(xis[d, t])\n",
    "\n",
    "        # (D, T-1, N, N) -> (N, N)\n",
    "        # (D, T-1, N) -> (N, )\n",
    "        A = np.sum(np.sum(xis, axis=1), axis=0) / np.sum(np.sum(gammas[:, :-1, :], axis=1), axis=0)[:, np.newaxis]\n",
    "        \n",
    "        k_t = []  # [0-[] , 1-[], ...\n",
    "        for k in range(M):\n",
    "            k_list = []\n",
    "            for d in range(D):\n",
    "                t_list = [t for t in range(T) if V.index(O[d][t])==k]\n",
    "                k_list.append(t_list)\n",
    "            k_t.append(k_list)\n",
    "        \n",
    "        for k in range(M):\n",
    "            temp = np.zeros(N)\n",
    "            for d in range(D):\n",
    "                temp += np.sum(gammas[d, k_t[k][d], :], axis=1)\n",
    "            B[:, k] = temp / np.sum(np.sum(gammas, axis=1), axis=0)\n",
    "        \n",
    "        # (D, N) -> (N, )\n",
    "        PI = np.sum(gammas[:, 0, :], axis=0) / D\n",
    "        # print(f\"第{n}次迭代\", PI)\n",
    "        # 收敛停止迭代\n",
    "        if np.all(abs(PI-PI_last) < epsilon) and\\\n",
    "            np.all(abs(A-A_last) < epsilon) and\\\n",
    "            np.all(abs(B-B_last) < epsilon):\n",
    "            break\n",
    "    print(\"A: \\n\", A)\n",
    "    print(\"B: \\n\", B)\n",
    "    print(\"PI: \\n\", PI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = [1, 2, 3, 4]\n",
    "V = ['红', '白']\n",
    "O1 = ['红', '白', '红', '红', '白', '红', '白', '白']\n",
    "O2 = ['白', '红', '红', '白', '白', '红', '红', '白']\n",
    "O = [O1, O2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baum_welch(O, V, Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测问题 解码问题\n",
    "已知模型$\\lambda=(A, B, \\pi)$和观测序列$O＝(o_1，o_2,…,o_T)$，求对给定观测序列条件概率$P(I|O)$最大的状态序列$I＝(i_1，i_2,…,i_T)$。维特比算法应用动态规划高效地求解最优路径，即概率最大的状态序列\n",
    "### 近似算法\n",
    "\n",
    "一个可能的近似解法是求出观测序列$O$在每个时刻$t$最可能的隐藏状态$i_t^*$然后得到一个近似的隐藏状态序列$I^*= \\{i_1^*,i_2^*,...i_T^*\\}$。要这样近似求解不难，前向后向算法评估观察序列概率中定义：在给定模型$\\lambda$和观测序列$O$时，在时刻$t$处于状态$q_i$的概率是$\\gamma_t(i)$，这个概率可以通过HMM的前向算法与后向算法计算。\n",
    "$$\\gamma_t(i) = \\frac{ \\alpha_t(i)\\beta_t(i)}{\\sum\\limits_{j=1}^N \\alpha_t(j)\\beta_t(j)}$$\n",
    "这样我们有：$$i_t^* = arg \\max_{1 \\leq i \\leq N}[\\gamma_t(i)], \\; t =1,2,...T$$\n",
    "\n",
    "近似算法很简单，但是却不能保证预测的状态序列是整体是最可能的状态序列，因为预测的状态序列中某些相邻的隐藏状态可能存在转移概率为0的情况, 即对某些$i,j, a_{ij}=0$\n",
    "\n",
    "### 维特比算法\n",
    "维特比算法是一个通用的解码算法，是基于动态规划的求序列最短路径的方法.动态规划算法需要找到合适的局部状态，以及局部状态的递推公式。在HMM中，维特比算法定义了两个局部状态用于递推。\n",
    "第一个局部状态是在时刻$t$隐藏状态为$i$所有可能的状态转移路径$i_1, i_2, \\cdots, i_t$中的概率最大值.\n",
    "$$\\delta_t(i) = \\max_{i_1,i_2,...i_{t-1}}\\;P(i_t=i, i_1,i_2,...i_{t-1},o_t,o_{t-1},...o_1|\\lambda),\\; i =1,2,...N$$\n",
    "由定义可得$\\delta$的递推式:\n",
    "$$\\begin{align} \\delta_{t+1}(i) & =  \\max_{i_1,i_2,...i_{t}}\\;P(i_{t+1}=i, i_1,i_2,...i_{t},o_{t+1},o_{t},...o_1|\\lambda) \\\\ & = \\max_{1 \\leq j \\leq N}\\;[\\delta_t(j)a_{ji}]b_i(o_{t+1})\\end{align}$$\n",
    "\n",
    "定义在时刻$t$隐藏状态为$i$的所有单个状态转移路径中概率最大的第$t-1$个节点的隐藏状态为\n",
    "$$\\Psi_t(i) = arg \\; \\max_{1 \\leq j \\leq N}\\;[\\delta_{t-1}(j)a_{ji}]$$\n",
    "\n",
    "**算法流程**  \n",
    "\n",
    "输入：HMM模型$\\lambda = (A, B, \\Pi)$，观测序列$O=(o_1,o_2,...o_T)$</p>\n",
    "输出：最有可能的隐藏状态序列$I^*= \\{i_1^*,i_2^*,...i_T^*\\}$</p>\n",
    "1）初始化局部状态：$$\\delta_1(i) = \\pi_ib_i(o_1),\\;i=1,2...N$$$$\\Psi_1(i)=0,\\;i=1,2...N$$</p>\n",
    "2) 进行动态规划递推时刻$t=2,3,...T$时刻的局部状态：$$\\delta_{t}(i) = \\max_{1 \\leq j \\leq N}\\;[\\delta_{t-1}(j)a_{ji}]b_i(0_{t}),\\;i=1,2...N$$$$\\Psi_t(i) = arg \\; \\max_{1 \\leq j \\leq N}\\;[\\delta_{t-1}(j)a_{ji}],\\;i=1,2...N$$</p>\n",
    "3) 计算时刻$T$最大的$\\delta_{T}(i)$,即为最可能隐藏状态序列出现的概率。计算时刻$T$最大的$\\Psi_t(i)$,即为时刻$T$最可能的隐藏状态。$$P* = \\max_{1 \\leq j \\leq N}\\delta_{T}(i)$$$$i_T^* = arg \\; \\max_{1 \\leq j \\leq N}\\;[\\delta_{T}(i)]$$</p>\n",
    "4) 利用局部状态$\\Psi(i)$开始回溯。对于$t=T-1,T-2,...,1$：$$i_t^* = \\Psi_{t+1}(i_{t+1}^*)$$</p>\n",
    "最终得到最有可能的隐藏状态序列$I^*= \\{i_1^*,i_2^*,...i_T^*\\}$</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(A, B, PI, O, V, Q):\n",
    "    N = len(Q)  # 隐藏状态 集合长度\n",
    "    M = len(V)  # 观测状态 集合长度\n",
    "    T = len(O)  # 观测状态 序列长度\n",
    "    delta = np.zeros((T, N))\n",
    "    Psi = np.zeros((T, N))\n",
    "    i_path = np.zeros(T, dtype=int)  # 最佳路径\n",
    "    for t in range(T):\n",
    "        k = V.index(O[t])  # t时刻的观测状态的序号\n",
    "        if t == 0:\n",
    "            delta[t] = PI * B[:, t]\n",
    "            Psi[t] = 0\n",
    "        else:\n",
    "            for i in range(N):\n",
    "                delta[t, i] = np.max(delta[t-1, :] * A[:, i] * B[i, k])\n",
    "                Psi[t, i] = np.argmax(delta[t-1, :] * A[:, i])\n",
    "#         print(Psi[t])\n",
    "#         print(delta[t])\n",
    "    max_P = np.max(delta[-1, :])\n",
    "    i_path[-1] = np.argmax(delta[-1, :]) \n",
    "    # print(i_path)\n",
    "    for t in range(T-2, -1, -1):\n",
    "        i_path[t] = Psi[t+1, i_path[t+1]] \n",
    "    return max_P, i_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例10.3\n",
    "Q = [1, 2, 3]\n",
    "V = ['红', '白']\n",
    "A = np.array([[0.5, 0.2, 0.3], [0.3, 0.5, 0.2], [0.2, 0.3, 0.5]])\n",
    "B = np.array([[0.5, 0.5], [0.4, 0.6], [0.7, 0.3]])\n",
    "# O = ['红', '白', '红', '红', '白', '红', '白', '白']\n",
    "O = ['红', '白', '红']\n",
    "PI = np.array([0.2, 0.4, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_P, path = viterbi(A, B, PI, O, V, Q)\n",
    "max_P, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#习题10.1\n",
    "Q = [1, 2, 3]\n",
    "V = ['红', '白']\n",
    "A = np.array([[0.5, 0.2, 0.3], [0.3, 0.5, 0.2], [0.2, 0.3, 0.5]])\n",
    "B = np.array([[0.5, 0.5], [0.4, 0.6], [0.7, 0.3]])\n",
    "# O = ['红', '白', '红', '红', '白', '红', '白', '白']\n",
    "O = ['红', '白', '红', '白']    #习题10.1的例子\n",
    "PI = np.array([0.2, 0.4, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_P, path = viterbi(A, B, PI, O, V, Q)\n",
    "max_P, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenMarkov:\n",
    "    def forward(self, Q, V, A, B, O, PI):  # 使用前向算法\n",
    "        N = len(Q)  #可能存在的状态数量\n",
    "        M = len(O)  # 观测序列的大小\n",
    "        alphas = np.zeros((N, M))  # alpha值\n",
    "        T = M  # 有几个时刻，有几个观测序列，就有几个时刻\n",
    "        for t in range(T):  # 遍历每一时刻，算出alpha值\n",
    "            indexOfO = V.index(O[t])  # 找出序列对应的索引\n",
    "            for i in range(N):\n",
    "                if t == 0:  # 计算初值\n",
    "                    alphas[i][t] = PI[t][i] * B[i][indexOfO]  # P176（10.15）\n",
    "                    print(\n",
    "                        'alpha1(%d)=p%db%db(o1)=%f' % (i, i, i, alphas[i][t]))\n",
    "                else:\n",
    "                    alphas[i][t] = np.dot(\n",
    "                        [alpha[t - 1] for alpha in alphas],\n",
    "                        [a[i] for a in A]) * B[i][indexOfO]  # 对应P176（10.16）\n",
    "                    print('alpha%d(%d)=[sigma alpha%d(i)ai%d]b%d(o%d)=%f' %\n",
    "                          (t, i, t - 1, i, i, t, alphas[i][t]))\n",
    "                    # print(alphas)\n",
    "        P = np.sum([alpha[M - 1] for alpha in alphas])  # P176(10.17)\n",
    "        # alpha11 = pi[0][0] * B[0][0]    #代表a1(1)\n",
    "        # alpha12 = pi[0][1] * B[1][0]    #代表a1(2)\n",
    "        # alpha13 = pi[0][2] * B[2][0]    #代表a1(3)\n",
    "\n",
    "    def backward(self, Q, V, A, B, O, PI):  # 后向算法\n",
    "        N = len(Q)  # 可能存在的状态数量\n",
    "        M = len(O)  # 观测序列的大小\n",
    "        betas = np.ones((N, M))  # beta\n",
    "        for i in range(N):\n",
    "            print('beta%d(%d)=1' % (M, i))\n",
    "        for t in range(M - 2, -1, -1):\n",
    "            indexOfO = V.index(O[t + 1])  # 找出序列对应的索引\n",
    "            for i in range(N):\n",
    "                betas[i][t] = np.dot(\n",
    "                    np.multiply(A[i], [b[indexOfO] for b in B]),\n",
    "                    [beta[t + 1] for beta in betas])\n",
    "                realT = t + 1\n",
    "                realI = i + 1\n",
    "                print(\n",
    "                    'beta%d(%d)=[sigma a%djbj(o%d)]beta%d(j)=(' %\n",
    "                    (realT, realI, realI, realT + 1, realT + 1),\n",
    "                    end='')\n",
    "                for j in range(N):\n",
    "                    print(\n",
    "                        \"%.2f*%.2f*%.2f+\" % (A[i][j], B[j][indexOfO],\n",
    "                                             betas[j][t + 1]),\n",
    "                        end='')\n",
    "                print(\"0)=%.3f\" % betas[i][t])\n",
    "        # print(betas)\n",
    "        indexOfO = V.index(O[0])\n",
    "        P = np.dot(\n",
    "            np.multiply(PI, [b[indexOfO] for b in B]),\n",
    "            [beta[0] for beta in betas])\n",
    "        print(\"P(O|lambda)=\", end=\"\")\n",
    "        for i in range(N):\n",
    "            print(\n",
    "                \"%.1f*%.1f*%.5f+\" % (PI[0][i], B[i][indexOfO], betas[i][0]),\n",
    "                end=\"\")\n",
    "        print(\"0=%f\" % P)\n",
    "\n",
    "    def viterbi(self, Q, V, A, B, O, PI):\n",
    "        N = len(Q)  #可能存在的状态数量\n",
    "        M = len(O)  # 观测序列的大小\n",
    "        deltas = np.zeros((N, M))\n",
    "        psis = np.zeros((N, M))\n",
    "        I = np.zeros((1, M))\n",
    "        for t in range(M):\n",
    "            realT = t + 1\n",
    "            indexOfO = V.index(O[t])  # 找出序列对应的索引\n",
    "            for i in range(N):\n",
    "                realI = i + 1\n",
    "                if t == 0:\n",
    "                    deltas[i][t] = PI[0][i] * B[i][indexOfO]\n",
    "                    psis[i][t] = 0\n",
    "                    print('delta1(%d)=pi%d * b%d(o1)=%.2f * %.2f=%.2f' %\n",
    "                          (realI, realI, realI, PI[0][i], B[i][indexOfO],\n",
    "                           deltas[i][t]))\n",
    "                    print('psis1(%d)=0' % (realI))\n",
    "                else:\n",
    "                    deltas[i][t] = np.max(\n",
    "                        np.multiply([delta[t - 1] for delta in deltas],\n",
    "                                    [a[i] for a in A])) * B[i][indexOfO]\n",
    "                    print(\n",
    "                        'delta%d(%d)=max[delta%d(j)aj%d]b%d(o%d)=%.2f*%.2f=%.5f'\n",
    "                        % (realT, realI, realT - 1, realI, realI, realT,\n",
    "                           np.max(\n",
    "                               np.multiply([delta[t - 1] for delta in deltas],\n",
    "                                           [a[i] for a in A])), B[i][indexOfO],\n",
    "                           deltas[i][t]))\n",
    "                    psis[i][t] = np.argmax(\n",
    "                        np.multiply(\n",
    "                            [delta[t - 1] for delta in deltas],\n",
    "                            [a[i]\n",
    "                             for a in A])) + 1  #由于其返回的是索引，因此应+1才能和正常的下标值相符合。\n",
    "                    print('psis%d(%d)=argmax[delta%d(j)aj%d]=%d' %\n",
    "                          (realT, realI, realT - 1, realI, psis[i][t]))\n",
    "        print(deltas)\n",
    "        print(psis)\n",
    "        I[0][M - 1] = np.argmax([delta[M - 1] for de`lta in deltas\n",
    "                                 ]) + 1  #由于其返回的是索引，因此应+1才能和正常的下标值相符合。\n",
    "        print('i%d=argmax[deltaT(i)]=%d' % (M, I[0][M - 1]))\n",
    "        for t in range(M - 2, -1, -1):\n",
    "            I[0][t] = psis[int(I[0][t + 1]) - 1][t + 1]\n",
    "            print('i%d=psis%d(i%d)=%d' % (t + 1, t + 2, t + 2, I[0][t]))\n",
    "        print(\"状态序列I：\", I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考: [隐马尔科夫模型HMM](https://www.cnblogs.com/pinard/p/6945257.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "参考代码：https://blog.csdn.net/tudaodiaozhale\n",
    "\n",
    "中文注释制作：机器学习初学者\n",
    "\n",
    "微信公众号：ID:ai-start-com\n",
    "\n",
    "配置环境：python 3.5+\n",
    "\n",
    "代码全部测试通过。\n",
    "![gongzhong](../gongzhong.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
