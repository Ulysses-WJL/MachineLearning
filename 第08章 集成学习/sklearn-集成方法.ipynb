{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging meta-estimator(Bagging 元估计器)\n",
    "---\n",
    "在集成算法中，bagging 方法会在原始训练集的随机子集上构建一类黑盒估计器的多个实例，然后把这些估计器的预测结果结合起来形成最终的预测结果。 该方法通过在构建模型的过程中引入随机性，来减少基估计器的**方差**(例如，决策树)。 在多数情况下，bagging 方法提供了一种非常简单的方式来对单一模型进行改进，而无需修改背后的算法。 因为 bagging 方法可以减小过拟合，所以通常在强分类器和复杂模型上使用时表现的很好（例如，完全生长的决策树，fully developed decision trees），相比之下 boosting 方法则在弱模型上表现更好（例如，浅层决策树，shallow decision trees）。\n",
    "\n",
    "bagging 方法有很多种，其主要区别在于随机抽取训练子集的方法不同：\n",
    "\n",
    "- 如果抽取的数据集的随机子集是样例的随机子集，我们叫做粘贴 (Pasting) [B1999] 。\n",
    "- 如果样例抽取是有放回的，我们称为 Bagging [B1996] 。\n",
    "- 如果抽取的数据集的随机子集是特征的随机子集，我们叫做**随机子空间** (Random Subspaces) [H1998] 。\n",
    "- 最后，如果基估计器构建在对于样本和特征抽取的子集之上时，我们叫做随机补丁 (Random Patches) [LG2012]。\n",
    "\n",
    "在 scikit-learn 中，bagging 方法使用统一的`BaggingClassifier` 元估计器（或者 `BaggingRegressor` ），基估计器和随机子集抽取策略由用户指定。`max_samples` 和` max_features` 控制着子集的大小（对于样例和特征）， `bootstrap`和 `bootstrap_features `控制着样例和特征的抽取是有放回还是无放回的。 当使用样本子集时，通过设置 `oob_score=True `，可以使用袋外(out-of-bag)样本来评估泛化精度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Single estimator versus bagging: bias-variance decomposition](https://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Gilles Louppe <g.louppe@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Settings\n",
    "n_repeat = 50       # Number of iterations for computing expectations\n",
    "n_train = 50        # Size of the training set\n",
    "n_test = 1000       # Size of the test set\n",
    "noise = 0.1         # Standard deviation of the noise\n",
    "np.random.seed(0)\n",
    "\n",
    "# Change this for exploring the bias-variance decomposition of other\n",
    "# estimators. This should work well for estimators with high variance (e.g.,\n",
    "# decision trees or KNN), but poorly for estimators with low variance (e.g.,\n",
    "# linear models).\n",
    "estimators = [(\"Tree\", DecisionTreeRegressor()),  # 单一决策树回归\n",
    "              (\"Bagging(Tree)\", BaggingRegressor(DecisionTreeRegressor()))  # 多个决策树Bagging\n",
    "             ]\n",
    "\n",
    "n_estimators = len(estimators)\n",
    "\n",
    "\n",
    "# Generate data\n",
    "def f(x):\n",
    "    x = x.ravel()\n",
    "\n",
    "    return np.exp(-x ** 2) + 1.5 * np.exp(-(x - 2) ** 2)\n",
    "\n",
    "\n",
    "def generate(n_samples, noise, n_repeat=1):\n",
    "    \"\"\"\n",
    "    生成数据\n",
    "    X (-5, 5) 之间正态分布\n",
    "    y = f(x) + noise\n",
    "    repeat: 多个数据集 X相同 y不同\n",
    "    \"\"\"\n",
    "    X = np.random.rand(n_samples) * 10 - 5\n",
    "    X = np.sort(X)\n",
    "\n",
    "    if n_repeat == 1:\n",
    "        y = f(X) + np.random.normal(0.0, noise, n_samples)\n",
    "    else:\n",
    "        y = np.zeros((n_samples, n_repeat))\n",
    "\n",
    "        for i in range(n_repeat):\n",
    "            y[:, i] = f(X) + np.random.normal(0.0, noise, n_samples)\n",
    "\n",
    "    X = X.reshape((n_samples, 1))\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(n_repeat):\n",
    "    X, y = generate(n_samples=n_train, noise=noise)\n",
    "    X_train.append(X)\n",
    "    y_train.append(y)\n",
    "\n",
    "# X_test (n_test, 1)  y_test (n_test, n_repeat) 同一个测试X, n_repeat种噪声 \n",
    "# X_train (n_train, 1)* n_repeat  y_train (n_train,) * n_repeat\n",
    "X_test, y_test = generate(n_samples=n_test, noise=noise, n_repeat=n_repeat)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Loop over estimators to compare\n",
    "for n, (name, estimator) in enumerate(estimators):\n",
    "    # Compute predictions\n",
    "    y_predict = np.zeros((n_test, n_repeat))\n",
    "    \n",
    "    # 每个学习器 X_train, y_train都不相同, 测试用的X_test相同 得到n_repeat个模型的预测结果\n",
    "    for i in range(n_repeat):\n",
    "        estimator.fit(X_train[i], y_train[i])\n",
    "        y_predict[:, i] = estimator.predict(X_test)\n",
    "    \n",
    "    # y_predict (n_test, n_repeat)\n",
    "    \n",
    "    # 偏差^2 + 方差 + 噪声 = MSE 均方根误差\n",
    "    # Bias^2 + Variance + Noise decomposition of the mean squared error\n",
    "    y_error = np.zeros(n_test)\n",
    "    \n",
    "    # MSE\n",
    "    # y_error_i = \\sum_{j=1}^{n_repeat} \\sum_{k=1}^{n_repeat} (y_test[i, k] - y_predict[i, j])^2 \n",
    "    for i in range(n_repeat):  \n",
    "        for j in range(n_repeat):\n",
    "            y_error += (y_test[:, j] - y_predict[:, i]) ** 2\n",
    "    \n",
    "    # 一个 样本点 计算了 n_repeat * n_repeat个error值\n",
    "    y_error /= (n_repeat * n_repeat)\n",
    "    \n",
    "    # 相同x ,不同噪声生成的n_repeat种y\n",
    "    y_noise = np.var(y_test, axis=1)\n",
    "    \n",
    "    # 偏差 预测值偏离实际值的值\n",
    "    y_bias = (f(X_test) - np.mean(y_predict, axis=1)) ** 2\n",
    "    \n",
    "    # 多个模型预测值的方差\n",
    "    y_var = np.var(y_predict, axis=1)\n",
    "\n",
    "    print(\"{0}: {1:.4f} (error) = {2:.4f} (bias^2) \"\n",
    "          \" + {3:.4f} (var) + {4:.4f} (noise)\".format(name,\n",
    "                                                      np.mean(y_error),\n",
    "                                                      np.mean(y_bias),\n",
    "                                                      np.mean(y_var),\n",
    "                                                      np.mean(y_noise)))\n",
    "\n",
    "    # Plot figures\n",
    "    plt.subplot(2, n_estimators, n + 1)\n",
    "    plt.plot(X_test, f(X_test), \"b\", label=\"$f(x)$\")\n",
    "    plt.plot(X_train[0], y_train[0], \".b\", label=\"LS ~ $y = f(x)+noise$\")\n",
    "    \n",
    "    # $\\^y(x) 不同模型的预测值\n",
    "    for i in range(n_repeat):\n",
    "        if i == 0:\n",
    "            plt.plot(X_test, y_predict[:, i], \"r\", label=r\"$\\^y(x)$\")\n",
    "        else:\n",
    "            plt.plot(X_test, y_predict[:, i], \"r\", alpha=0.05)\n",
    "    \n",
    "    # E_ls \\^y(x) n_repeat模型的预测值均值\n",
    "    plt.plot(X_test, np.mean(y_predict, axis=1), \"c\",\n",
    "             label=r\"$\\mathbb{E}_{LS} \\^y(x)$\")\n",
    "\n",
    "    plt.xlim([-5, 5])\n",
    "    plt.title(name)\n",
    "\n",
    "    if n == n_estimators - 1:\n",
    "        plt.legend(loc=(1.1, .5))\n",
    "\n",
    "    plt.subplot(2, n_estimators, n_estimators + n + 1)\n",
    "    plt.plot(X_test, y_error, \"r\", label=\"$error(x)$\")\n",
    "    plt.plot(X_test, y_bias, \"b\", label=\"$bias^2(x)$\"),\n",
    "    plt.plot(X_test, y_var, \"g\", label=\"$variance(x)$\"),\n",
    "    plt.plot(X_test, y_noise, \"c\", label=\"$noise(x)$\")\n",
    "\n",
    "    plt.xlim([-5, 5])\n",
    "    plt.ylim([0, 0.1])\n",
    "\n",
    "    if n == n_estimators - 1:\n",
    "\n",
    "        plt.legend(loc=(1.1, .5))\n",
    "\n",
    "plt.subplots_adjust(right=.75)\n",
    "plt.savefig('single_vs_bagging.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果看, 2者之间最明显的差别就是使用bagging方法后方差显著减小了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 由随机树组成的森林\n",
    "\n",
    "## RandomForest 随机森林\n",
    "在随机森林中，集成模型中的每棵树构建时的样本都是由训练集经过有放回抽样得来的。\n",
    "\n",
    "另外，在构建树的过程中进行结点分割时，选择的分割点是所有特征的最佳分割点，或特征的大小为 max_features 的随机子集的最佳分割点。\n",
    "\n",
    "这两种随机性的目的是降低估计器的方差。的确，单棵决策树通常具有高方差，容易过拟合。随机森林构建过程的随机性能够产生具有不同预测错误的决策树。通过取这些决策树的平均，能够消除部分错误。随机森林虽然能够通过组合不同的树降低方差，但是有时会略微增加偏差。在实际问题中，方差的降低通常更加显著，所以随机森林能够取得更好地效果。\n",
    "\n",
    "scikit-learn 的实现是取每个分类器预测概率的平均，而不是让每个分类器对类别进行投票。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExtraTrees极限随机树 \n",
    "在极限随机树中（参见 ExtraTreesClassifier 和 ExtraTreesRegressor 类)， 计算分割点方法中的随机性进一步增强。 与随机森林相同，使用的特征是候选特征的随机子集；但是不同于随机森林寻找**最具有区分度**的阈值，这里的阈值是针对每个候选特征**随机生成**的，并且选择这些随机生成的阈值中的最佳者作为分割规则, 即每个特征随机生成分割阈值, 再从这些阈值中选取最优的。 这种做法通常能够减少一点模型的方差，代价则是略微地增大偏差："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数\n",
    "主要需要注意的参数\n",
    "- `n_estimators`: 森林里树的数量，通常数量越大，效果越好，但是计算时间也会随之增加。 此外要注意，当树的数量超过一个临界值之后，算法的效果并不会很显著地变好\n",
    "- `max_features`: 分割节点时考虑的特征的随机子集的大小。 这个值越低，方差减小得越多，但是偏差的增大也越多。 根据经验，回归问题中使用 max_features = None （总是考虑所有的特征）， 分类问题使用 max_features = \"sqrt\" （随机考虑 sqrt(n_features) 特征，其中 n_features 是特征的个数）是比较好的默认值\n",
    "- `max_depth = None` 和 `min_samples_split = 2` 结合通常会有不错的效果（即生成完全的树）。 请记住，这些（默认）值通常不是最佳的，同时还可能消耗大量的内存，最佳参数值应由交叉验证获得\n",
    "- `bootstrap`: 在随机森林中，默认使用自助采样法(True), 然而 extra-trees 的默认策略是使用整个数据集（bootstrap = False）\n",
    "- `oob_score`: 当使用自助采样法方法抽样时，泛化精度是可以通过剩余的或者袋外的样本来估算的，设置 oob_score = True 即可实现\n",
    "- `n_jobs`: 并行化, 默认n_jobs=-1 使用所有内核. 在建立大量的树，或者构建单个树需要相当长的时间时, 通过并行化可以实现显著的加速\n",
    "\n",
    "默认参数下模型复杂度是：O(M*N*log(N)) ， 其中 M 是树的数目， N 是样本数。 可以通过设置以下参数来降低模型复杂度： min_samples_split , max_leaf_nodes , max_depth 和 min_samples_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature_importances_ 特征重要性评估\n",
    "\n",
    "特征对目标变量预测的相对重要性可以通过（树中的决策节点的）特征使用的**相对顺序**（即深度）来进行评估. 决策树顶部使用的特征对更大一部分输入样本的最终预测决策做出贡献；因此，可以使用接受每个特征对最终预测的贡献的样本比例来评估该 特征的相对重要性 。scikit-learn通过将特征贡献的样本比例与纯度减少相结合得到特征的重要性。\n",
    "\n",
    "通过对多个随机树中的 预期贡献率 （expected activity rates） 取平均，可以减少这种估计的 方差 ，并将其用于特征选择。这被称作平均纯度减少，或MDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "clf = RandomForestClassifier(n_estimators=20, random_state=0)\n",
    "clf.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例: [Plot the decision surfaces of ensembles of trees on the iris dataset](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html#sphx-glr-auto-examples-ensemble-plot-forest-iris-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
    "AdaBoostClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Parameters\n",
    "n_classes = 3\n",
    "n_estimators = 30\n",
    "cmap = plt.cm.RdYlBu\n",
    "plot_step = 0.02 # fine step width for decision surface contours\n",
    "plot_step_coarser = 0.5 # step widths for coarse classifier guesses\n",
    "RANDOM_SEED = 13 # fix the seed on each iteration\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "plot_idx = 1\n",
    "\n",
    "models = [DecisionTreeClassifier(max_depth=None),\n",
    "            RandomForestClassifier(n_estimators=n_estimators),\n",
    "            ExtraTreesClassifier(n_estimators=n_estimators),\n",
    "            AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),\n",
    "            n_estimators=n_estimators)]\n",
    "for pair in ([0, 1], [0, 2], [2, 3]):\n",
    "    for model in models:\n",
    "        # We only take the two corresponding features\n",
    "        X = iris.data[:, pair]\n",
    "        y = iris.target\n",
    "        # Shuffle\n",
    "        idx = np.arange(X.shape[0])\n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        np.random.shuffle(idx)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "        # Standardize\n",
    "        mean = X.mean(axis=0)\n",
    "        std = X.std(axis=0)\n",
    "        X = (X - mean) / std\n",
    "        # Train\n",
    "        model.fit(X, y)\n",
    "        scores = model.score(X, y)\n",
    "        # Create a title for each column and the console by using str() and\n",
    "        # slicing away useless parts of the string\n",
    "        model_title = str(type(model)).split(\n",
    "        \".\")[-1][:-2][:-len(\"Classifier\")]\n",
    "        model_details = model_title\n",
    "        if hasattr(model, \"estimators_\"):\n",
    "            model_details += \" with {} estimators\".format(\n",
    "                len(model.estimators_))\n",
    "        print(model_details + \" with features\", pair,\n",
    "            \"has a score of\", scores)\n",
    "              \n",
    "              \n",
    "        plt.subplot(3, 4, plot_idx)\n",
    "        if plot_idx <= len(models):\n",
    "        # Add a title at the top of each column\n",
    "            plt.title(model_title, fontsize=9)\n",
    "        # Now plot the decision boundary using a fine mesh as input to a\n",
    "        # filled contour plot\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "        np.arange(y_min, y_max, plot_step))\n",
    "        # Plot either a single DecisionTreeClassifier or alpha blend the\n",
    "        # decision surfaces of the ensemble of classifiers\n",
    "        if isinstance(model, DecisionTreeClassifier):\n",
    "            Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = Z.reshape(xx.shape)\n",
    "            cs = plt.contourf(xx, yy, Z, cmap=cmap)\n",
    "        else:\n",
    "        # Choose alpha blend level with respect to the number\n",
    "        # of estimators\n",
    "        # that are in use (noting that AdaBoost can use fewer estimators\n",
    "        # than its maximum if it achieves a good enough fit early on)\n",
    "            estimator_alpha = 1.0 / len(model.estimators_)\n",
    "            for tree in model.estimators_:\n",
    "                Z = tree.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "                Z = Z.reshape(xx.shape)\n",
    "                cs = plt.contourf(xx, yy, Z, alpha=estimator_alpha, cmap=cmap)\n",
    "        # Build a coarser grid to plot a set of ensemble classifications\n",
    "        # to show how these are different to what we see in the decision\n",
    "        # surfaces. These points are regularly space and do not have a\n",
    "        # black outline\n",
    "        xx_coarser, yy_coarser = np.meshgrid(\n",
    "            np.arange(x_min, x_max, plot_step_coarser),\n",
    "            np.arange(y_min, y_max, plot_step_coarser))\n",
    "        Z_points_coarser = model.predict(np.c_[xx_coarser.ravel(),\n",
    "                                        yy_coarser.ravel()]\n",
    "                                        ).reshape(xx_coarser.shape)\n",
    "        cs_points = plt.scatter(xx_coarser, yy_coarser, s=15,\n",
    "                                c=Z_points_coarser, cmap=cmap,\n",
    "                                edgecolors=\"none\")\n",
    "        # Plot the training points, these are clustered together and have a\n",
    "        # black outline\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y,\n",
    "                    cmap=ListedColormap(['r', 'y', 'b']),\n",
    "                    edgecolor='k', s=20)\n",
    "        plot_idx += 1 # move on to the next plot in sequence\n",
    "plt.suptitle(\"Classifiers on feature subsets of the Iris dataset\", fontsize=12)\n",
    "plt.axis(\"tight\")\n",
    "plt.tight_layout(h_pad=0.2, w_pad=0.2, pad=2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# AdaBoost自适应提升方法\n",
    "\n",
    "核心思想: 反复修改的数据（校对者注：主要是修正数据的权重）来训练一系列的弱学习器(一个弱学习器模型仅仅比随机猜测好一点, 比如一个简单的决策树),由这些弱学习器的预测结果通过加权投票(或加权求和)的方式组合, 得到我们最终的预测结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### sklearn.ensemble.AdaBoostClassifier\n",
    "\n",
    "- algorithm：这个参数只有AdaBoostClassifier有。主要原因是scikit-learn实现了两种Adaboost分类算法，SAMME和SAMME.R。两者的主要区别是弱学习器权重的度量，SAMME使用了和我们的原理篇里二元分类Adaboost算法的扩展，即用对样本集分类效果作为弱学习器权重，而SAMME.R使用了对样本集分类的预测概率大小来作为弱学习器权重。由于SAMME.R使用了概率度量的连续值，迭代一般比SAMME快，因此AdaBoostClassifier的默认算法algorithm的值也是SAMME.R。我们一般使用默认的SAMME.R就够了，但是要注意的是使用了SAMME.R， 则弱分类学习器参数base_estimator必须限制使用支持概率预测的分类器。SAMME算法则没有这个限制。\n",
    "\n",
    "\n",
    "- n_estimators： AdaBoostClassifier和AdaBoostRegressor都有，就是我们的弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是50。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。\n",
    "\n",
    "\n",
    "-  learning_rate:  AdaBoostClassifier和AdaBoostRegressor都有，即每个弱学习器的权重修改速率\n",
    "\n",
    "\n",
    "- base_estimator：AdaBoostClassifier和AdaBoostRegressor都有，即我们的弱分类学习器或者弱回归学习器。弱学习器默认使用**决策树**,理论上可以选择任何一个分类或者回归学习器，不过需要支持样本权重。我们常用的一般是CART决策树或者神经网络MLP。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "iris = load_iris()\n",
    "clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.5)\n",
    "score = cross_val_score(clf, iris.data, iris.target, cv=5)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "示例: [Decision Tree Regression with AdaBoost](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html#sphx-glr-auto-examples-ensemble-plot-adaboost-regression-py) 使用 AdaBoost.R2 算法证明了回归\n",
    "\n",
    "单一决策树回归与AdaBoost回归比对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "# Author: Noel Dawe <noel.dawe@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "# importing necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "# Create the dataset\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.linspace(0, 6, 100)[:, np.newaxis]\n",
    "f = np.sin(X).ravel() + np.sin(6 * X).ravel()\n",
    "y = f + rng.normal(0, 0.1, X.shape[0])\n",
    "\n",
    "# Fit regression model\n",
    "regr_1 = DecisionTreeRegressor(max_depth=4)\n",
    "\n",
    "regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n",
    "                           n_estimators=300, random_state=rng)\n",
    "regr_1.fit(X, y)\n",
    "regr_2.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_1 = regr_1.predict(X)\n",
    "y_2 = regr_2.predict(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X, y, c=\"k\", label=\"training samples\")\n",
    "plt.plot(X, f, label=\"y\")\n",
    "plt.plot(X, y_1, c=\"g\", label=\"n_estimators=1\", linewidth=2)\n",
    "plt.plot(X, y_2, c=\"r\", label=\"n_estimators=300\", linewidth=2)\n",
    "plt.xlabel(\"data\")\n",
    "plt.ylabel(\"target\")\n",
    "plt.title(\"Boosted Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Discrete versus Real AdaBoost](https://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_hastie_10_2.html#sphx-glr-auto-examples-ensemble-plot-adaboost-hastie-10-2-py) 使用 AdaBoost-SAMME 和 AdaBoost-SAMME.R 比较 decision stump(决策树桩)， decision tree（决策树）和 boosted decision stump（增强决策树桩）的分类错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>,\n",
    "# Noel Dawe <noel.dawe@gmail.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "n_estimators = 400\n",
    "# A learning rate of 1. may not be optimal for both SAMME and SAMME.R\n",
    "learning_rate = 1.\n",
    "\n",
    "X, y = datasets.make_hastie_10_2(n_samples=12000, random_state=1)\n",
    "\n",
    "# The ten features are standard independent Gaussian and\n",
    "# the target ``y`` is defined by::\n",
    "\n",
    "#   y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1\n",
    "\n",
    "\n",
    "X_test, y_test = X[2000:], y[2000:]\n",
    "X_train, y_train = X[:2000], y[:2000]\n",
    "\n",
    "# 决策树桩\n",
    "dt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)\n",
    "dt_stump.fit(X_train, y_train)\n",
    "dt_stump_err = 1.0 - dt_stump.score(X_test, y_test)\n",
    "\n",
    "# 决策树 深度9\n",
    "dt = DecisionTreeClassifier(max_depth=9, min_samples_leaf=1)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_err = 1.0 - dt.score(X_test, y_test)\n",
    "\n",
    "# AdaBoost SAMME\n",
    "ada_discrete = AdaBoostClassifier(\n",
    "    base_estimator=dt_stump,\n",
    "    learning_rate=learning_rate,\n",
    "    n_estimators=n_estimators,\n",
    "    algorithm=\"SAMME\")\n",
    "ada_discrete.fit(X_train, y_train)\n",
    "\n",
    "# AdaBoost SAMME.R\n",
    "ada_real = AdaBoostClassifier(\n",
    "    base_estimator=dt_stump,\n",
    "    learning_rate=learning_rate,\n",
    "    n_estimators=n_estimators,\n",
    "    algorithm=\"SAMME.R\")\n",
    "ada_real.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot([1, n_estimators], [dt_stump_err] * 2, 'k-',\n",
    "        label='Decision Stump Error')\n",
    "ax.plot([1, n_estimators], [dt_err] * 2, 'k--',\n",
    "        label='Decision Tree Error')\n",
    "\n",
    "# 串行过程中每学习一个基学习器后的 集成预测结果\n",
    "ada_discrete_err = np.zeros((n_estimators,))\n",
    "for i, y_pred in enumerate(ada_discrete.staged_predict(X_test)):\n",
    "    ada_discrete_err[i] = zero_one_loss(y_pred, y_test)\n",
    "    \n",
    "ada_discrete_err_train = np.zeros((n_estimators,))\n",
    "for i, y_pred in enumerate(ada_discrete.staged_predict(X_train)):\n",
    "    ada_discrete_err_train[i] = zero_one_loss(y_pred, y_train)\n",
    "\n",
    "ada_real_err = np.zeros((n_estimators,))\n",
    "for i, y_pred in enumerate(ada_real.staged_predict(X_test)):\n",
    "    ada_real_err[i] = zero_one_loss(y_pred, y_test)\n",
    "\n",
    "ada_real_err_train = np.zeros((n_estimators,))\n",
    "for i, y_pred in enumerate(ada_real.staged_predict(X_train)):\n",
    "    ada_real_err_train[i] = zero_one_loss(y_pred, y_train)\n",
    "\n",
    "ax.plot(np.arange(n_estimators) + 1, ada_discrete_err,\n",
    "        label='Discrete AdaBoost Test Error',\n",
    "        color='red')\n",
    "ax.plot(np.arange(n_estimators) + 1, ada_discrete_err_train,\n",
    "        label='Discrete AdaBoost Train Error',\n",
    "        color='blue')\n",
    "ax.plot(np.arange(n_estimators) + 1, ada_real_err,\n",
    "        label='Real AdaBoost Test Error',\n",
    "        color='orange')\n",
    "ax.plot(np.arange(n_estimators) + 1, ada_real_err_train,\n",
    "        label='Real AdaBoost Train Error',\n",
    "        color='green')\n",
    "    \n",
    "ax.set_ylim((0.0, 0.5))\n",
    "ax.set_xlabel('n_estimators')\n",
    "ax.set_ylabel('error rate')\n",
    "leg = ax.legend(loc='upper right', fancybox=True)\n",
    "leg.get_frame().set_alpha(0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Gradient Tree Boosting（梯度树提升）\n",
    "\n",
    "Gradient Tree Boosting 或梯度提升回归树（GBRT）是对于任意的**可微损失函数**的提升算法的泛化。 GBRT 是一个准确高效的现有程序， 它既能用于分类问题也可以用于回归问题。梯度树提升模型被应用到各种领域，包括网页搜索排名和生态领域。\n",
    "\n",
    "GBRT 的优点:\n",
    "\n",
    "- 对混合型数据的自然处理（异构特征）\n",
    "- 强大的预测能力\n",
    "- 在输出空间中对异常点的鲁棒性(通过具有鲁棒性的损失函数实现)\n",
    "\n",
    "GBRT 的缺点:\n",
    "\n",
    "- 可扩展性差（校对者注：此处的可扩展性特指在更大规模的数据集/复杂度更高的模型上使用的能力，而非我们通常说的功能的扩展性；GBRT 支持自定义的损失函数，从这个角度看它的扩展性还是很强的！）。由于提升算法的有序性(也就是说下一步的结果依赖于上一步)，因此很难做并行.\n",
    "\n",
    "\n",
    "## 分类 GradientBoostingClassifier\n",
    "与上面提到的集成分类算法类似, 弱学习器(例如:回归树)的数量由参数 n_estimators 来控制；每个树的大小可以通过由参数 max_depth 设置树的深度，或者由参数 max_leaf_nodes 设置叶子节点数目来控制。 learning_rate 是一个在 (0,1] 之间的超参数，这个参数通过 shrinkage(缩减步长) 来控制过拟合.\n",
    ">注意:超过两类的分类问题需要在每一次迭代时推导 n_classes 个回归树。因此，所有的需要推导的树数量等于 n_classes * n_estimators 。对于拥有大量类别的数据集我们强烈推荐使用 RandomForestClassifier 来代替 GradientBoostingClassifier 。\n",
    "\n",
    "## 回归 GradientBoostingRegressor\n",
    "对于回归问题 GradientBoostingRegressor 支持一系列 different loss functions ，这些损失函数可以通过参数 loss 来指定；对于回归问题默认的损失函数是最小二乘损失函数（ 'ls' ）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例 [Gradient Boosting regression](https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py) 使用梯度提升树预测房价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# #############################################################################\n",
    "# Load data\n",
    "boston = datasets.load_boston()\n",
    "X, y = shuffle(boston.data, boston.target, random_state=13)\n",
    "X = X.astype(np.float32)\n",
    "offset = int(X.shape[0] * 0.9)\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "X_test, y_test = X[offset:], y[offset:]\n",
    "\n",
    "# #############################################################################\n",
    "# Fit regression model\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "'learning_rate': 0.01, 'loss': 'ls'}  # 损失函数使用 最小二乘法 \n",
    "clf = ensemble.GradientBoostingRegressor(**params)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "mse = mean_squared_error(y_test, clf.predict(X_test))\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "\n",
    "# #############################################################################\n",
    "# Plot training deviance\n",
    "# compute test set deviance\n",
    "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "# 记录每一步的损失\n",
    "for i, y_pred in enumerate(clf.staged_predict(X_test)):\n",
    "    test_score[i] = clf.loss_(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, clf.train_score_, 'b-',\n",
    "        label='Training Set Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "        label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')\n",
    "\n",
    "# #############################################################################\n",
    "# Plot feature importance\n",
    "feature_importance = clf.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, boston.feature_names[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
