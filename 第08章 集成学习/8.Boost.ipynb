{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8章 提升方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1．提升方法是将弱学习算法提升为强学习算法的统计学习方法。在分类学习中，提升方法通过反复修改训练数据的权值分布，构建一系列基本分类器（弱分类器），并将这些基本分类器线性组合，构成一个强分类器。代表性的提升方法是AdaBoost算法。\n",
    "\n",
    "AdaBoost模型是弱分类器的线性组合：\n",
    "\n",
    "$$f(x)=\\sum_{m=1}^{M} \\alpha_{m} G_{m}(x)$$\n",
    "\n",
    "2．AdaBoost算法的特点是通过迭代每次学习一个基本分类器。每次迭代中，提高那些被前一轮分类器错误分类数据的权值，而降低那些被正确分类的数据的权值。最后，AdaBoost将基本分类器的线性组合作为强分类器，其中给分类误差率小的基本分类器以大的权值，给分类误差率大的基本分类器以小的权值。\n",
    "\n",
    "3．AdaBoost的训练误差分析表明，AdaBoost的每次迭代可以减少它在训练数据集上的分类误差率，这说明了它作为提升方法的有效性。\n",
    "\n",
    "4．AdaBoost算法的一个解释是该算法实际是前向分步算法的一个实现。在这个方法里，模型是加法模型，损失函数是指数损失，算法是前向分步算法。\n",
    "每一步中极小化损失函数\n",
    "\n",
    "$$\\left(\\beta_{m}, \\gamma_{m}\\right)=\\arg \\min _{\\beta, \\gamma} \\sum_{i=1}^{N} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+\\beta b\\left(x_{i} ; \\gamma\\right)\\right)$$\n",
    "\n",
    "得 到 参 数$\\beta_{m}, \\gamma_{m}$。\n",
    "\n",
    "5．提升树是以分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中最有效的方法之一。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Boost\n",
    "\n",
    "“装袋”（bagging）和“提升”（boost）是构建组合模型的两种最主要的方法，所谓的组合模型是由多个基本模型构成的模型，组合模型的预测效果往往比任意一个基本模型的效果都要好。\n",
    "\n",
    "- 装袋：每个基本模型由从总体样本中随机抽样得到的不同数据集进行训练得到，通过重抽样得到不同训练数据集的过程称为装袋。\n",
    "\n",
    "- 提升：每个基本模型训练时的数据集采用不同权重，针对上一个基本模型分类错误的样本增加权重，使得新的模型重点关注误分类样本\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "AdaBoost是AdaptiveBoost的缩写，表明该算法是具有适应性的提升算法。\n",
    "\n",
    "算法的步骤如下：\n",
    "\n",
    "1）给每个训练样本（$x_{1},x_{2},….,x_{N}$）分配权重，初始权重$w_{1}$均为1/N。\n",
    "\n",
    "2）针对带有权值的样本进行训练，得到模型$G_m$（初始模型为G1）。\n",
    "\n",
    "3）计算模型$G_m$的误分率$e_m=\\sum_{i=1}^Nw_iI(y_i\\not= G_m(x_i))$\n",
    "\n",
    "4）计算模型$G_m$的系数$\\alpha_m=0.5\\log[(1-e_m)/e_m]$\n",
    "\n",
    "5）根据误分率e和当前权重向量$w_m$更新权重向量$w_{m+1}$。\n",
    "\n",
    "6）计算组合模型$f(x)=\\sum_{m=1}^M\\alpha_mG_m(x_i)$的误分率。\n",
    "\n",
    "7）当组合模型的误分率或迭代次数低于一定阈值，停止迭代；否则，回到步骤2）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection  import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=16)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "def create_data():\n",
    "    iris = load_iris()\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    df['label'] = iris.target\n",
    "    df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'label']\n",
    "    data = np.array(df.iloc[:100, [0, 1, -1]])\n",
    "    for i in range(len(data)):\n",
    "        if data[i,-1] == 0:\n",
    "            data[i,-1] = -1\n",
    "    return data[:,:2], data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:50,0],X[:50,1], label='0')\n",
    "plt.scatter(X[50:,0],X[50:,1], label='1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### AdaBoost in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分类问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二类分类问题的 基学习器 G_m\n",
    "\n",
    "def greater_than(X, feature, value):\n",
    "    return np.where(X[:, feature] > value, 1, -1) \n",
    "\n",
    "def less_than(X, feature, value):\n",
    "    return np.where(X[:, feature] < value, 1, -1)\n",
    "\n",
    "FUNC_LIST = [greater_than, less_than]\n",
    "\n",
    "\n",
    "class BiClassifer:\n",
    "    def __init__(self, funcs=FUNC_LIST):# 样本权值\n",
    "        self.funcs = funcs\n",
    "        self.value_split = None\n",
    "        self.func_i = None\n",
    "        self.feature = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def error_(y_pred, y, weights):\n",
    "        return np.sum(weights * (y_pred != y))\n",
    "        \n",
    "    def choose_best_value(self, X, y, feature):\n",
    "        # 返回 最小的error, 最佳分割点 相应的func \n",
    "        value_split_list = np.sort(X[:, feature])\n",
    "        value_split = (value_split_list[1:] + value_split_list[:-1]) / 2\n",
    "        \n",
    "        error_one_feature = np.zeros((len(value_split), 4))\n",
    "        for i, value in enumerate(value_split):\n",
    "            error_min = np.inf\n",
    "            for j, func in enumerate(self.funcs):\n",
    "                y_pred = func(X, feature, value)\n",
    "                error = self.error_(y_pred, y, self.weights)\n",
    "                if error < error_min:\n",
    "                    error_min = error\n",
    "                    error_one_feature[i,:] = [error, value, j, feature]\n",
    "\n",
    "        min_index = np.argmin(error_one_feature[:, 0])\n",
    "        return error_one_feature[min_index]\n",
    "    \n",
    "    def choose_best_feature(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "        features = np.arange(n_features)\n",
    "        \n",
    "        error_all_features = np.zeros((n_features, 4))\n",
    "        for i in range(n_features):\n",
    "            error_all_features[i, :] = self.choose_best_value(X, y, i)\n",
    "        best_ = np.argmin(error_all_features[:, 0])\n",
    "        return error_all_features[best_]\n",
    "    \n",
    "    def fit(self, X, y, weights=None):\n",
    "        self.weights = weights\n",
    "        if self.weights is None:\n",
    "            self.weights = np.ones(X.shape[0]) / X.shape[0]\n",
    "        error, value_split, func_i, feature = self.choose_best_feature(X, y)\n",
    "        # print(value_split, func_i)\n",
    "        self.value_split = value_split\n",
    "        self.func_i = int(func_i)\n",
    "        self.feature = int(feature)\n",
    "        return error\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = self.funcs[self.func_i](X, self.feature, self.value_split)\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        # 无权重\n",
    "        return np.sum(self.predict(X) == y) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**例8.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(10).reshape(10, 1)\n",
    "y = np.array([1, 1, 1, -1, -1, -1, 1, 1, 1, -1])\n",
    "\n",
    "clf = BiClassifer()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoostClassifier:\n",
    "    def __init__(self, n_estimators=50, base_classifier=BiClassifer, epsilon=0.05):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.bc_ = base_classifier\n",
    "        self.epsilon = epsilon  # 错误率阈值\n",
    "        self.X = None\n",
    "        self.y =None\n",
    "        self.weights = None\n",
    "        self.func_list = []  # alpha 基分类器 \n",
    "    \n",
    "    def Z_(self, alpha, clf):\n",
    "        # 计算归一化因子 Z_m = \\sum_{i=1}^N w_{mi} exp(-alpha_m y_i G_m(x_i))\n",
    "        z = np.sum(self.weights * np.exp(-alpha * self.y * clf.predict(self.X)))\n",
    "        return z\n",
    "    \n",
    "    def w_(self, alpha, clf, Z):\n",
    "        # 样本权值更新 w_{m+1, i} = w_{mi} / Z exp(-alpha_m y_i G_m(x_i))\n",
    "        self.weights =  self.weights / Z * np.exp(-alpha * self.y * clf.predict(self.X))\n",
    "       \n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        n_sample, n_feature = X.shape\n",
    "        self.weights = np.ones(n_sample) / n_sample  # 初始的权重 D1\n",
    "        Z_list = []\n",
    "        for i in range(self.n_estimators):\n",
    "            clf = self.bc_()\n",
    "            error = clf.fit(X, y, self.weights)\n",
    "            alpha = 0.5 * np.log((1-error) / error)\n",
    "            # 归一化因子Z_m\n",
    "            Z_ = self.Z_(alpha, clf)\n",
    "            Z_list.append(Z_)\n",
    "            # 更新w\n",
    "            self.w_(alpha, clf, Z_)\n",
    "            # print('em:', error)\n",
    "            self.func_list.append((alpha, clf))\n",
    "            error_upper_bound = np.prod(Z_list)\n",
    "#             if error_upper_bound < self.epsilon:\n",
    "#                 break\n",
    "                \n",
    "    def predict(self, X):\n",
    "        clf_num = len(self.func_list)\n",
    "        y_pred = np.zeros((clf_num, X.shape[0]))\n",
    "        for i, (alpha, clf) in enumerate(self.func_list):\n",
    "            y = alpha * clf.predict(X)\n",
    "            y_pred[i, :] = y\n",
    "        return np.where(np.mean(y_pred, axis=0) > 0, 1, -1)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return np.sum(self.predict(X) == y) / len(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**例8.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(10).reshape(10, 1)\n",
    "y = np.array([1, 1, 1, -1, -1, -1, 1, 1, 1, -1])\n",
    "clf_ada = AdaBoostClassifier(n_estimators=3, base_classifier=BiClassifer)\n",
    "clf_ada.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ada.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ada.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=12345)\n",
    "clf = AdaBoostClassifier(n_estimators=10, base_classifier=BiClassifer)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**鸢尾花**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100次结果\n",
    "result = []\n",
    "for i in range(1, 101):\n",
    "    X, y = create_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "    clf = AdaBoostClassifier(n_estimators=100, base_classifier=BiClassifer)\n",
    "    clf.fit(X_train, y_train)\n",
    "    r = clf.score(X_test, y_test)\n",
    "    # print('{}/100 score：{}'.format(i, r))\n",
    "    result.append(r)\n",
    "\n",
    "print('average score:{:.3f}%'.format(sum(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[马疝病的预测](https://github.com/apachecn/AiLearning/blob/master/docs/ml/7.%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%92%8CAdaBoost.md#%E9%A1%B9%E7%9B%AE%E6%A1%88%E4%BE%8B-%E9%A9%AC%E7%96%9D%E7%97%85%E7%9A%84%E9%A2%84%E6%B5%8B)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.loadtxt('./horseColicTraining2.txt')\n",
    "data_test = np.loadtxt(\"./horseColicTest2.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data_train[:, :-1], data_train[:, -1]\n",
    "X_test, y_test = data_test[:, :-1], data_test[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BiClassifer()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.value_split, clf.func_i, clf.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.5)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 回归问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor:\n",
    "    def __init__(self):# 样本权值\n",
    "        self.value_split = None\n",
    "        self.feature = None\n",
    "        self.c = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def data_split(X, feature, value):\n",
    "        # 使用feature特征的value取值将X切分开\n",
    "        left, right = [], []\n",
    "        for i in range(X.shape[0]):\n",
    "            if X[i, feature] < value:\n",
    "                left.append(i)\n",
    "            else:\n",
    "                right.append(i)\n",
    "        return left, right\n",
    "        \n",
    "    \n",
    "    def choose_best_value(self, X, y, feature):\n",
    "        # 返回 最小的error, 最佳分割点 相应的func \n",
    "        value_split_list = np.sort(X[:, feature])\n",
    "        value_split = (value_split_list[1:] + value_split_list[:-1]) / 2\n",
    "        \n",
    "        error_one_feature = np.zeros((len(value_split), 5))\n",
    "        for i, value in enumerate(value_split):\n",
    "            error_min = np.inf\n",
    "            left, right = self.data_split(X, feature, value)\n",
    "            c1 = np.mean(y[left])\n",
    "            c2 = np.mean(y[right])\n",
    "            error = np.sum((y[left] - c1) ** 2) + np.sum((y[right] - c2) ** 2)\n",
    "\n",
    "            error_one_feature[i,:] = [error, feature, value, c1, c2]\n",
    "\n",
    "        min_index = np.argmin(error_one_feature[:, 0])\n",
    "        return error_one_feature[min_index]\n",
    "    \n",
    "    def choose_best_feature(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "        error_all_features = np.zeros((n_features, 5))\n",
    "        for i in range(n_features):\n",
    "            error_all_features[i, :] = self.choose_best_value(X, y, i)\n",
    "        best_ = np.argmin(error_all_features[:, 0])\n",
    "        return error_all_features[best_]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        error, feature, value_split, c1, c2  = self.choose_best_feature(X, y)\n",
    "        # print(value_split, func_i)\n",
    "        self.value_split = value_split\n",
    "        self.feature = int(feature)\n",
    "        self.c = [c1, c2]\n",
    "        return error\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.where(X[:, self.feature] < self.value_split, self.c[0], self.c[1])\n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**例 8.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(1, 11).reshape(10, 1)\n",
    "y = np.array([5.56, 5.70, 5.91, 6.4, 6.8, 7.05, 8.90, 8.70, 9.00, 9.05])\n",
    "clf = Regressor()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.value_split, clf.c, clf.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回归问题\n",
    "class AdaBoostRegressor:\n",
    "    def __init__(self, n_estimators=20, base_learner=Regressor):\n",
    "        self.clf_num = n_estimators\n",
    "        self.func_list = []\n",
    "        self.bl_ = base_learner\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "        r = y\n",
    "        y_pred_list = []\n",
    "        for i in range(self.clf_num):\n",
    "            clf = self.bl_()\n",
    "            clf.fit(X, r)\n",
    "            self.func_list.append(clf)\n",
    "            y_pred = clf.predict(X)\n",
    "            y_pred_list.append(y_pred)\n",
    "            r = y - np.sum(y_pred_list, axis=0)\n",
    "        return np.sum(r ** 2)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        n_estimators = len(self.func_list)\n",
    "        y_pred = np.zeros((n_estimators, X.shape[0]))\n",
    "        for i in range(n_estimators):\n",
    "            y = self.func_list[i].predict(X)\n",
    "            y_pred[i, :] = y\n",
    "        return np.sum(y_pred, axis=0)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return np.sum(self.predict(X) == y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostRegressor(n_estimators=6)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, '-', label='y')\n",
    "plt.plot(X, y_pred, '--', label='y_pred')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "参考代码：https://github.com/wzyonggege/statistical-learning-method\n",
    "\n",
    "中文注释制作：机器学习初学者\n",
    "\n",
    "微信公众号：ID:ai-start-com\n",
    "\n",
    "配置环境：python 3.5+\n",
    "\n",
    "代码全部测试通过。\n",
    "![gongzhong](../gongzhong.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
