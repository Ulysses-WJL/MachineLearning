{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "np.set_printoptions(precision=4, threshold=15,suppress=True)\n",
    "pd.options.display.max_rows = 20\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding the Nearest Neighbors**\n",
    "\n",
    "For the simple task of finding the nearest neighbors between two sets of data, the unsupervised algorithms within\n",
    "sklearn.neighbors can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NearestNeighbors(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
       "                 metric_params=None, n_jobs=None, n_neighbors=2, p=2,\n",
       "                 radius=1.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "X = np.array([[-1., -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)  # 训练集\n",
    "nbrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NearestNeighbors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nbrs.kneighbors?\n",
    "```\n",
    "Signature: nbrs.kneighbors(X=None, n_neighbors=None, return_distance=True)\n",
    "Docstring:\n",
    "Finds the K-neighbors of a point.\n",
    "Returns indices of and distances to the neighbors of each point.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == 'precomputed'\n",
    "    The query point or points.\n",
    "    If not provided, neighbors of each indexed point are returned.\n",
    "    In this case, the query point is not considered its own neighbor.\n",
    "\n",
    "n_neighbors : int\n",
    "    Number of neighbors to get (default is the value\n",
    "    passed to the constructor).\n",
    "\n",
    "return_distance : boolean, optional. Defaults to True.\n",
    "    If False, distances will not be returned\n",
    "\n",
    "Returns\n",
    "-------\n",
    "dist : array\n",
    "    Array representing the lengths to points, only present if\n",
    "    return_distance=True\n",
    "\n",
    "ind : array\n",
    "    Indices of the nearest points in the population matrix.\n",
    "\n",
    "Examples\n",
    "--------\n",
    "In the following example, we construct a NeighborsClassifier\n",
    "class from an array representing our data set and ask who's\n",
    "the closest point to [1,1,1]\n",
    "\n",
    ">>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
    ">>> from sklearn.neighbors import NearestNeighbors\n",
    ">>> neigh = NearestNeighbors(n_neighbors=1)\n",
    ">>> neigh.fit(samples) # doctest: +ELLIPSIS\n",
    "NearestNeighbors(algorithm='auto', leaf_size=30, ...)\n",
    ">>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS\n",
    "(array([[0.5]]), array([[2]]))\n",
    "\n",
    "As you can see, it returns [[0.5]], and [[2]], which means that the\n",
    "element is at distance 0.5 and is the third element of samples\n",
    "(indexes start at 0). You can also query for multiple points:\n",
    "\n",
    ">>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
    ">>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS\n",
    "array([[1],\n",
    "       [2]]...)\n",
    "File:      d:\\python3.7\\lib\\site-packages\\sklearn\\neighbors\\base.py\n",
    "Type:      method\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = nbrs.kneighbors(X)  # 查询集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.    , 1.    ],\n",
       "       [0.    , 1.    ],\n",
       "       [0.    , 1.4142],\n",
       "       [0.    , 1.    ],\n",
       "       [0.    , 1.    ],\n",
       "       [0.    , 1.4142]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances  # (dis1, dis2)最近的2个点的距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [2, 1],\n",
       "       [3, 4],\n",
       "       [4, 3],\n",
       "       [5, 4]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices  # 最近的2个点的index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the query set matches the training set, the nearest neighbor of each point is the point itself, at a distance ofzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 1., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbrs.kneighbors_graph(X).toarray()\n",
    "# produce a sparse graph(稀疏图) showing the connections between neighboring points:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KDTree and BallTree Classes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.neighbors.kd_tree.KDTree at 0x563ca0544b40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "kdt = KDTree(X, leaf_size=30, metric='euclidean')  # 欧式距离  叶节点\n",
    "kdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [2, 1],\n",
       "       [3, 4],\n",
       "       [4, 3],\n",
       "       [5, 4]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdt.query(X, k=2, return_distance=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最近邻分类\n",
    "scikit-learn 实现了两种不同的最近邻分类器： \n",
    "- KNeighborsClassifier 基于每个查询点的 k 个最近邻实现，其中 k 是用户指定的整数值。\n",
    "- RadiusNeighborsClassifier 基于每个查询点的固定半径 r 内的邻居数量实现， 其中 r 是用户指定的浮点数值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn.neighbors.KNeighborsClassifier\n",
    "\n",
    "- n_neighbors: 临近点个数\n",
    "- p: 距离度量 Power parameter for the Minkowski metric 默认2\n",
    "- algorithm: 近邻算法，可选{'auto', 'ball_tree', 'kd_tree', 'brute'}\n",
    " * 'auto': 根据传递给`fit`方法的数据自行推断使用的算法\n",
    " * 'brute': 暴力求解 复杂度$O(DN^2)$\n",
    " * 'kd_tree': 使用KDTree, 对于小于20的D,$O(DlogN)$;对于较大的D接近$O(DN)$\n",
    " * 'ball_tree': 使用BallTree, 复杂度$O(DlogN)$\n",
    "- weights: 确定近邻的权重\n",
    " * 'uniform': 均匀分布. 所有邻近点的权重是一致的\n",
    " * 'dustance': 与距离成反比. 距离近的点的权重更大\n",
    " * [callbale]: a user-defined function which accepts an array of distances, and returns an\n",
    "    array of the same shape containing the weights.\n",
    "- leaf_size: Leaf size passed to BallTree or KDTree. 对于小数据集 (n小于30), log(N)相当于N, 暴力算法比基于树的算法更加有效, 控制了查询切换到暴力计算样本数量. 默认30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用鸢尾花数据进行分析, 对四个特征都进行了分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       ...,\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from figures import plot_2d_separator\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data # [:100, :2]\n",
    "Y = iris.target # [:100]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data_1(data):\n",
    "    # 使用归一化预处理数据\n",
    "    return normalize(data, 'l2')\n",
    "\n",
    "def normalize_data_2(data):\n",
    "    # 使用归一化预处理数据\n",
    "    X_std1 = normalize(data[:, :2], 'l2')  # 缩放至L2范数为1\n",
    "    X_std2 = normalize(data[:, 2:], 'l2')\n",
    "    return np.concatenate((X_std1, X_std2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_data(data):\n",
    "    # 将输入向量 X 上的每个特征缩放到 [0,1] 或 [- 1，+1]， 或将其标准化，使其均值为 0，方差为 1\n",
    "    standardizer = StandardScaler().fit(data)  \n",
    "    return standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2, ..., 147, 148, 149])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_indices = np.arange(X.shape[0])\n",
    "x_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices, X_test_indices, Y_train, Y_test = train_test_split(x_indices, Y, test_size=0.2)\n",
    "# 将样本分为训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 18,  34,  49, ...,  94, 144,  70])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0238, -0.1677,  0.6835,  0.6514],\n",
       "       [-1.1009, -1.7864, -0.2868, -0.292 ],\n",
       "       [ 1.0238,  0.5261,  1.0831,  1.1905],\n",
       "       ...,\n",
       "       [-1.1009,  1.2199, -1.3713, -1.3702],\n",
       "       [-0.101 , -0.8614,  0.1698, -0.292 ],\n",
       "       [ 1.0238,  0.0636,  1.026 ,  1.5948]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_std = np.concatenate((X_std1, X_std2), axis=1)\n",
    "standardizer = StandardScaler().fit(X[X_train_indices])\n",
    "X_std = standardizer.transform(X[X_train_indices])\n",
    "X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8038, 0.5516, 0.2206, 0.0315],\n",
       "       [0.8281, 0.507 , 0.2366, 0.0338],\n",
       "       [0.8053, 0.5483, 0.2228, 0.0343],\n",
       "       ...,\n",
       "       [0.7165, 0.3307, 0.5732, 0.2205],\n",
       "       [0.6747, 0.37  , 0.5876, 0.2503],\n",
       "       [0.6903, 0.351 , 0.5967, 0.2106]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normalize_1 = normalize_data_1(X)\n",
    "X_normalize_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8245, 0.5658, 0.9899, 0.1414],\n",
       "       [0.8529, 0.5222, 0.9899, 0.1414],\n",
       "       [0.8266, 0.5628, 0.9884, 0.1521],\n",
       "       ...,\n",
       "       [0.908 , 0.4191, 0.9333, 0.359 ],\n",
       "       [0.8768, 0.4808, 0.92  , 0.3919],\n",
       "       [0.8914, 0.4532, 0.943 , 0.3328]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normalize_2 = normalize_data_2(X)\n",
    "X_normalize_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='distance')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用原始数据进行测试\n",
    "knn1 = KNeighborsClassifier(weights='distance').fit(X[X_train_indices], Y_train)\n",
    "knn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn1.score(X[X_test_indices], Y_test)  # 使用测试集测试正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard\n",
    "knn2 = KNeighborsClassifier(weights='distance').fit(X_std, Y_train)\n",
    "X_standard_test = standardizer.transform(X[X_test_indices])\n",
    "knn2.score(X_standard_test, Y_test)  # 使用测试集测试正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize 1\n",
    "knn3 = KNeighborsClassifier(weights='distance').fit(X_normalize_1[X_train_indices], Y_train)\n",
    "knn3.score(X[X_test_indices], Y_test)  # 使用测试集测试正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize 2\n",
    "knn4 = KNeighborsClassifier(weights='distance').fit(X_normalize_2[X_train_indices], Y_train)\n",
    "knn4.score(X[X_test_indices], Y_test)  # 使用测试集测试正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_node = [5.9, 3. , 5.1, 1.8]\n",
    "# node = standardizer.transform([test_node])\n",
    "knn1.predict([test_node])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于半径的 KNN 分类器\n",
    "**sklearn.neighbors.RadiusNeighborsClassifier**\n",
    "如果数据是不均匀采样的，那么 `RadiusNeighborsClassifier` 中的基于半径的近邻分类可能是更好的选择。用户指定一个固定半径 ，使得稀疏邻居中的点使用较少的最近邻来分类。对于高维参数空间，这个方法会由于所谓的 “维度灾难” 而变得不那么有效。\n",
    "\n",
    "`RadiusNeighborsClassifier`与`KNeighborsClassifier`非常相似，但有两个参数除外。 首先，在`RadiusNeighborsClassifier`中，我们需要指定固定区域的半径`radius`(默认1.0)，用于确定观测是否是半径内的邻居。 将半径设置为某个值，最好将其视为任何其他超参数，并在模型选择期间对其进行调整。 第二个有用的参数是`outlier_label`(默认None)，它表示半径内没有观测的观测的标签 - 这本身通常可以是识别异常值的有用工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.9007,  1.019 , -1.3402, -1.3154],\n",
       "       [-1.143 , -0.132 , -1.3402, -1.3154],\n",
       "       [-1.3854,  0.3284, -1.3971, -1.3154],\n",
       "       ...,\n",
       "       [ 0.7957, -0.132 ,  0.8196,  1.0539],\n",
       "       [ 0.4322,  0.7888,  0.9333,  1.4488],\n",
       "       [ 0.0687, -0.132 ,  0.7628,  0.7907]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建标准化器\n",
    "standardizer = StandardScaler()\n",
    "# 标准化特征\n",
    "X_std = standardizer.fit_transform(X)\n",
    "X_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RadiusNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                          metric_params=None, n_jobs=-1, outlier_label=None,\n",
       "                          p=2, radius=0.5, weights='uniform')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练半径邻居分类器\n",
    "rnn = RadiusNeighborsClassifier(radius=.5, n_jobs=-1).fit(X_std, Y)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建新观测点\n",
    "new_observations = [[5, 3, 1, 0], [6, 3 , 4.9, 1.9]]\n",
    "rnn.predict(standardizer.transform(new_observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
