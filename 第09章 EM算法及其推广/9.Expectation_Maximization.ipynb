{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章 EM算法及其推广"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization algorithm\n",
    "\n",
    "### Maximum likehood function\n",
    "\n",
    "[likehood & maximum likehood](http://fangs.in/post/thinkstats/likelihood/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1．EM算法是含有隐变量的概率模型极大似然估计或极大后验概率估计的迭代算法。含有隐变量(latent variable)的概率模型的数据表示为$\\theta$ )。这里，$Y$是观测变量的数据，$Z$是隐变量的数据，$\\theta$ 是模型参数。EM算法通过迭代求解观测数据的对数似然函数${L}(\\theta)=\\log {P}(\\mathrm{Y} | \\theta)$的极大化，实现极大似然估计。每次迭代包括两步：\n",
    "\n",
    "$E$步，求期望，即求$logP\\left(Z | Y, \\theta\\right)$ )关于$ P\\left(Z | Y, \\theta^{(i)}\\right)$)的期望：\n",
    "\n",
    "$$Q\\left(\\theta, \\theta^{(i)}\\right)=\\sum_{Z}P\\left(Z | Y, \\theta^{(i)}\\right) \\log P(Y, Z | \\theta) $$\n",
    "称为$Q$函数，这里$\\theta^{(i)}$是参数的现估计值；\n",
    ">**Q 函数**: 完全数据(Y, Z)的**对数似然函数$\\log P(Y, Z|\\theta)$关于**给定观测数据$Y$的当前参数$\\theta^{(i)}$下对未观测数据$Z$的**条件概率分布$P(Z|Y,\\theta^{(i)})$的期望**称为Q函数。\n",
    "\n",
    "$M$步，求极大，即极大化$Q$函数得到参数的新估计值：\n",
    "\n",
    "$$\\theta^{(i+1)}=\\arg \\max _{\\theta} Q\\left(\\theta, \\theta^{(i)}\\right)$$\n",
    " \n",
    "在构建具体的EM算法时，重要的是定义$Q$函数。每次迭代中，EM算法通过极大化$Q$函数来增大对数似然函数${L}(\\theta)$。\n",
    "\n",
    "2．EM算法在每次迭代后均提高观测数据的似然函数值，即\n",
    "\n",
    "$$P\\left(Y | \\theta^{(i+1)}\\right) \\geqslant P\\left(Y | \\theta^{(i)}\\right)$$\n",
    "\n",
    "在一般条件下EM算法是收敛的，但不能保证收敛到全局最优。所以在应用中, 初值的选择变得非常重要, 常用方法是选取几个不同的初值进行迭代, 选择其中估计值最好的.\n",
    "\n",
    "3．EM算法应用极其广泛，主要应用于含有隐变量的概率模型的学习。高斯混合模型的参数估计是EM算法的一个重要应用，下一章将要介绍的隐马尔可夫模型的非监督学习也是EM算法的一个重要应用。\n",
    "\n",
    "4．EM算法还可以解释为$F$函数的极大-极大算法。EM算法有许多变形，如GEM算法。GEM算法的特点是每次迭代增加$F$函数值（并不一定是极大化$F$函数），从而增加似然函数值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在统计学中，似然函数（likelihood function，通常简写为likelihood，似然）是一个非常重要的内容，在非正式场合似然和概率（Probability）几乎是一对同义词，但是在统计学中似然和概率却是两个不同的概念。概率是在特定环境下某件事情发生的可能性，也就是结果没有产生之前依据环境所对应的参数来预测某件事情发生的可能性，比如抛硬币，抛之前我们不知道最后是哪一面朝上，但是根据硬币的性质我们可以推测任何一面朝上的可能性均为50%，这个概率只有在抛硬币之前才是有意义的，抛完硬币后的结果便是确定的；而似然刚好相反，是在确定的结果下去推测产生这个结果的可能环境（参数），还是抛硬币的例子，假设我们随机抛掷一枚硬币1,000次，结果500次人头朝上，500次数字朝上（实际情况一般不会这么理想，这里只是举个例子），我们很容易判断这是一枚标准的硬币，两面朝上的概率均为50%，这个过程就是我们运用出现的结果来判断这个事情本身的性质（参数），也就是似然。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P (Y|\\theta) = \\prod[\\pi p^{y_i}(1-p)^{1-y_i}+(1-\\pi) q^{y_i}(1-q)^{1-y_i}]$$\n",
    "\n",
    "### E step:\n",
    "\n",
    "$$\\mu^{i+1}=\\frac{\\pi (p^i)^{y_i}(1-(p^i))^{1-y_i}}{\\pi (p^i)^{y_i}(1-(p^i))^{1-y_i}+(1-\\pi) (q^i)^{y_i}(1-(q^i))^{1-y_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Y**: 观测到的随机变量数据, 不完全数据(incomplete-data);  \n",
    "**Z**: 隐随机变量数据;  \n",
    "Y与Z连在一起称为完全数据(complete-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EM 算法的导出\n",
    "\n",
    "EM算法能近似实现对观测数据的极大似然估计的原因:  \n",
    "\n",
    "目标函数: $$L(\\theta)=\\log P(Y|\\theta)=\\log \\sum_Z P(Y,Z|\\theta)=\\log(\\sum_Z P(Y|Z,\\theta)P(Z|\\theta))$$\n",
    "在逐步迭代过程中, 假设第$i$次迭代后的$\\theta$的估计值是$\\theta^{(i)}$, 我们希望新的$\\theta$能使$L(\\theta)$增大, 并逐步达到极大值\n",
    ":\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\theta)-L(\\theta^{(i)})&=\\log \\left(\\sum_ZP(Y|Z,\\theta^{(i)})\\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Y|Z,\\theta^{(i)})}\\right)-\\log P(Y|\\theta^{(i)})\\\\\n",
    "&\\ge\\sum_Z P(Z|Y,\\theta^{(i)})\\log \\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^{(i)})}-\\log P(Y|\\theta^{(i)})\\\\\n",
    "&=\\sum_Z P(Z|Y,\\theta^{(i)})\\log \\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^{(i)})}-\\sum_ZP(Z|Y,\\theta^{(i)})\\log P(Y|\\theta^{(i)})\\\\\n",
    "&=\\sum_ZP(Z|Y,\\theta^{(i)})\\log \\frac{P(Y|Z,\\theta)P(Z|\\theta)}{P(Z|Y,\\theta^{(i)})P(Y|\\theta^{(i)})}\n",
    "\\end{align}$$\n",
    "\n",
    ">使用了Jensen不等式, 对于凹函数log\n",
    "$$\\log \\sum_j \\lambda_j y_j \\ge \\sum_j \\lambda_j \\log y_j,\\quad s.t., \\lambda_j \\ge 0, \\sum_j \\lambda_j = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "令$$B(\\theta, \\theta^{(i)}) \\hat= L(\\theta^{(i)}) + \\sum_ZP(Z|Y,\\theta^{(i)})[log P(Y|Z,\\theta)P(Z|\\theta) - logP(Z|Y,\\theta^{(i)})P(Y|\\theta^{(i)})]$$\n",
    "则, $L(\\theta) \\geq B(\\theta, \\theta^{(i)})$,使$B(\\theta, \\theta^{(i)})$增大的$\\theta$, 也可以使$L(\\theta)$增大, 问题变为\n",
    "$$\\begin{aligned}\n",
    "\\theta^{(i+1)} &= \\mathop{argmax}_{\\theta}B(\\theta, \\theta^{(i)})  \\\\\n",
    "&= \\mathop{argmax}_{\\theta}\\left(\\sum_Z P(Z|Y, \\theta^{(i)})log(P(Y|Z, \\theta)P(Z|\\theta)\\right) \\\\\n",
    "&= \\mathop{argmax}_{\\theta}\\left(\\sum_Z P(Z|Y, \\theta^{(i)})log(P(Y, Z|\\theta)\\right) \\\\\n",
    "&= \\mathop{argmax}_{\\theta}Q(\\theta, \\theta^{(i)})\n",
    "\\end{aligned}$$\n",
    "等价于EM算法的一次迭代, 即求Q函数及其极大化."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高斯混合模型GMM\n",
    "\n",
    "**混合模型**，有多种，高斯混合模型是最常用的。\n",
    "\n",
    "高斯混合模型(Gaussian Mixture Model)是具有如下**概率分布**的模型:\n",
    "$$\n",
    "P(y|\\theta)=\\sum\\limits^{K}_{k=1}\\alpha_k\\phi(y|\\theta_k)\n",
    "$$\n",
    "其中， $\\theta=(\\alpha_1,\\alpha_2,\\dots,\\alpha_K;\\theta_1,\\theta_2,\\dots,\\theta_K)$\n",
    "\n",
    "$\\alpha_k$是系数，$\\alpha_k\\ge0$，$\\sum\\limits^{K}_{k=1}\\alpha_k=1$, $\\phi(y|\\theta_k)$ 是**高斯分布密度**，$\\theta_k=(\\mu,\\sigma^2)$\n",
    "$$\n",
    "\\phi(y|\\theta_k)=\\frac{1}{\\sqrt{2\\pi}\\sigma_k}\\exp\\left(-\\frac{(y-\\mu_k)^2}{2\\sigma_k^2}\\right)\n",
    "$$\n",
    "上式表示第k个**分**模型。\n",
    "以上, 注意几点：\n",
    "\n",
    "1. GMM的描述是概率分布，形式上可以看成是加权求和\n",
    "2. 加权求和的权重$\\alpha$满足$\\sum_{k=1}^K\\alpha_k=1$的约束\n",
    "\n",
    "3. 求和符号中除去权重的部分，是高斯分布密度(PDF)。高斯混合模型是一种$\\sum(权重\\times 分布密度)=分布$的表达\n",
    "    高斯混合模型的参数估计是EM算法的一个重要应用，隐马尔科夫模型的非监督学习也是EM算法的一个重要应用。\n",
    "\n",
    "4. 书中描述的是一维的高斯混合模型，d维的形式如下[^2]，被称作多元正态分布，也叫多元高斯分布\n",
    "$$\n",
    "\\phi(y|\\theta_k)=\\frac{1}{\\sqrt{(2\\pi)^d|\\Sigma|}}\\exp\\left(-\\frac{(y-\\mu_k)^T\\Sigma^{-1}(y-\\mu_k)}{2}\\right)\n",
    "$$\n",
    "其中，协方差矩阵$\\Sigma\\in \\mathbb{R}^{n\\times n}$\n",
    "\n",
    "5. 另外，关于高斯模型的混合，还有另外一种混合的方式，沿着时间轴的方向做混合。可以理解为滤波器，典型的算法就是Kalman Filter，对应了时域与频域之间的关系，两个高斯的混合依然是高斯，混合的方法是卷积，而不是简单的加法，考虑到的是概率密度的混合，也是一种线性的加权。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用EM算法估计GMM的参数$\\theta$:\n",
    "#### 1. 明确隐变量,写出完全数据的对数似然函数  \n",
    "可以设想观测数据$y_j, j=1, 2, \\cdots, N$的产生方式: 根据概率$\\alpha_k$选择第$k$个高斯分布分模型$\\phi(y|\\theta_k)$(**未知**), 由它生成观测数据$y_j$(**已知**), 隐变量$\\gamma_{jk}$可以表示成:\n",
    "$$\\gamma_{jk}  = \\begin{cases}1, \\quad 第j个观测来自第k个分模型 \\\\ 0, \\quad otherwise \\end{cases} \\\\ j=1, 2, \\cdots, N ; k=1, 2, \\cdots, K$$\n",
    "$\\gamma_{jk}$是0-1随机变量.  \n",
    "完全数据的对数似然函数:\n",
    "$$\\begin{align}\n",
    "P(y, \\gamma|\\theta) &= \\prod_{j=1}^N P(y_j, \\gamma_{j1}, \\gamma_{j2}, \\cdots, \\gamma_{jK}|\\theta) \\\\\n",
    "&= \\prod_{j=1}^N \\prod_{k=1}^K[\\alpha_k \\phi(y_j|\\theta_k)]^{\\gamma_{jk}} \\\\\n",
    "&= \\prod_{k=1}^K \\alpha_k^{n_k} \\prod_{j=1}^N[\\phi(y_j|\\theta_k)]^{\\gamma_{jk}} \\\\\n",
    "&= \\prod_{k=1}^K \\alpha_k^{n_k} \\prod_{j=1}^N \\left[\\frac {1} {\\sqrt{2\\pi}\\sigma_k} exp\\left(-\\frac {(y_j - u_k)^2}{2\\sigma_k^2}\\right)\\right]^{\\gamma_{jk}}\n",
    "\\end{align}$$\n",
    " 其中$n_k=\\sum_{j=1}^N\\gamma_{jk}$ (第k个模型共产生了几个观测值, 即被使用了多少次), $\\sum_{k=1}^Kn_k=N$(所有的分模型共生成N个观测值)\n",
    "\n",
    " 完全数据对数似然函数\n",
    "$$\n",
    "  \\log P(y,\\gamma|\\theta)=\\sum_{k=1}^K\\left\\{n_k\\log \\alpha_k+\\sum_{j=1}^N\\gamma_{jk}\\left[\\log \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\log \\sigma_k -\\frac{1}{2\\sigma^2}(y_j-\\mu_k)^2\\right]\\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. E步,确定Q函数\n",
    "\n",
    "把$Q$ 函数表示成参数形式\n",
    "\n",
    "$$\\begin{aligned}Q(\\theta,\\theta^{(i)})=&E[\\log P(y,\\gamma|\\theta)|y,\\theta^{(i)}]\\\\=&\\color{green}E\\left\\{\\sum_{k=1}^K\\left\\{\\color{red}{n_k}\\log \\alpha_k+\\color{blue}{\\sum_{j=1}^N\\gamma _{jk}}\\left[\\log \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\log \\sigma _k-\\frac{1}{2\\sigma^2(y_j-\\mu_k)^2}\\right]\\right\\}\\right\\}\\\\=&\\color{green}E\\left\\{\\sum_{k=1}^K\\left\\{\\color{red}{\\sum_{j=1}^N\\gamma _{jk}}\\log \\alpha_k+\\color{blue}{\\sum_{j=1}^N\\gamma _{jk}}\\left[\\log \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\log \\sigma _k-\\frac{1}{2\\sigma^2(y_j-\\mu_k)^2}\\right]\\right\\}\\right\\}\\\\=&\\sum_{k=1}^K\\left\\{\\color{red}{\\sum_{j=1}^{N}(}\\color{green}E\\color{red}{\\gamma_{jk})}\\log \\alpha_k+\\color{blue}{\\sum_{j=1}^N}(\\color{green}{E}\\color{blue}{\\gamma _{jk}})\\left[\\log \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\log \\sigma _k-\\frac{1}{2\\sigma^2(y_j-\\mu_k)^2}\\right]\\right\\}\\\\\\end{aligned}$$\n",
    "\n",
    "$$\\begin{aligned}\\hat \\gamma _{jk}= &\\color{purple}{E(\\gamma_{jk}|y,\\theta)=P(\\gamma_{jk}=1|y,\\theta)}\\\\=&\\frac{P(\\gamma_{jk}=1,y_j|\\theta)}{\\sum_{k=1}^KP(\\gamma_{jk}=1,y_j|\\theta)}\\\\=&\\frac{P(y_j|\\color{red}{\\gamma_{jk}=1,\\theta})\\color{green}{P(\\gamma_{jk}=1|\\theta)}}{\\sum_{k=1}^KP(y_j|\\gamma_{jk}=1,\\theta)P(\\gamma_{jk}=1|\\theta)}\\\\=&\\frac{\\color{green}{\\alpha_k}\\phi(y_j|\\color{red}{\\theta_k)}}{\\sum_{k=1}^K\\alpha_k\\phi(y_j|\\theta_k)}\\end{aligned}$$\n",
    "- 注意这里$E(\\gamma_{jk}|y,\\theta)$，记为$\\hat\\gamma_{jk}$， 对应了E步求的**期望**中的一部分。\n",
    "\n",
    "- $\\hat \\gamma_{jk}$为分模型$k$对观测数据$y_j$的**响应度**, 即当前模型参数下第j个观测数据来自第k个分模型的概率。这里，紫色标记的第一行参考伯努利分布的期望。\n",
    "\n",
    "$$Q(\\theta,\\theta^{(i)})=\\sum_{k=1}^Kn_k\\log \\alpha_k+\\sum_{j=1}^N\\hat \\gamma_{jk}\\left[\\log \\left(\\frac{1}{\\sqrt{2\\pi}}\\right)-\\log \\sigma_k-\\frac{1}{2\\sigma_k^2}(y_j-\\mu_k)^2\\right]$$\n",
    "其中$i$表示第$i$步迭代\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. M步\n",
    "\n",
    "求函数$Q(\\theta,\\theta^{(i)})$对$\\theta$的极大值，分别求$\\sigma, \\mu, \\alpha$\n",
    "$$\n",
    "\\theta^{(i+1)}=\\arg\\max_\\theta Q(\\theta,\\theta^{(i)})\n",
    "$$\n",
    "\n",
    "- $\\arg\\max$ 就是求Q的极值对应的参数$\\theta$，如说是离散的，遍历所有值，最大查找，如果是连续的，偏导为零求极值。\n",
    "- $\\frac {\\partial Q}{\\partial \\mu_k}=0, \\frac {\\partial{Q}}{\\partial{\\sigma^2}}= 0$  得到$\\hat\\mu_k, \\hat \\sigma_k^2$\n",
    "- 条件$\\sum_{k=1}^K\\alpha_k=1, 根据拉格朗日乘子法, 得到$\\hat\\alpha_k$\n",
    "\n",
    "$$\\hat \\mu_k = \\frac {\\sum_{j=1}^N\\hat \\gamma_{jk}y_j}{\\sum_{j=1}^N\\hat \\gamma_{jk}}$$\n",
    "$$\\hat \\sigma_k^2 = \\frac {\\sum_{j=1}^N\\hat \\gamma_{jk}(y_j-\\mu_k)^2}{\\sum_{j=1}^N\\hat \\gamma_{jk}}$$\n",
    "$$\\hat \\alpha_k= \\frac {n_k}{N} = \\frac {\\sum_{j=1}^N \\hat \\gamma_{jk}}{N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M step:\n",
    "\n",
    "$$\\pi^{i+1}=\\frac{1}{n}\\sum_{j=1}^n\\mu^{i+1}_j$$\n",
    "\n",
    "$$p^{i+1}=\\frac{\\sum_{j=1}^n\\mu^{i+1}_jy_i}{\\sum_{j=1}^n\\mu^{i+1}_j}$$\n",
    "\n",
    "$$q^{i+1}=\\frac{\\sum_{j=1}^n(1-\\mu^{i+1}_jy_i)}{\\sum_{j=1}^n(1-\\mu^{i+1}_j)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 例9.1  3枚硬币的例子\n",
    "class EM:\n",
    "    def __init__(self, prob, max_iter=None):\n",
    "        self.pro_A, self.pro_B, self.pro_C = prob\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    # e_step\n",
    "    def pmf(self, i):\n",
    "        pro_1 = self.pro_A * math.pow(self.pro_B, data[i]) * math.pow(\n",
    "            (1 - self.pro_B), 1 - data[i])\n",
    "        pro_2 = (1 - self.pro_A) * math.pow(self.pro_C, data[i]) * math.pow(\n",
    "            (1 - self.pro_C), 1 - data[i])\n",
    "        return pro_1 / (pro_1 + pro_2)\n",
    "\n",
    "    # m_step\n",
    "    def fit(self, data):\n",
    "        count = len(data)\n",
    "        print('init prob:{}, {}, {}'.format(self.pro_A, self.pro_B,\n",
    "                                            self.pro_C))\n",
    "        iter_num = self.max_iter or count\n",
    "        for d in range(iter_num):\n",
    "            _ = yield\n",
    "            _pmf = [self.pmf(k) for k in range(count)]\n",
    "            pro_A = 1 / count * sum(_pmf)\n",
    "            pro_B = sum([_pmf[k] * data[k] for k in range(count)]) / sum(\n",
    "                [_pmf[k] for k in range(count)])\n",
    "            pro_C = sum([(1 - _pmf[k]) * data[k]\n",
    "                         for k in range(count)]) / sum([(1 - _pmf[k])\n",
    "                                                        for k in range(count)])\n",
    "            print('{}/{}  pro_a:{:.3f}, pro_b:{:.3f}, pro_c:{:.3f}'.format(\n",
    "                d+1, count, pro_A, pro_B, pro_C))\n",
    "            self.pro_A = pro_A\n",
    "            self.pro_B = pro_B\n",
    "            self.pro_C = pro_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[1,1,0,1,0,0,1,0,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = EM(prob=[0.5, 0.5, 0.5], max_iter=6)\n",
    "f = em.fit(data)\n",
    "next(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "while True:\n",
    "    try:\n",
    "        f.send(i)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一次迭代\n",
    "f.send(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二次\n",
    "f.send(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = EM(prob=[0.4, 0.6, 0.7])\n",
    "f2 = em.fit(data)\n",
    "next(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "while i < 20:\n",
    "    try:\n",
    "        f2.send(i)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.send(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2.send(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Docstring:\n",
    "binomial(n, p, size=None)\n",
    "\n",
    "Draw samples from a binomial distribution.\n",
    "\n",
    "Samples are drawn from a binomial distribution with specified\n",
    "parameters, n trials and p probability of success where\n",
    "n an integer >= 0 and p is in the interval [0,1]. (n may be\n",
    "input as a float, but it is truncated to an integer in use)\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "n : int or array_like of ints\n",
    "    Parameter of the distribution, >= 0. Floats are also accepted,\n",
    "    but they will be truncated to integers.\n",
    "p : float or array_like of floats\n",
    "    Parameter of the distribution, >= 0 and <=1.\n",
    "size : int or tuple of ints, optional\n",
    "    Output shape.  If the given shape is, e.g., ``(m, n, k)``, then\n",
    "    ``m * n * k`` samples are drawn.  If size is ``None`` (default),\n",
    "    a single value is returned if ``n`` and ``p`` are both scalars.\n",
    "    Otherwise, ``np.broadcast(n, p).size`` samples are drawn.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.random.binomial(2000, .5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "参考代码：https://github.com/wzyonggege/statistical-learning-method\n",
    "\n",
    "中文注释制作：机器学习初学者\n",
    "\n",
    "微信公众号：ID:ai-start-com\n",
    "\n",
    "配置环境：python 3.5+\n",
    "\n",
    "代码全部测试通过。\n",
    "![gongzhong](../gongzhong.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
