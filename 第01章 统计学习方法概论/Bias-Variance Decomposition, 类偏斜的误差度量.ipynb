{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 偏置-方差 分解\n",
    "---\n",
    "偏差-方差分解(Bias-Variance Decomposition)是统计学派看待模型复杂度的观点。Bias-variance分解是机器学习中一种重要的分析技术。给定学习目标和训练集规模，它可以把一种学习算法的期望误差分解为三个非负项的和，即样本真实噪音noise、偏差bias和方差variance。\n",
    "\n",
    "noise 本真噪音是任何学习算法在该学习目标上的期望误差的下界；( 任何方法都克服不了的误差)\n",
    "bias 度量了某种学习算法的平均估计结果所能逼近学习目标的程度；（独立于训练样本的误差，刻画了匹配的准确性和质量：一个高的偏置意味着一个坏的匹配）\n",
    "variance 则度量了在面对同样规模的不同训练集时，学习算法的估计结果发生变动的程度。（相关于观测样本的误差，刻画了一个学习算法的精确性和特定性：一个高的方差意味着一个弱的匹配）\n",
    "\n",
    "偏差度量了学习算法期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力；方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；噪声表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度……泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。给定学习任务，为了取得好的泛化性能，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 期望损失(泛化误差)分解\n",
    "\n",
    "expected loss = bias^2 + variance + noise\n",
    "\n",
    "具体推导过程, 见 2.5偏差与方差(机器学习-周志华)\n",
    "\n",
    "偏差、方差和噪声的含义\n",
    "- 偏差：度量了模型的期望预测和真实结果的偏离程度，刻画了模型本身的拟合能力。\n",
    "\n",
    "- 方差：度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。\n",
    "\n",
    "- 噪声：表达了当前任务上任何模型所能达到的期望泛化误差的下界，刻画了学习问题本身的难度。\n",
    "\n",
    "## 偏差-方差窘境\n",
    "\n",
    "在偏置和方差之间有一个折中。对于非常灵活的模型来说,偏置较小,方差较大。对于相对固定的模型来说,偏置较大,方差较小。有着最优预测能力的模型时在偏置和方差之间取得最优的平衡的模型。\n",
    "\n",
    "灵活的模型(次数比较高的多项式)会有比较低的偏置和比较高的方差，而比较严格的模型(比如一次线性回归)就会得到比较高的偏置和比较低的方差。\n",
    "\n",
    "具体实例参考sklearn官方例子: [Single estimator versus bagging: bias-variance decomposition](https://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考:\n",
    "\n",
    "机器学习-周志华\n",
    "\n",
    "[偏置方差分解Bias-variance Decomposition](https://blog.csdn.net/pipisorry/article/details/50638749)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 减少方差和偏差的方法\n",
    "\n",
    "1. 获得更多的训练实例——解决高方差\n",
    "2. 尝试减少特征的数量——解决高方差\n",
    "3. 尝试获得更多的特征——解决高偏差\n",
    "4. 尝试增加多项式特征——解决高偏差\n",
    "5. 尝试减少正则化程度 λ——解决高偏差\n",
    "6. 尝试增加正则化程度 λ——解决高方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类偏斜的误差度量\n",
    "\n",
    "查准率(Precision)与查全率(Recall)\n",
    "1. **正确肯定（True Positive,TP）**：预测为真，实际为真\n",
    "2. **正确否定（True Negative,TN）**：预测为假，实际为假\n",
    "3. **错误肯定（False Positive,FP）**：预测为真，实际为假\n",
    "4. **错误否定（False Negative,FN）**：预测为假，实际为真\n",
    "\n",
    "则 \n",
    "- 查准率(Precision) = **TP/(TP+FP)**.例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好.\n",
    "- 查全率(Recall)=**TP/(TP+FN)**。例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好.\n",
    "\n",
    "如果我们希望只在非常确信的情况下预测为真（肿瘤为恶性），即我们希望更高的查准率，我们可以使用比 0.5 更大的阀值，如 0.7，0.9。这样做我们会减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。\n",
    "\n",
    "如果我们希望提高查全率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，我们可以使用比 0.5 更小的阀值，如 0.3.\n",
    "\n",
    "F1-score: 精确率和召回率的均值, 即:\n",
    "$$\\frac 2 {F_1} = \\frac 1 {P} + \\frac 1 {R}$$\n",
    "\n",
    "\n",
    "|            |              | **预测值**   |             |\n",
    "| ---------- | ------------ | ------------ | ----------- |\n",
    "|            |              | **Positive** | **Negtive** |\n",
    "| **实际值** | **Positive** | **TP**       | **FN**      |\n",
    "|            | **Negtive**  | **FP**       | **TN**      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
