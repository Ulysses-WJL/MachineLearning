<?xml version="1.0" encoding="UTF-8" standalone="no"?><map version="0.8.1"><node CREATED="1577350205962" ID="0ib1pi8pahkvps7j55qktc90v0" MODIFIED="1577350205962" TEXT="集成学习ensemble learning"><node CREATED="1577350205962" ID="7prjh547066t12p2r06ors2hsc" MODIFIED="1577350205962" POSITION="right" TEXT="集成方法"><node CREATED="1577350205962" FOLDER="true" ID="5u18j9qjvi4m0tlmk9ohetv74h" MODIFIED="1577350205962" TEXT="Boosting提升方法, 串行方法, AdaBoost自适应boosting"><node CREATED="1577350205962" ID="6e67mq5d76va501rkhefstvrd3" MODIFIED="1577350205962" TEXT="简述"><node CREATED="1577350205962" ID="60emj3luc4rn02jjov18apgrn2" MODIFIED="1577350205962" TEXT="将弱学习器提升为强学习器的算法:先从初始训练集训练出一个基学习器, 再根据基学习器的表现对训练样本权值分布进行调整, 使得先前基学习器做错的训练样本在后续受到更多关注, 然后基于调整后的样本分布来训练下一个基学习器. 最后将这些基本分类器线性组合，构成一个强分类器"/></node><node CREATED="1577350205962" ID="6a3cmbo55vn0vetihokcu59qgp" MODIFIED="1577350205962" TEXT="特点"><node CREATED="1577350205962" ID="1b3a3ttqdhlrbnumf7va2rh7t0" MODIFIED="1577350205962" TEXT="通过迭代每次学习一个基本分类器, 每次迭代中, 提高那些在前一轮被错分类的数据的权值. 最后线性组合时, 误差率大的基分类器权值小, 误差小的基分类器权值大"/></node><node CREATED="1577350205962" ID="5cb53qp69qc4qbrigi5pit47dt" MODIFIED="1577350205962" TEXT="AdaBoost算法分析"><node CREATED="1577350205962" ID="3oj53v91n9obdot668k8uic8fi" MODIFIED="1577350205962" TEXT="1. 关于 使用具有权值分布D_m的训练数据"><node CREATED="1577350205962" ID="5oribummhhjg0vairl1h4d2fn7" MODIFIED="1577350205962" TEXT=" 并不是改变样本数据, 划分方式也没有变（比如阈值分类器中的分类点的选取方式），变是评价每个划分错误的结果"/><node CREATED="1577350205962" ID="0ms05lbpcaki1eatc74f5fm405" MODIFIED="1577350205962" TEXT="不同的权值分布上，不同样本错分对评价结果的贡献不同，分类器中分类错误的会被放大，分类正确的系数会减小"/></node><node CREATED="1577350205962" ID="2s2tl07qtdsq1d1pgpsarikp4e" MODIFIED="1577350205962" TEXT="2. 基分类器的分类误差率 将样本权值和分类器系数联系到一起"/></node><node CREATED="1577350205962" ID="20c696qeqvtoplvg2t422dlq0j" MODIFIED="1577350205962" TEXT="误差分析"><node CREATED="1577350205962" ID="700mtn079jfjeg2jmkukb8m68d" MODIFIED="1577350205962" TEXT="1. 在训练过程中不断减小训练误差: 在每一轮使用适当的基学习器, 使误差下降最快"/><node CREATED="1577350205962" ID="3hlj5eft5vv2mpmgqh3n1tfpe7" MODIFIED="1577350205962" TEXT="2. 误差以指数速率下降"/></node><node CREATED="1577350205962" ID="02v98ro9p41jhltnv6b9ar8t51" MODIFIED="1577350205962" TEXT="AdaBoost算法的解释: 前向分步算法和指数损失"><node CREATED="1577350205962" ID="070lhbb4k8k0r5k63h94f5mv33" MODIFIED="1577350205962" TEXT="可以认为AdaBoost算法是加法模型, 损失函数为指数函数, 学习算法为前向分步算法时的二类分类学习方法"/></node><node CREATED="1577350205962" ID="04grdtlp7mma7epld0l3l35h0c" MODIFIED="1577350205962" TEXT="提升树boosting tree: 前向分步算法, 加法模型, 基分类器为分类树或回归树"><node CREATED="1577350205962" ID="3tgd7nomjfk1egqui2ep6j83bo" MODIFIED="1577350205962" TEXT="01损失函数 二类分类问题&#9; 参照Adaboost算法"/><node CREATED="1577350205962" ID="3qar0uhehi7bq3cilnnoui4p28" MODIFIED="1577350205962" TEXT="平方损失函数 回归问题&#9;"><node CREATED="1577350205962" ID="6mpv31ji9hi7lhfnqm14icbsal" MODIFIED="1577350205962" TEXT="下一步学习器 拟合当前模型的残差"/></node><node CREATED="1577350205962" ID="3s5iuqek8h3qsiec8420rhc510" MODIFIED="1577350205962" TEXT="一般损失函数 梯度提升GBDT"><node CREATED="1577350205962" ID="5h711f3qdtpqf4phntkk34ldcr" MODIFIED="1577350205962" TEXT="使用损失函数的负梯度在当前模型的值, 作为残差的估计"/></node></node><node CREATED="1577350205962" ID="6f8s526kslr8pdv5g84lavr952" MODIFIED="1577350205962" TEXT="例子: 图片(苹果)识别"/></node><node CREATED="1577350205962" FOLDER="true" ID="3cllo93j8ocoqq7bbk2636s9ml" MODIFIED="1577350205962" TEXT="Bagging装袋(Boostrap Aggregating), 并行化方法"><node CREATED="1577350205962" ID="708tm0rng3hv9fop9u7ceuccit" MODIFIED="1577350205962" TEXT="简述"><node CREATED="1577350205962" ID="1601oe8vaaed5bos8ijt63bkt0" MODIFIED="1577350205962" TEXT="从包含m个样本的数据集中可生成T个含m个样本的训练集, 然后基于每个训练集训练出一个基学习器, 然后使用blending的方法进行组合(经常选择简单投票, Uniformly Blending)."/></node><node CREATED="1577350205962" ID="68fqoove69o65hqj406rat3ui9" MODIFIED="1577350205962" TEXT="bootstrap sampling 自助采样, 无放回采样"/><node CREATED="1577350205962" ID="4na4n7snlr1vrqrm4h5jc9dc5f" MODIFIED="1577350205962" TEXT="Out-Of-Bag Estimate包外验证  self-validation"/><node CREATED="1577350205962" ID="138v4pfcd3siu4lbovks6thqdu" MODIFIED="1577350205962" TEXT="随机森林RandomForest"><node CREATED="1577350205962" ID="16e6t3jlccj4d8cclhal2r638b" MODIFIED="1577350205962" TEXT="优点"><node CREATED="1577350205962" ID="57q1fq5fkiasr0r20uejnpu6et" MODIFIED="1577350205962" TEXT="第一，不同决策树可以由不同主机并行训练生成，效率很高；"/><node CREATED="1577350205962" ID="1ua05mj82gac2nuq8rcatd3ce2" MODIFIED="1577350205962" TEXT="第二，随机森林算法继承了C&amp;RT的优点；"/><node CREATED="1577350205962" ID="0v631tn5orlcvikeeb59bujfe6" MODIFIED="1577350205962" TEXT="第三，将所有的决策树通过bagging的形式结合起来，避免了单个决策树造成过拟合的问题"/></node><node CREATED="1577350205962" ID="5sim72h7vehmj9al93htkbbv80" MODIFIED="1577350205962" TEXT="缺点: 劣势在于模型大小、是个很难去解释的黑盒子"/><node CREATED="1577350205962" ID="7lg1jfh7abs1m44bv852qegv7p" MODIFIED="1577350205962" TEXT="构造方法 RF = bagging + random-subspace C&amp;RT"><node CREATED="1577350205962" ID="1t969c4mm7nc4rroiag688oi1k" MODIFIED="1577350205962" TEXT="1. 数据样本随机化"/><node CREATED="1577350205962" ID="4ni82dgh4ona3mf2fp4bu8r7pj" MODIFIED="1577350205962" TEXT="2. 特征选择随机化"/></node><node CREATED="1577350205962" ID="4p62coqj06uu6lk2v4oehetmgr" MODIFIED="1577350205962" TEXT="与单一决策树的对比"><node CREATED="1577350205962" ID="63c74t25vaofvrns977trkltac" MODIFIED="1577350205962" TEXT="Decision Tree有增大方差的特性, 使用bagging方法可以减少方差"/></node></node><node CREATED="1577350205962" ID="7la15ud840rgqtc5apuljfklf0" MODIFIED="1577350205962" TEXT="例子: 声纳信号分析"/><node CREATED="1577350205962" ID="15irjjcpdujqeelf50mohmfpge" MODIFIED="1577350205962" TEXT="自由主题"/></node></node><node CREATED="1577350205962" ID="5e328v0d9l5gq2g0rn7fsilag8" MODIFIED="1577350205962" POSITION="right" TEXT="Blending结合策略"><node CREATED="1577350205962" ID="6a4pkfsue0l9e5mmmlqngg3fkk" MODIFIED="1577350205962" TEXT="validation 选一个正确率最高的"/><node CREATED="1577350205962" ID="08u203pmfphrucf10q4us36i03" MODIFIED="1577350205962" TEXT="平均法, 数值型输出"><node CREATED="1577350205962" ID="2obupijdfc45ldst8mq6866gnq" MODIFIED="1577350205962" TEXT="简单平均法simple averaging"/><node CREATED="1577350205962" ID="2oqmricntr8s7a2clp5rt1rllp" MODIFIED="1577350205962" TEXT="加权平均法weighted averaging"/></node><node CREATED="1577350205962" ID="0jk6j583m67l7i8fl205sbnuba" MODIFIED="1577350205962" TEXT="投票法, 分类任务"><node CREATED="1577350205962" ID="453hdvvnjlc0bctkk2bdksqdiv" MODIFIED="1577350205962" TEXT="绝对多数投票majority voting"/><node CREATED="1577350205962" ID="0kg29v2459kv0ld6khvp699lab" MODIFIED="1577350205962" TEXT="相对多数投票法plurality voting"/><node CREATED="1577350205962" ID="6i4tq82405hnf985cfujkddfap" MODIFIED="1577350205962" TEXT="加权投票法weighted voting"/></node><node CREATED="1577350205962" ID="53knfrsb0jmbrtv1c2lmf67sig" MODIFIED="1577350205962" TEXT="学习法  代表为Stacking"/></node><node CREATED="1577350205962" ID="5v037mqu6kd5jta66qosvsi03f" MODIFIED="1577350205962" POSITION="right" TEXT="个体与集成"><node CREATED="1577350205962" ID="22gqto6mvkmbvqetl3jomn1iiv" MODIFIED="1577350205962" TEXT="弱学习器与强学习器"/></node><node CREATED="1577350205962" ID="5vkitgskesio73a2bupq3e51r9" MODIFIED="1577350205962" POSITION="left" TEXT="定义"><node CREATED="1577350205962" ID="4hlp1iuekjf4aiflilh52nrv0h" MODIFIED="1577350205962" TEXT="通过构建并结合多个学习器来完成学习任务"/></node></node></map>
