{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 广义线性模型 Generalized Linear Models\n",
    "---\n",
    "本章主要讲述一些用于回归的方法，其中目标值 y 是输入变量 x 的线性组合。 数学概念表示为：如果$\\hat{y}$是预测值，那么有：\n",
    "\n",
    "$\\hat{y}(w, x) = w^{(0)} + w^{(1)} x^{(1)} + ... + w^{(p)} x^{(p)}$\n",
    "\n",
    "在整个模块中，我们定义向量 $w = (w_1,..., w_p)$ 作为`coef_`(coefficient, 回归系数)，定义 $w_0$ 作为 `intercept_`截距.\n",
    "线性模型虽简单, 却有丰富的变化.考虑单调可微函数$g(\\cdot)$\n",
    "($g(\\cdot)$连续且充分光滑), 令\n",
    "\n",
    "$$y = g^{-1}(w^Tx+b)$$\n",
    "这样得到的模型称为`广义线性模型`,函数$g(\\cdot)$称为`联系函数`(link function). 对数线性回归(Logistic Regression)是$g(\\cdot)=ln(\\cdot)$时的特例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归\n",
    "---\n",
    "\n",
    "回归的目的是预测数值型的目标值。最直接的办法是依据输入写出一个目标值的计算公式。\n",
    "\n",
    "假如你想要预测兰博基尼跑车的功率大小，可能会这样计算:\n",
    "\n",
    "HorsePower = 0.0015 * annualSalary - 0.99 * hoursListeningToPublicRadio\n",
    "\n",
    "这就是所谓的 `回归方程(regression equation)`，其中的 0.0015 和 -0.99 称作 `回归系数（regression weights）`，求这些回归系数的过程就是回归。\n",
    "\n",
    "给定数据集 $D = \\{(x_1, y_1), (x_2, y_2), \\cdots, (x_n, y_n)\\}$, 其中$x_i=(x_i^{(1)}, x_i^{(2)}, \\cdots, x_i^{(d)}), y_i \\in \\mathbb{R}$. 线性回归(linear regression)试图学得一个线性模型以尽可能准确地预测实值输出标记.\n",
    "$$f(x_i) = wx_i, 使得f(x_i)\\simeq y_i$$\n",
    "\n",
    "对于离散属性, 若属性值之间存在`序`(order)关系, 可通过连续化将其转为连续值; 若属性值间不存在序关系,假定有k个属性值, 则通常将其转为k维向量(OneHotEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 普通最小二乘法 Ordinary Least Squares\n",
    "---\n",
    "LinearRegression 拟合一个带有系数$w = (w^{(0)}, ...,w^{(p)})$的线性模型，使得数据集实际观测数据和预测数据（估计值）之间的残差平方和(对应了常用的欧几里得距离Euclidean distance)最小。其数学表达式为:\n",
    "$$\\underset{w}{min} {|| X w - y||_2}^2$$\n",
    "求解w使$E_{w} = \\sum_{i=1}^{p}(w^Tx_i-y_i)^2$(loss function)最小化的过程, 称为线性回归模型的最小二乘`参数估计`(parameter estimation). $E_{w}$是关于$w$的凸函数, 可以令其对$w$并令导数为0, 得到$w$的最优解的闭式(closed-form)解.\n",
    "\n",
    "用矩阵形式可以写成$E_{w} = (y-Xw)^T(y-Xw)$, 对$w$求导得到:\n",
    "$\\frac {\\partial E_w}{\\partial w} = 2X^T(Xw-y)$, 令其为0得到$\\hat w = (X^TX)^{-1}X^Ty$\n",
    "\n",
    "需要对矩阵求逆，因此这个方程只在逆矩阵存在的时候适用，我们在程序代码中对此作出判断。 判断矩阵是否可逆的一个可选方案是:\n",
    "\n",
    "判断矩阵的行列式是否为 0，若为0，矩阵就不存在逆矩阵，不为0的话，矩阵才存在逆矩阵。\n",
    "\n",
    "[矩阵求导参考1](https://blog.csdn.net/daaikuaichuan/article/details/80620518)\n",
    "\n",
    "[矩阵求导参考2](http://blog.csdn.net/nomadlx53/article/details/50849941)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [机器学习实战](https://github.com/apachecn/AiLearning/blob/master/docs/ml/8.%E5%9B%9E%E5%BD%92.md)\n",
    "___\n",
    "数据格式\n",
    "```\n",
    "x0       x1       y\n",
    "1.000000\t0.067732\t3.176513\n",
    "1.000000\t0.427810\t3.816464\n",
    "1.000000\t0.995731\t4.550095\n",
    "1.000000\t0.738336\t4.256571\n",
    "1.000000\t0.981083\t4.560815\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "np.set_printoptions(threshold=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data.txt')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.all(data[:, 0]==1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取X 和 y\n",
    "X, y = data[:, [1]], data[:, [2]]\n",
    "X  # 保持 X 为 n_sample * n_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.mat(y).ravel()\n",
    "y1.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.intercept_ = None\n",
    "        self.coef_ = None\n",
    "\n",
    "    @staticmethod\n",
    "    def transform(X):\n",
    "        ones = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((ones, X), axis=1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def least_squares(X, y):\n",
    "        A = X.T * X\n",
    "        if np.linalg.det(A) == 0.0:\n",
    "            print('...')\n",
    "        w = A.I * X.T * y\n",
    "        return w\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = self.transform(deepcopy(X))\n",
    "        X = np.mat(X)\n",
    "        y = np.mat(y[:, np.newaxis])\n",
    "        weights = self.least_squares(X, y)  # n_feature * 1\n",
    "        self.intercept_ = weights[0, 0]\n",
    "        self.coef_ = np.array(weights[1:, 0]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearRegression()\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression1():\n",
    "    data = np.loadtxt('data.txt')\n",
    "    X, y = data[:, [1]], data[:, 2]\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(X, y)\n",
    "    print(clf.coef_, clf.intercept_)\n",
    "    \n",
    "    def line_(x0):\n",
    "        return clf.coef_[0] * x0 + clf.intercept_\n",
    "    \n",
    "    def corr_(X, y):\n",
    "        # 使用预测值f(x)和y的相关系数来表示, 预测值和实际值的匹配程度\n",
    "        corr = np.corrcoef(line_(X).ravel(), y)\n",
    "        print(f'相关系数:\\n{corr}\\n')\n",
    "        \n",
    "    corr_(X, y)\n",
    "    plt.figure()\n",
    "    plt.scatter(X[:,0], y[:], c='r', edgecolors='k')\n",
    "    xmin, xmax = X[:, 0].min(), X[:, 0].max()\n",
    "    plt.plot([xmin, xmax], [line_(xmin), line_(xmax)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 局部加权线性回归\n",
    "---\n",
    "线性回归的一个问题是有可能出现欠拟合现象，因为它求的是具有最小均方差的无偏估计。显而易见，如果模型欠拟合将不能取得最好的预测效果。所以有些方法允许在估计中引入一些偏差，从而降低预测的均方误差。\n",
    "\n",
    "一个方法是`局部加权线性回归`（Locally Weighted Linear Regression，LWLR）。在这个算法中，我们给预测点附近的每个点赋予一定的权重()，然后与 线性回归 类似，在这个子集上基于最小均方误差来进行普通的回归。我们需要最小化的目标函数大致为:\n",
    "$$\\sum_i w^{(i)}(y^{(i)}-\\theta^Tx^{(i)})^2$$\n",
    "参考最小二乘法,\n",
    "$$\\begin{aligned}J(\\theta)\n",
    "&= \\frac 1 2 \\sum_{i=1}^m w^{(i)}(y^{(i)}-h_{\\theta}(x^{(i)}))^2 \\\\\n",
    "&= \\frac 1 2 (X\\theta - y)^TW(X\\theta-y)\\end{aligned}$$\n",
    "$J(\\theta)$对$\\theta$求导:\n",
    "$$\\nabla_{\\theta}J(\\theta) = X^TWX\\theta - X^TWy$$\n",
    "该算法解出回归系数 $\\theta$ 的形式如下:\n",
    "$$\\hat \\theta = (X^TWX)^{-1}X^TWy$$\n",
    "目标函数中 w 为权重，不是回归系数。与 kNN 一样，这种算法每次预测均需要事先选取出对应的数据子集。\n",
    "\n",
    "LWLR 使用 “核”（与支持向量机中的核类似）来对附近的点赋予更高的权重。核的类型可以自由选择，最常用的核就是高斯核，高斯核对应的权重如下:\n",
    "$$w(i, i) = exp\\left(\\frac {(x^{(i)}-x)^2}{-2k^2}\\right)$$\n",
    "\n",
    "这样就构建了一个只含对角元素的权重矩阵 w，并且点 x 与 x(i) 越近，w(i, i) 将会越大。上述公式中包含一个需要用户指定的参数 k ，它决定了对附近的点赋予多大的权重，这也是使用 LWLR 时唯一需要考虑的参数，下面的图给出了参数 k 与权重的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(9).reshape(3,3)\n",
    "np.linalg.norm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(9)\n",
    "a * a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_w_(x, k):\n",
    "    # 预测点x=.5\n",
    "    return np.exp((x-.5).T * x / (-2 * k**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "ax1, ax2, ax3, ax4 = figure.subplots(4)\n",
    "ax1.scatter(X[:,0], y[:], c='r', edgecolors='k')\n",
    "x_ = np.linspace(0, 1, 500)\n",
    "print(plt_w_(x_, 0.5))\n",
    "ax2.plot(x_, plt_w_(x_, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_, plt_w_(x_, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
