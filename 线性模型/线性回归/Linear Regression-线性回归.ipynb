{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 广义线性模型 Generalized Linear Models\n",
    "---\n",
    "本章主要讲述一些用于回归的方法，其中目标值 y 是输入变量 x 的线性组合。 数学概念表示为：如果$\\hat{y}$是预测值，那么有：\n",
    "\n",
    "$\\hat{y}(\\theta, x) = \\theta^{(0)} + \\theta^{(1)} x^{(1)} + ... + \\theta^{(p)} x^{(p)}$\n",
    "\n",
    "在整个模块中，我们定义向量 $w = (\\theta_1,..., \\theta_p)$ 作为`coef_`(coefficient, 回归系数)，定义 $\\theta_0$ 作为 `intercept_`截距.\n",
    "线性模型虽简单, 却有丰富的变化.考虑单调可微函数$g(\\cdot)$\n",
    "($g(\\cdot)$连续且充分光滑), 令\n",
    "\n",
    "$$y = g^{-1}(\\theta^Tx+b)$$\n",
    "这样得到的模型称为`广义线性模型`,函数$g(\\cdot)$称为`联系函数`(link function). 对数线性回归(Logistic Regression)是$g(\\cdot)=ln(\\cdot)$时的特例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归\n",
    "---\n",
    "\n",
    "回归的目的是预测数值型的目标值。最直接的办法是依据输入写出一个目标值的计算公式。\n",
    "\n",
    "假如你想要预测兰博基尼跑车的功率大小，可能会这样计算:\n",
    "\n",
    "HorsePower = 0.0015 * annualSalary - 0.99 * hoursListeningToPublicRadio\n",
    "\n",
    "这就是所谓的 `回归方程(regression equation)`，其中的 0.0015 和 -0.99 称作 `回归系数（regression weights）`，求这些回归系数的过程就是回归。\n",
    "\n",
    "给定数据集 $D = \\{(x_1, y_1), (x_2, y_2), \\cdots, (x_n, y_n)\\}$, 其中$x_i=(x_i^{(1)}, x_i^{(2)}, \\cdots, x_i^{(d)}), y_i \\in \\mathbb{R}$. 线性回归(linear regression)试图学得一个线性模型以尽可能准确地预测实值输出标记.\n",
    "$$f(x_i) = \\theta^T x_i, 使得f(x_i)\\simeq y_i$$\n",
    "\n",
    "对于离散属性, 若属性值之间存在`序`(order)关系, 可通过连续化将其转为连续值; 若属性值间不存在序关系,假定有k个属性值, 则通常将其转为k维向量(OneHotEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 普通最小二乘法 Ordinary Least Squares\n",
    "---\n",
    "LinearRegression 拟合一个带有系数$w = (\\theta^{(0)}, ...,\\theta^{(p)})$的线性模型，使得数据集实际观测数据和预测数据（估计值）之间的残差平方和(对应了常用的欧几里得距离Euclidean distance)最小。其数学表达式为:\n",
    "$$\\underset{\\theta}{min} {|| X\\theta - y||_2}^2$$\n",
    "求解\\theta使$E_{\\theta} = \\sum_{i=1}^{p}(\\theta^Tx_i-y_i)^2$(loss function)最小化的过程, 称为线性回归模型的最小二乘`参数估计`(parameter estimation). $E_{\\theta}$是关于$w$的凸函数, 可以令其对$\\theta$并令导数为0, 得到$\\theta$的最优解的闭式(closed-form)解.\n",
    "\n",
    "用矩阵形式可以写成$E_{\\theta} = (y-X\\theta)^T(y-X\\theta)$, 对$w$求导得到:\n",
    "$\\frac {\\partial E_\\theta}{\\partial \\theta} = 2X^T(X\\theta-y)$, 令其为0得到$\\hat\\theta = (X^TX)^{-1}X^Ty$\n",
    "\n",
    "需要对矩阵求逆，因此这个方程只在逆矩阵存在的时候适用，我们在程序代码中对此作出判断。 判断矩阵是否可逆的一个可选方案是:\n",
    "\n",
    "判断矩阵的行列式是否为 0，若为0，矩阵就不存在逆矩阵，不为0的话，矩阵才存在逆矩阵。\n",
    "\n",
    "[矩阵求导参考1](https://blog.csdn.net/daaikuaichuan/article/details/80620518)\n",
    "\n",
    "[矩阵求导参考2](http://blog.csdn.net/nomadlx53/article/details/50849941)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例子来自**[机器学习实战](https://github.com/apachecn/AiLearning/blob/master/docs/ml/8.%E5%9B%9E%E5%BD%92.md)**\n",
    "___\n",
    "数据格式\n",
    "## 1. 线性回归\n",
    "```\n",
    "x0       x1       y\n",
    "1.000000\t0.067732\t3.176513\n",
    "1.000000\t0.427810\t3.816464\n",
    "1.000000\t0.995731\t4.550095\n",
    "1.000000\t0.738336\t4.256571\n",
    "1.000000\t0.981083\t4.560815\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingularMatrixError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data.txt')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取X 和 y\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "X  # 保持 X 为 n_sample * n_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.intercept_ = None\n",
    "        self.coef_ = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def least_squares(X, y):\n",
    "        A = X.T * X\n",
    "        if np.linalg.det(A) == 0.0:\n",
    "            print('...')\n",
    "        w = A.I * X.T * y\n",
    "        return w\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.mat(X)\n",
    "        y = np.mat(y[:, np.newaxis])\n",
    "        theta = self.least_squares(X, y)  # n_feature * 1\n",
    "        self.intercept_ = theta[0, 0]\n",
    "        self.coef_ = np.array(theta[1:, 0]).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression1():\n",
    "    data = np.loadtxt('data.txt')\n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    clf = LinearRegression()\n",
    "    clf.fit(X, y)\n",
    "    print(clf.coef_, clf.intercept_)\n",
    "    \n",
    "    def line_(x0):\n",
    "        return clf.coef_[0] * x0 + clf.intercept_\n",
    "    \n",
    "    def corr_(X, y):\n",
    "        # 使用预测值f(x)和y的相关系数来表示, 预测值和实际值的匹配程度\n",
    "        corr = np.corrcoef(line_(X[:, 1]).ravel(), y)\n",
    "        print(f'相关系数:\\n{corr}\\n')\n",
    "        \n",
    "    corr_(X, y)\n",
    "    plt.figure()\n",
    "    plt.scatter(X[:,1], y[:], c='r', edgecolors='k')\n",
    "    xmin, xmax = X[:, 1].min(), X[:, 1].max()\n",
    "    plt.plot([xmin, xmax], [line_(xmin), line_(xmax)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用 sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "data = np.loadtxt('data.txt')\n",
    "X, y = data[:, [1]], data[:, -1]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=123)\n",
    "clf = LinearRegression()\n",
    "clf.fit(X, y)\n",
    "clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 普通最小二乘法的复杂度\n",
    "该方法使用 X 的奇异值分解来计算最小二乘解。如果 X 是一个形状为 (n_samples, n_features)的矩阵，设$$n_{samples} \\geq n_{features}$$, 则该方法的复杂度为$$O(n_{samples} n_{fearures}^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 局部加权线性回归\n",
    "---\n",
    "线性回归的一个问题是有可能出现欠拟合现象，因为它求的是具有最小均方差的无偏估计。显而易见，如果模型欠拟合将不能取得最好的预测效果。所以有些方法允许在估计中引入一些偏差，从而降低预测的均方误差。\n",
    "\n",
    "一个方法是`局部加权线性回归`（Locally Weighted Linear Regression，LWLR）。在这个算法中，我们给预测点附近的每个点赋予一定的权重()，然后与 线性回归 类似，在这个子集上基于最小均方误差来进行普通的回归。我们需要最小化的目标函数大致为:\n",
    "$$\\sum_i w_i(y_i-\\theta^Tx_i)^2$$\n",
    "参考最小二乘法,\n",
    "$$\\begin{aligned}J(\\theta)\n",
    "&= \\sum_{i=1}^m w_i(y_i-h_{\\theta}(x_i))^2 \\\\\n",
    "&= (X\\theta - y)^TW(X\\theta-y)\\end{aligned}$$\n",
    "$J(\\theta)$对$\\theta$求导:\n",
    "$$\\nabla_{\\theta}J(\\theta) = 2X^TWX\\theta - 2X^TWy$$\n",
    "该算法解出回归系数 $\\theta$ 的形式如下:\n",
    "$$\\hat \\theta = (X^TWX)^{-1}X^TWy$$\n",
    "\n",
    "LWLR 使用 “核”（与支持向量机中的核类似）来对附近的点赋予更高的权重。核的类型可以自由选择，最常用的核就是高斯核，高斯核对应的权重如下:\n",
    "$$w(i, i) = exp\\left(\\frac {(x_i-x)^2}{-2k^2}\\right)$$\n",
    "\n",
    "这样就构建了一个只含对角元素的权重矩阵 w，并且点 x 与 $x_i$ 越近，w(i, i) 将会越大。上述公式中包含一个需要用户指定的参数 k ，它决定了对附近的点赋予多大的权重，这也是使用 LWLR 时唯一需要考虑的参数，下面的图给出了参数 k 与权重的关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_w_(x, k):\n",
    "    # 预测点x=.5\n",
    "    x_diff = x - .5\n",
    "    return np.exp(x_diff.T * x_diff / (-2 * k**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "ax1, ax2, ax3, ax4, ax5 = figure.subplots(5)\n",
    "ax1.scatter(X[:,0], y[:], c='r', edgecolors='k')\n",
    "x_ = np.linspace(0, 1, 500)\n",
    "print(plt_w_(x_, 0.5))\n",
    "ax2.plot(x_, plt_w_(x_, 0.5), label='k=0.5')\n",
    "ax2.legend()\n",
    "ax3.plot(x_, plt_w_(x_, 0.1), label='k=0.1')\n",
    "ax3.legend()\n",
    "ax4.plot(x_, plt_w_(x_, 0.01), label='k=0.01')\n",
    "ax4.legend()\n",
    "ax5.plot(x_, plt_w_(x_, 10), label='k=10')\n",
    "ax5.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的图是 每个点的权重图（假定我们正预测的点是 x = 0.5），最上面的图是原始数据集，第二个图显示了当 k = 0.5 时，大部分的数据都用于训练回归模型；而最下面的图显示当 k=0.01 时，仅有很少的局部点被用于训练回归模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression_LW:\n",
    "    def __init__(self, k=1):\n",
    "        self.k_ = k\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "    \n",
    "    @staticmethod\n",
    "    def weight(X, k, test_point):\n",
    "        # 计算每个测试点时, 其他点的权重\n",
    "        n_sample = X.shape[0]\n",
    "        w = np.eye(n_sample)\n",
    "        for i in range(n_sample):\n",
    "            diff_X = X[i,:] - test_point\n",
    "            w[i,i] = np.exp(diff_X * diff_X.T / (-2 * k**2))\n",
    "        return w\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.X = np.mat(X)\n",
    "        self.y = np.mat(y[:, np.newaxis])\n",
    "        \n",
    "    def predict(self, test_points):\n",
    "        y_pred = np.zeros(test_points.shape[0]) # n_sample, \n",
    "        \n",
    "        for i in range(test_points.shape[0]):\n",
    "            test_point = test_points[i, :]\n",
    "            w = self.weight(self.X, self.k_, test_point)\n",
    "            A = self.X.T * w * self.X\n",
    "            if np.linalg.det(A) == 0.0:\n",
    "                raise SingularMatrixError('奇异矩阵, 用其它方法求')\n",
    "            theta = A.I * self.X.T * w * self.y\n",
    "            y_pred[i] = test_point * theta  # 1*nf * n_f*1\n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression2():\n",
    "    data = np.loadtxt('data.txt')\n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    for ax, k in zip(fig.subplots(3), (1, 0.01, 0.003)):\n",
    "        clf = LinearRegression_LW(k)\n",
    "        clf.fit(X, y)\n",
    "        ax.scatter(X[:,1], y[:], c='r', edgecolors='k', s=2)\n",
    "        y_pred = clf.predict(X)\n",
    "        print(f'k:{k}, errorr: {((y_pred - y)**2).sum()}')\n",
    "        ind = X[:, 1].argsort()  # x 从小到大排序\n",
    "        ax.plot(X[:, 1][ind], y_pred[ind])  # 画出 预测的曲线\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图使用了 3 种不同平滑值绘出的局部加权线性回归的结果。上图中的平滑系数 k =1.0，中图 k = 0.01，下图 k = 0.003 。可以看到，k = 1.0 时的使所有数据等比重，其模型效果与基本的线性回归相同，k=0.01时该模型可以挖出数据的潜在规律，而 k=0.003时则考虑了太多的噪声，进而导致了过拟合现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例: 预测鲍鱼的年龄\n",
    "---\n",
    "将数据分为测试集和训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('abalone.txt')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abalone_age():\n",
    "    data = np.loadtxt('abalone.txt')\n",
    "    for i in range(10):\n",
    "        print(\"*\"*30)\n",
    "        print(f'第{i+1}次测试:')\n",
    "        X, y = data[300*i:300*i + 300, :-1], data[300 * i:300*i + 300, -1]  # 每次取300来测试\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=123)\n",
    "        def error_(y1, y2):\n",
    "            return ((y1-y2)**2).sum() / len(y1)\n",
    "        for k in (10, 1, 0.1):  # 不同的核\n",
    "            clf = LinearRegression_LW(k)\n",
    "            clf.fit(X_train, y_train)\n",
    "            try:\n",
    "                y_pred_train = clf.predict(X_train)\n",
    "                y_pred_test = clf.predict(X_test)\n",
    "                print(f\"k:{k}, 训练集误差: {error_(y_train, y_pred_train)}, 测试集误差: {error_(y_test, y_pred_test)}\")\n",
    "            except SingularMatrixError as e:\n",
    "                print(e.args)\n",
    "                continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone_age()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据我们上边的测试，可以看出:\n",
    "简单线性回归达到了与局部加权现行回归类似的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 缩减系数来 “理解” 数据\n",
    "---\n",
    "如果数据的特征比样本点还多应该怎么办？是否还可以使用线性回归和之前的方法来做预测？答案是否定的，即我们不能再使用前面介绍的方法。这是因为在计算$(X^TX)^{-1}$ 的时候会出错。\n",
    "\n",
    "如果特征比样本点还多(n_feature > n_sample)，也就是说输入数据的矩阵x可能不是**满秩矩阵**。非满秩矩阵求逆时会出现问题。\n",
    "\n",
    "为了解决这个问题，我们引入了`岭回归（ridge regression）`这种缩减方法。接着是`lasso法`，最后介绍`前向逐步回归`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(12).reshape(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.dot(a.T, a)\n",
    "np.linalg.det(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.reshape(4, 3)\n",
    "np.linalg.det(np.dot(b.T, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 1 岭回归 Ridge （L2 惩罚）\n",
    "---\n",
    "岭估计器是普通`LinearRegression`的简单正则化（称为`l2`惩罚）, 通过对系数的大小施加惩罚来解决普通最小二乘法的一些问题.  特别是，它具有的优点是，在计算上不比普通的最小二乘估计更昂贵. \n",
    "> L2范数是指向量中各元素的的平方和然后再求平方根。有人把它叫“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。\n",
    ">L2范数与L1不同，他不会让参数等于0，而是让每个参数都接近于0。那么L2范数又有什么好处呢？\n",
    "- 防止过拟合 : 一般的用法是在损失函数后面加上$\\theta$的L2范数。这是一种规则化。\n",
    "- 优化求解变得稳定快速: 简单地说他可以让$\\theta$在接近全局最优点$\\theta^*$的时候，还保持着较大的梯度。这样可以跳出局部最优，也使得收敛速度变快。\n",
    "\n",
    "\n",
    "岭回归最小化的是带罚项的残差平方和\n",
    "$$\\underset {\\theta}{min} ||X\\theta - y||_2^2 + \\alpha ||\\theta||_2^2$$\n",
    "其中， $\\alpha \\geq 0$ 是控制系数收缩量的复杂性参数： $\\alpha$ 的值越大，收缩量越大，模型对共线性的鲁棒性也更强。\n",
    "\n",
    "与上面的操作一样对$\\theta$求导, \n",
    "$$\\nabla_{\\theta}J(\\theta) = 2X^T(X\\theta-y) + 2\\alpha I\\theta$$\n",
    "得到最优的$\\hat \\theta = (X^TX+\\alpha I)^{-1}X^Ty$, 它是一个关于$\\alpha$的函数.  \n",
    "可以这样认为: 岭回归就是在矩阵$X^TX$ 上加一个$\\alpha I$从而使得矩阵非奇异，进而能对$X^TX+\\alpha I$求逆。其中矩阵I是一个 n_feature * n_feature（等于列数)的单位矩阵\n",
    "\n",
    "岭回归最先用来处理特征数多于样本数的情况，现在也用于在估计中加入偏差，从而得到更好的估计.通过引入惩罚项，能够减少不重要的参数，这个技术在统计学中也叫作 `缩减(shrinkage)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-2, 6, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard(X):\n",
    "    return (X - X.mean(0)) / X.var(0) \n",
    "data = np.loadtxt('abalone.txt')\n",
    "X, y = data[:, :-1], data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ridge(X, y, num=30):\n",
    "    y = y - y.mean()\n",
    "    X = (X - X.mean(0)) / X.var(0)\n",
    "    X = np.mat(X)\n",
    "    y = np.mat(y)\n",
    "    # 需要对数据进行标准化处理\n",
    "    I = np.eye(X.shape[1])\n",
    "    alpha_list = np.logspace(-10, num-11, num)\n",
    "    theta_list = []\n",
    "#     print(f\"X是奇异矩阵么: {'yes' if not np.linalg.det(X) else 'no'}\")\n",
    "    for alpha in alpha_list:\n",
    "        A = (np.dot(X.T, X) + alpha * I)\n",
    "        theta = A.I * X.T * y\n",
    "        theta_list.append(theta)\n",
    "    return theta_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('abalone.txt')\n",
    "X, y = data[:, :-1], data[:, [-1]]\n",
    "num = 30\n",
    "theta_list = test_ridge(X, y, num)\n",
    "alpha_list = np.logspace(-10, num-11, num)\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(X.shape[1]):\n",
    "    thetas = list(map(lambda x:x[i, 0], theta_list))\n",
    "    plt.plot(alpha_list, thetas)\n",
    "ax = plt.gca()\n",
    "# ax.invert_xaxis()\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.logspace(-10, 20-1, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "𝛼 的值越大，收缩量越大，模型对共线性的鲁棒性也更强. \n",
    "在最左边，即𝛼 最小时，可以得到所有系数的原始值（与线性回归一致）；而在右边，系数全部缩减为0；在中间部分的某值将可以取得最好的预测效果。为了定量地找到最佳参数值，还需要进行交叉验证。另外，要判断哪些变量对结果预测最具有影响力，在上图中观察它们对应的系数大小就可以了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Lasso（L1 惩罚）\n",
    "---\n",
    "**套索方法(Lasso，The Least Absolute Shrinkage and Selection Operator)**\n",
    "Lasso估计器可用于对系数施加稀疏性, 它在一些情况下是有用的，因为它倾向于使用具有较少参数值的情况，有效地减少给定解决方案所依赖变量的数量。 换句话说，如果我们认为许多特征不相关，那么我们会更喜欢它。这是通过所谓的 `l1` 惩罚来完成的。\n",
    "\n",
    "> L1范数是指向量中各个元素绝对值之和，也叫“稀疏规则算子”（Lasso regularization）\n",
    "\n",
    "其最小化的目标函数是\n",
    "$$\\underset {\\theta} {min} ||X\\theta-y||^2 + \\alpha||\\theta||_1$$\n",
    "\n",
    "在增加如下约束时，普通的最小二乘法回归会得到与岭回归一样的公式:\n",
    "\n",
    "$\\sum_{k=1}^n \\theta_k^2 \\leq \\alpha$\n",
    "\n",
    "上式限定了所有回归系数的平方和不能大于$\\alpha$。使用普通的最小二乘法回归在当两个或更多的特征相关时，可能会得到一个很大的正系数和一个很大的负系数。正是因为上述限制条件的存在，使用岭回归可以避免这个问题。\n",
    "\n",
    "与岭回归类似，另一个缩减方法lasso也对回归系数做了限定，对应的约束条件如下:\n",
    "\n",
    "$\\sum_{k=1}^n |\\theta_k| \\leq \\alpha$\n",
    "\n",
    "唯一的不同点在于，这个约束条件使用绝对值取代了平方和。虽然约束形式只是稍作变化，结果却大相径庭: 在$\\alpha$足够小的时候，一些系数会因此被迫缩减到0.这个特性可以帮助我们更好地理解数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 前向逐步回归 \n",
    "前向逐步回归算法可以得到与 lasso 差不多的效果，但更加简单。它属于一种贪心算法，即每一步都尽可能减少误差。一开始，所有权重都设置为 0，然后每一步所做的决策是对某个权重增加或减少一个很小的值\n",
    "伪代码如下:\n",
    "\n",
    "```\n",
    "数据标准化，使其分布满足 0 均值 和单位方差\n",
    "在每轮迭代过程中: \n",
    "    设置当前最小误差 lowestError 为正无穷\n",
    "    对每个特征:\n",
    "        增大或缩小:\n",
    "            改变一个系数得到一个新的 w\n",
    "            计算新 w 下的误差\n",
    "            如果误差 Error 小于当前最小误差 lowestError: 设置 Wbest 等于当前的 W\n",
    "        将 W 设置为新的 Wbest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_wise_abalone(iter_num=100, epsilon=0.01):\n",
    "    def standard_(X):\n",
    "        return (X - X.mean(0)) / X.var(0)\n",
    "    \n",
    "    def error_(y1, y2):\n",
    "        return ((y1-y2)**2).sum() / len(y1)\n",
    "    \n",
    "    def stage_wise(X, y, iter_num, epsilon):\n",
    "        n_sample, n_feature = X.shape\n",
    "        weights = np.zeros((n_feature, 1))\n",
    "        weights_best = weights.copy()\n",
    "        w_list = []\n",
    "        for i in range(iter_num):\n",
    "            error_least = np.inf\n",
    "            for j in range(n_feature):\n",
    "                for sign in (-1, +1):\n",
    "                    weights_new = weights.copy()\n",
    "                    weights_new[j] += sign * epsilon\n",
    "                    error_new = error_(np.dot(X, weights_new), y)\n",
    "                    if error_new < error_least:\n",
    "                        error_least = error_new\n",
    "                        weights_best = weights_new\n",
    "                weights = weights_best.copy()\n",
    "            w_list.append(weights)\n",
    "        return w_list, error_least\n",
    "    \n",
    "    data = np.loadtxt('abalone.txt')\n",
    "    X, y = data[:, :-1], data[:, [-1]]\n",
    "    X = standard_(X)\n",
    "    y = y - y.mean()\n",
    "\n",
    "    w_list, error_least = stage_wise(X, y, iter_num, epsilon)\n",
    "    print(f'最终的误差{error_least}')\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(X.shape[1]):\n",
    "        thetas = list(map(lambda x:x[i, 0], w_list))\n",
    "        plt.plot(range(len(w_list)), thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_wise_abalone(200, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逐步线性回归算法的主要优点在于它可以帮助人们理解现有的模型并作出改进。当构建了一个模型后，可以运行该算法找出重要的特征，这样就有可能及时停止对那些不重要特征的收集。最后，如果用于测试，该算法每100次迭代后就可以构建出一个模型，可以使用类似于10折交叉验证的方法比较这些模型，最终选择使误差最小的模型。\n",
    "\n",
    "当应用缩减方法（如逐步线性回归或岭回归）时，模型也就增加了偏差（bias），与此同时却减小了模型的方差。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
