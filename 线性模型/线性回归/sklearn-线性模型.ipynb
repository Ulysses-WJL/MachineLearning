{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 普通最小二乘法回归\n",
    "---\n",
    "最标准的线性模型是“普通最小二乘回归”，通常简称为“线性回归”。 它没有对coef_施加任何额外限制，因此当特征数量很大时，它会变得行为异常，并且模型会过拟合。\n",
    "$$\\underset{\\theta}{min} {|| X\\theta - y||_2}^2$$\n",
    "通过回归方程求导得到的最佳系数$\\hat\\theta = (X^TX)^{-1}X^Ty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=16)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**make_regression**\n",
    "```\n",
    "Generate a random regression problem.\n",
    "    n_samples=100,\n",
    "    n_features=100,\n",
    "    n_informative=10,\n",
    "    n_targets=1,\n",
    "    bias=0.0,\n",
    "    effective_rank=None,\n",
    "    tail_strength=0.5,\n",
    "    noise=0.0,\n",
    "    shuffle=True,\n",
    "    coef=False,\n",
    "    random_state=None,\n",
    "```\n",
    "```\n",
    "Returns\n",
    "-------\n",
    "X : array of shape [n_samples, n_features]\n",
    "    The input samples.\n",
    "\n",
    "y : array of shape [n_samples] or [n_samples, n_targets]\n",
    "    The output values.\n",
    "\n",
    "coef : array of shape [n_features] or [n_features, n_targets], optional\n",
    "    The coefficient of the underlying linear model. It is returned only if\n",
    "    coef is True.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只有10个是有用的特征 , 添加了噪声\n",
    "X, y,  = make_regression(n_samples=200, n_features=30, n_informative=10,\n",
    "                                  noise=100, coef=True, random_state=5)  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5, train_size=60, test_size=140)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R2 score, 用来计算[可决系数](https://baike.baidu.com/item/%E5%8F%AF%E5%86%B3%E7%B3%BB%E6%95%B0)(the coefficient of determination)**\n",
    "$$R^2(y, \\hat y) = 1 - \\frac {\\sum_{i=1}^{n\\_sample}(y_i - \\hat y_i)^2}{\\sum_{i=1}^{n\\_sample}(y_i - \\bar y)^2}$$\n",
    "其中$y_i$表示第i个样本的真实值, $\\hat y_i$ 为预测值,   \n",
    "$\\bar y = \\frac 1 {n\\_sample}\\sum_{i=1}^{n\\_sample} y_i$为实际值的均值\n",
    "R2 score的值为1.0时最佳, 也可能为负数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"R^2 on training set: %f\" % lr.score(X_train, y_train))\n",
    "print(\"R^2 on test set: %f\" % lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# 整体集合的R2 score\n",
    "r2_score(np.dot(X, true_coef), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "# 系数从大到小排序 画图比较\n",
    "coefficient_sorting = np.argsort(true_coef)[::-1]\n",
    "plt.plot(true_coef[coefficient_sorting], \"o\", label=\"true\")\n",
    "plt.plot(lr.coef_[coefficient_sorting], \"o\", label=\"linear regression\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**learning curve 学习曲线**\n",
    "---\n",
    "A learning curve shows the validation and training score of an estimator for varying numbers of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    LinearRegression(), X, y, train_sizes=np.linspace(.1, 1, 5), cv=5)\n",
    "train_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(est, X, y):\n",
    "    plt.figure()\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        LinearRegression(), X, y, train_sizes=np.linspace(.1, 1, 20), cv=5)\n",
    "    estimator_name = est.__class__.__name__\n",
    "    # 训练集的 训练集大小-socre 分数 曲线\n",
    "    line = plt.plot(train_sizes, train_scores.mean(axis=1), '--', label=f\"train scores {estimator_name}\")\n",
    "    plt.plot(train_sizes, test_scores.mean(axis=1), '-', label=f\"test scores {estimator_name}\")\n",
    "    plt.xlabel(\"Training set size\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylim(-0.1, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(LinearRegression(), X, y)\n",
    "# 不同训练集大小 训练出来的 结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**普通最小二乘法的复杂度**\n",
    "\n",
    "该方法使用 X 的奇异值分解来计算最小二乘解。如果 X 是一个形状为 (n_samples, n_features)的矩阵，设$$n_{samples} \\geq n_{features}$$, 则该方法的复杂度为$$O(n_{samples} n_{fearures}^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 岭回归（L2 惩罚）\n",
    "---\n",
    "岭估计器是普通LinearRegression的简单正则化（称为 l2 惩罚）。 特别是，它具有的优点是，在计算上不比普通的最小二乘估计更昂贵。\n",
    "$$\\underset {\\theta}{min} ||X\\theta - y||_2^2 + \\alpha ||\\theta||_2^2$$\n",
    "其中， $\\alpha \\geq 0$ 是控制系数收缩量的复杂性参数： $\\alpha$ 的值越大，收缩量越大，模型对共线性的鲁棒性也更强。\n",
    "\n",
    "最优的$\\hat \\theta = (X^TX+\\alpha I)^{-1}X^Ty$, 它是一个关于$\\alpha$的函数.  \n",
    "**岭回归的复杂度**\n",
    "这种方法与 `普通最小二乘法` 的复杂度是相同的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们加载一个不满秩（low effective rank）数据集来比较岭回归和线性回归。秩是矩阵线性无关组的数量，满秩是指一个$m \\times n$矩阵中行向量或列向量中现行无关组的数量等于$min(m,n)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建一个有3个自变量的数据集，但是其秩为2，因此3个自变量中有两个自变量存在相关性\n",
    "X, y = make_regression(n_samples=2000, n_features=3, effective_rank=2, noise=10)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression(lr, X, y):\n",
    "    n_sample, n_feature = X.shape\n",
    "    n_bootstraps = 1000 # 1000次\n",
    "    coefs = np.zeros((n_bootstraps, n_feature))\n",
    "    scores = np.zeros((n_bootstraps, 2))\n",
    "    \n",
    "    for i in range(n_bootstraps):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25)\n",
    "        lr.fit(X_train, y_train)\n",
    "        scores[i] = (lr.score(X_train, y_train), lr.score(X_test, y_test))\n",
    "        coefs[i] = lr.coef_\n",
    "    f, axes = plt.subplots(nrows=n_feature, sharex=True, sharey=True, figsize=(10, 8))\n",
    "    for i, ax in enumerate(axes):\n",
    "        # 频率分布直方图\n",
    "        ax.hist(coefs[:, i], alpha=.5)\n",
    "        ax.set_title(\"Coef {}\".format(i))\n",
    "    plt.show()\n",
    "    return coefs, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 普通的线性回归\n",
    "coefs_lr, scores_lr = plot_regression(LinearRegression(), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 岭回归\n",
    "coefs_ridge, scores_ridge = plot_regression(Ridge(), X, y)  # 正则化系数 alpha 默认1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然, 岭回归的系数更接近0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(coefs_ridge - coefs_lr, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从均值上看，线性回归比岭回归的系数要大很多。均值显示的差异其实是线性回归的系数隐含的偏差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_lr.var(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs_ridge.var(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lr.mean(0), scores_ridge.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "岭回归的系数方差也会小很多。这就是机器学习里著名的偏差-方差均衡(Bias-Variance Trade-off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化岭回归参数 \n",
    "---\n",
    "用OLS（普通最小二乘法）做回归也许可以显示两个变量之间的某些关系；但是，当alpha参数正则化之后，那些关系就会消失.\n",
    "\n",
    "在linear_models模块中，有一个对象叫RidgeCV，表示**岭回归交叉检验**（ridge cross-validation）。这个交叉检验类似于**留一交叉验证法**（leave-one-out cross-validation，LOOCV）\n",
    "\n",
    "指定cv属性的值将触发(通过GridSearchCV的)交叉验证。例如，cv=10将触发10折的交叉验证，而不是广义交叉验证(GCV)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=2, effective_rank=1, noise=10)\n",
    "rcv = RidgeCV(alphas=np.logspace(-5, 5, 11))\n",
    "rcv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拟合模型之后，alpha参数就是最优参数：\n",
    "rcv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看0.1附近更优的alpha值\n",
    "rcv = RidgeCV(alphas=np.linspace(.05, .2, 16))\n",
    "rcv.fit(X, y)\n",
    "rcv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = np.linspace(0.001, 1, 1000)\n",
    "rcv = RidgeCV(alphas=alpha_list, store_cv_values=True)  # 保存交叉检验的数据\n",
    "rcv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcv.cv_values_.shape  # 100次交叉验证  1000个不同alpha 的均方根误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_alpha_idx = rcv.cv_values_.mean(0).argmin()\n",
    "min_alpha_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list[min_alpha_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_ridgecv(alpha_list, X, y):\n",
    "    rcv = RidgeCV(alphas=alpha_list, store_cv_values=True)  # 保存交叉检验的数据\n",
    "    rcv.fit(X, y)\n",
    "    min_alpha_idx = rcv.cv_values_.mean(0).argmin()\n",
    "    f, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.set_title(r\"Various values of $\\alpha$\")\n",
    "    xy = (alpha_list[min_alpha_idx], rcv.cv_values_.mean(axis=0)[min_alpha_idx])\n",
    "    xytext = (xy[0] + .01, xy[1] + .1)\n",
    "    \n",
    "    ax.annotate(r'Chosen $\\alpha$', xy=xy, xytext=xytext,\n",
    "            arrowprops=dict(facecolor='black', shrink=0, width=0)\n",
    "            )\n",
    "    ax.plot(alpha_list, rcv.cv_values_.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_ridgecv(np.linspace(0.001, 0.1, 100), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
