{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 层次聚类-聚合聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('testSet2.txt')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:, 0], data[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(10).reshape(10, 1).tolist()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "class AgglomerativeCluster:\n",
    "    def __init__(self, n_clusters=3, linkage_type='average'):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.linkage_type = linkage_type\n",
    "    \n",
    "    def get_linkage(self, cluster_1, cluster_2):\n",
    "        if self.linkage_type == 'single':  # 最短\n",
    "            linkage = np.inf\n",
    "            for sample_1 in cluster_1:\n",
    "                for sample_2 in cluster_2:\n",
    "                    distance = self.distances[sample_1, sample_2]\n",
    "                    if distance < linkage:\n",
    "                        linkage = distance\n",
    "        elif self.linkage_type == 'complete':  # 最长\n",
    "            linkage = 0\n",
    "            for sample_1 in cluster_1:\n",
    "                for sample_2 in cluster_2:\n",
    "                    distance = self.distances[sample_1, sample_2]\n",
    "                    if distance > linkage:\n",
    "                        linkage = distance\n",
    "        elif self.linkage_type == 'average':  # 平均距离\n",
    "            distances = []\n",
    "            for sample_1 in cluster_1:\n",
    "                for sample_2 in cluster_2:\n",
    "                    distance = self.distances[sample_1, sample_2]\n",
    "                    distances.append(distance)\n",
    "            linkage = np.mean(distances)\n",
    "        else:  # 方差最小化\n",
    "            pass\n",
    "        return linkage\n",
    "    \n",
    "    \n",
    "    def clustering(self, clusters):\n",
    "        while True:\n",
    "            combins = [c for c in combinations(range(len(clusters)), 2)]\n",
    "            linkages = [self.get_linkage(clusters[c[0]], clusters[c[1]]) for c in combins]\n",
    "            # 类间距最小的2个类\n",
    "            min_arg = combins[np.argmin(linkages)]\n",
    "            # 合并距离最短的2个簇\n",
    "            clusters.append(clusters[min_arg[0]] + clusters[min_arg[1]])\n",
    "            \n",
    "            # 注意list pop的顺序 先后再前\n",
    "            clusters.pop(max(min_arg))\n",
    "            clusters.pop(min(min_arg))\n",
    "            if len(clusters) == self.n_clusters:\n",
    "                break\n",
    "        return clusters\n",
    "    \n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        # 相同簇的样本放在同一个子list中\n",
    "        clusters = np.arange(n_samples).reshape(n_samples, 1).tolist()\n",
    "        self.labels_ = np.zeros(n_samples, dtype=int)\n",
    "        \n",
    "        # d_{ij} 样本X_i 与 X_j 间距\n",
    "        self.distances = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            self.distances[i, :] = np.sqrt(np.sum(np.square(X[i] - X[:]), axis=1))\n",
    "        # 聚合聚类   \n",
    "        self.clusters = self.clustering(clusters)\n",
    "        \n",
    "        # 聚类结果转成labels的形式\n",
    "        for c in range(len(self.clusters)):\n",
    "            for i in clusters[c]:\n",
    "                self.labels_[i] = c\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AgglomerativeCluster(3, linkage_type='average')\n",
    "clf.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'g', 'b']\n",
    "for i in range(3):\n",
    "    plt.scatter(data[clf.labels_==i, 0], data[clf.labels_==i, 1], color=colors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "X_std = scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AgglomerativeClustering(linkage='complete', affinity='euclidean', n_clusters=3)\n",
    "clf.fit(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要注意的参数:\n",
    "- affinity: Metric used to compute the linkage. Can be “euclidean”, “l1”, “l2”, “manhattan”, “cosine”, or “precomputed”. default: “euclidean”\n",
    "- linkage: Which linkage criterion to use. [{“ward”, “complete”, “average”, “single”}, optional (default=”ward”)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MeanShift 均值移动聚类\n",
    "---\n",
    "MeanShift 算法旨在于发现一个样本密度平滑的 blobs 。均值漂移(Mean Shift)算法是基于质心的算法，通过更新质心的候选位置，这些侯选位置通常是所选定区域内点的均值。然后，这些候选位置在后处理阶段被过滤以消除近似重复，从而形成最终质心集合。\n",
    "\n",
    "给定第$ t $次迭代中的候选质心$ x_i$ , 候选质心的位置将被按照如下公式更新:\n",
    "\n",
    "$$x_i^{t+1} = x_i^t + m(x_i^t)$$\n",
    "\n",
    "其中$ N(x_i) $是围绕$ x_i $周围一个给定距离范围内的样本邻域, m 是均值偏移向量(mean shift vector), 该向量是所有质心中指向 点密度增加最多的区域的偏移向量。使用以下等式计算，有效地将质心更新为其邻域内样本的平均值:\n",
    "\n",
    "$$m(x_i) = \\frac{\\sum_{x_j \\in N(x_i)}K(x_j - x_i)x_j}{\\sum_{x_j \\in N(x_i)}K(x_j - x_i)}$$\n",
    "\n",
    "算法自动设定聚类的数目，而不是依赖参数 带宽（bandwidth）,带宽是决定搜索区域的size的参数。 这个参数可以手动设置，但是如果没有设置，可以使用提供的函数 estimate_bandwidth 获取 一个估算值。\n",
    "\n",
    "该算法不是高度可扩展的，因为在执行算法期间需要执行多个最近邻搜索。 该算法保证收敛，但是当 质心的变化较小时，算法将停止迭代。\n",
    "\n",
    "通过找到给定样本的最近质心来给新样本打上标签。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MeanShift有两个我们应该注意的重要参数。 首先，`bandwidth`设置区域（即观测核）半径，用于确定移动方向。 在我们的比喻中，带宽是一个人可以在雾中看到的距离。 我们可以手动设置此参数，但默认情况下会自动估算合理的带宽（计算成本会显着增加）。 其次，有时在均值移动中，观测核中没有其他观测结果。 也就是说，我们足球上的一个人看不到任何其它人。 默认情况下，MeanShift将所有这些“孤例”观测值分配给最近观测核。 但是，如果我们想要留出这些孤例，我们可以设置`cluster_all = False`，其中孤例观测标签为 -1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A demo of the mean-shift clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, _ = make_blobs(n_samples=10000, centers=centers, cluster_std=0.6)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute clustering with MeanShift\n",
    "\n",
    "# The following bandwidth can be automatically detected using\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "ms.fit(X)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "labels_unique = np.unique(labels)\n",
    "n_clusters_ = len(labels_unique)\n",
    "\n",
    "print(\"number of estimated clusters : %d\" % n_clusters_)\n",
    "print('centers: ', cluster_centers)\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.clf()\n",
    "\n",
    "colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
    "for k, col in zip(range(n_clusters_), colors):\n",
    "    my_members = labels == k\n",
    "    cluster_center = cluster_centers[k]\n",
    "    plt.plot(X[my_members, 0], X[my_members, 1], col + '.')\n",
    "    plt.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "        markeredgecolor='k', markersize=14)\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCN\n",
    "---\n",
    "DBSCAN 算法将簇视为被低密度区域分隔的高密度区域。由于这个相当普遍的观点， DBSCAN发现的簇可以是任何形状的，与假设簇是凸的 K-means 相反。 DBSCAN 的核心概念是 core samples, 是指位于高密度区域的样本。 因此一个簇是一组核心样本，每个核心样本彼此靠近（通过某个距离度量测量） 和一组接近核心样本的非核心样本（但本身不是核心样本）。算法中的两个参数, min_samples 和 eps,正式的定义了我们所说的 稠密（dense）。较高的 min_samples 或者较低的 eps 都表示形成簇所需的较高密度。\n",
    "\n",
    "更正式的,我们定义核心样本是指数据集中的一个样本的 eps 距离范围内，存在 min_samples 个其他样本，这些样本被定为为核心样本的邻居( neighbors) 。这告诉我们，核心样本在向量空间的稠密区域。一个簇是一个核心样本的集合，可以通过递归来构建，选取一个核心样本，查找它所有的邻居样本中的核心样本，然后查找新获取的核心样本的邻居样本中的核心样本，递归这个过程。 簇中还具有一组非核心样本，它们是簇中核心样本的邻居的样本，但本身并不是核心样本。 显然，这些样本位于簇的边缘。\n",
    "\n",
    "根据定义，任何核心样本都是簇的一部分，任何不是核心样本并且和任意一个核心样本距离都大于eps 的样本将被视为异常值。\n",
    "\n",
    "当参数min_samples 主要表示算法对噪声的容忍度(当处理大型噪声数据集时, 需要考虑增加该参数的值), 针对具体地数据集和距离函数，参数eps 如何进行合适地取值是非常关键，这通常不能使用默认值。参数eps控制了点地领域范围。如果取值太小,大部分地数据并不会被聚类(被标注为 -1 代表噪声); 如果取值太大，可能会 导致 相近 的多个簇被合并成一个,甚至整个数据集都被分配到一个簇。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- eps: 邻域, 观测到被认为是邻居的另一个观测的最大距离\n",
    "- min_samples: 小于上面的eps距离的最小观测数量\n",
    "- metric: eps使用的距离度量。 例如，minkowski，euclidean等（请注意，如果使用 Minkowski 距离，参数p可用于设置 Minkowski 度量的指数）\n",
    "\n",
    "Demo of DBSCAN clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n",
    "                            random_state=0)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)  # 分类-1 表示噪声\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"% metrics.adjusted_rand_score(labels_true, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\" % metrics.adjusted_mutual_info_score(labels_true, labels,\n",
    "    average_method='arithmetic'))\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(X, labels))\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "            for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "    class_member_mask = (labels == k)\n",
    "    xy = X[class_member_mask & core_samples_mask]  # 核心点\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "            markeredgecolor='k', markersize=14)\n",
    "    xy = X[class_member_mask & ~core_samples_mask]  # 非核心点\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "            markeredgecolor='k', markersize=6)\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用高斯混合模型的概率聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一些数据 模拟女性和男性的身高\n",
    "N = 1000\n",
    "in_m = 72\n",
    "in_w = 66\n",
    "\n",
    "s_m = 2\n",
    "s_w = 2\n",
    "m = np.random.normal(in_m, s_m, N)\n",
    "w = np.random.normal(in_w, s_w, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Histogram of Heights\")\n",
    "plt.hist(m, alpha=.5, label='Men')\n",
    "plt.hist(w, alpha=.5, label='Women')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对分组二次抽样，训练分布，之后预测剩余分组\n",
    "random_sample = np.random.choice([True, False], size=m.size) \n",
    "m_test = m[random_sample] \n",
    "m_train = m[~random_sample]\n",
    "\n",
    "w_test = w[random_sample] \n",
    "w_train = w[~random_sample] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要获得男性和女性高度的经验分布，基于训练集：\n",
    "from scipy import stats\n",
    "\n",
    "m_pdf = stats.norm(m_train.mean(), m_train.std()) \n",
    "w_pdf = stats.norm(w_train.mean(), w_train.std()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_pdf.pdf(m[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_pdf.pdf(m[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设当男性的概率更高时，我们会猜测，但是如果女性的概率更高，我们会覆盖它。\n",
    "guesses_m = np.ones_like(m_test) \n",
    "guesses_m[m_pdf.pdf(m_test) < w_pdf.pdf(m_test)] = 0\n",
    "guesses_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准确度\n",
    "guesses_m.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 女性分组\n",
    "guesses_w = np.ones_like(w_test)\n",
    "guesses_w[m_pdf.pdf(w_test) > w_pdf.pdf(w_test)] = 0\n",
    "guesses_w.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方差不相同时\n",
    "s_m = 1 \n",
    "s_w = 4\n",
    "\n",
    "m = np.random.normal(in_m, s_m, N) \n",
    "w = np.random.normal(in_w, s_w, N) \n",
    "\n",
    "m_test = m[random_sample] \n",
    "m_train = m[~random_sample]\n",
    "\n",
    "w_test = w[random_sample] \n",
    "w_train = w[~random_sample] \n",
    "f, ax = plt.subplots(figsize=(7, 5)) \n",
    "ax.set_title(\"Histogram of Heights\") \n",
    "ax.hist(m_train, alpha=.5, label=\"Men\"); \n",
    "ax.hist(w_train, alpha=.5, label=\"Women\"); \n",
    "ax.legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_A = np.random.normal(0, 1, size=(100, 2)) \n",
    "class_B = np.random.normal(4, 1.5, size=(100, 2)) \n",
    "f, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "ax.scatter(class_A[:,0], class_A[:,1], label='A', c='r')\n",
    "ax.scatter(class_B[:,0], class_B[:,1], label='B')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack((class_A, class_B))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.hstack((np.ones(100), np.zeros(100)))\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture \n",
    "\n",
    "# train = np.random.choice([True, False], 200) \n",
    "gmm = GaussianMixture(n_components=2)\n",
    "gmm.fit(X[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 权重\n",
    "gmm.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 均值\n",
    "gmm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方差\n",
    "gmm.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm.precisions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_ = gmm.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['A', 'B']\n",
    "for i in range(2):\n",
    "    k = labels_ == i\n",
    "    plt.scatter(X[k, 0], X[k, 1], label=labels[i])\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
