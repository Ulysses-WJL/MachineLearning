{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第14章 聚类\n",
    "## 本章概要\n",
    "1. 聚类是针对给定样本, 依据它们属性的相似度或距离, 将其归并到若干个\"类\"或\"簇\"的数据分析问题. 一个类时样本的一个子集. 直观上, 相似的样本聚集在同类, 不相似的样本分散在不同类.\n",
    "\n",
    "2. 距离或相似度度量在聚类方法中起着重要作用.  \n",
    "  常用的距离有闵可夫斯基距离, 包括欧氏距离、曼哈顿距离、切比雪夫距离以及马哈拉诺比斯距离。 常用的相似度度量有相关系数、夹角余弦。  \n",
    "  用距离度量相似度时，距离越小表示样本越相似；用相关系数时，相关系数越大表示样本越相似。\n",
    "\n",
    "3. 类时样本的子集, 比如有如下定义:  \n",
    "  设$T$为给定正数, 若集合$G$中任意两个样本$x_i, x_j$, 有\n",
    "$$d_{ij} \\leq T$$\n",
    "则称$G$为一个类或簇.\n",
    "描述类的特征的指标有: 中心、直径、散布矩阵、协方差矩阵.\n",
    "4. 聚类过程中用到的类与类之间的距离也称连接。类与类之间的距离包括最短距、最长距离、中心距离、平均距离。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本概念\n",
    "### 相似度或距离\n",
    "假设训练数据$X$由$n$个样本组成, 每个样本是一个$m$维向量.\n",
    "$$\n",
    "X=\\left[\n",
    "\\begin{matrix}\n",
    " x_{11} & \\cdots & x_{1n}       \\\\\n",
    " \\vdots &        & \\vdots \t\t\\\\\n",
    " x_{m1} & \\cdots & x_{mn}       \\\\\n",
    "\\end{matrix}\n",
    "\\right]\n",
    "$$\n",
    "每一行对应一个特征, 每一列对应一个样本.\n",
    "聚类的核心概念是**相似度**(similiarity)或**距离**(distance)\n",
    "\n",
    "#### 闵可夫斯基距离\n",
    "样本$x_i$与样本$x_j$的闵可夫斯基距离(Minkowski distance)定义为\n",
    "$$\n",
    "d_{ij}=\\left(\\sum_{k=1}^m|x_{ki}-x_{kj}|^p\\right)^\\frac{1}{p}\\\\\n",
    "p \\ge 1\n",
    "$$\n",
    "这里$p \\geq 1$. \n",
    "闵可夫斯基距离越大相似度越小.\n",
    "\n",
    "当$P=2$时称为**欧几里得距离**(Euclidean distance).\n",
    "\n",
    "当$p=1$时称为**曼哈顿距离**(Manhattan distance).\n",
    "\n",
    "当$p=\\infty$时称为**切比雪夫距离**(Chebyshev distance), 取各个坐标数值差的绝对值的最大值, 即\n",
    "$$d_{ij}=max_k|x_{ki} - x_{kj}|$$\n",
    "![minkowski](../img/minkowski.png)\n",
    "#### 马哈拉诺比斯距离\n",
    "\n",
    "马哈拉诺比斯距离(Mahalanobis distance), 简称马氏距离, 考虑各个分量(特征)之间的相关性并与各个分量的尺度无关. 马氏距离越大相似度越小.\n",
    "\n",
    "$$d_{ij}=\\left[(x_i-x_j)^\\mathrm TS^{-1}(x_i-x_j)\\right]^{\\frac{1}{2}}$$\n",
    "其中$$x_i = (x_{1i}, x_{2i}, \\cdots, x_{mi})^T, \\quad x_j = (x_{1j}, x_{2j}, \\cdots, x_{mj})^T$$\n",
    "当$S$为单位矩阵时, 即样本数据的各个分量互相独立且各个分量的方差为1时, 马氏距离就是欧氏距离.\n",
    "#### 相关系数\n",
    "样本之间的相似度也可以用相关系数(correlation coefficient)来表示. 相关系数的绝对值越接近于1, 表示样本越相似.\n",
    "$$\n",
    "r_{ij}=\\frac{\\sum\\limits_{k=1}^m(x_{ki}-\\bar x_i)(x_{kj}-\\bar x_j)}{\\left[\\sum\\limits_{k=1}^m(x_{ki}-\\bar x_i)^2\\sum\\limits_{k=1}^m(x_{kj}-\\bar x_j)^2\\right]^\\frac{1}{2}}\\\\\n",
    "\\bar x_i=\\frac{1}{m}\\sum\\limits_{k=1}^mx_{ki}\\\\\n",
    "\\bar x_j=\\frac{1}{m}\\sum\\limits_{k=1}^mx_{kj}\n",
    "$$\n",
    "#### 夹角余弦\n",
    "样本之间的相似度也可以用夹角余弦(cosine)来表示.夹角余弦越接近于1, 表示样本越相似\n",
    "$$\n",
    "s_{ij}=\\frac{\\sum\\limits_{k=1}^m x_{ki}x_{kj}}{\\left[\\sum\\limits_{k=1}^mx_{ki}^2\\sum\\limits_{k=1}^mx_{kj}^2\\right]^\\frac{1}{2}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 类或簇\n",
    "通过聚类得到的类或簇，本质是样本的子集。\n",
    "+ 硬聚类（hard clustering）：一个样本只能属于一个类, 或类的交集为空集; \n",
    "+ 软聚类（soft clustering）: 一个样本可以属于多个类, 或类的交集不为空集;\n",
    "\n",
    "类或簇的定义:\n",
    "1. (最常用)设$T$为给定正数, 若集合$G$中任意两个样本$x_i, x_j$, 有\n",
    "$$d_{ij} \\leq T$$\n",
    "则称$G$为一个类或簇.\n",
    "\n",
    "2. 设$T$为给定正数, 若集合$G$中任意样本$x_i$, 一定存在$G$中的另一个样本$x_j$使得\n",
    "$$d_{ij} \\leq T$$\n",
    "则称$G$为一个类或簇.\n",
    "\n",
    "3. 设$T$为给定正数, 若对集合$G$中任意一个样本$x_i$, $G$中另一个样本$x_j$满足\n",
    "$$  \\frac 1 {n_G-1}\\sum_{x_j\\in G} d_{ij} \\leq T$$\n",
    "则称$G$为一个类或簇.\n",
    "\n",
    "4. 设$T和V$为给定的两个正数, 如果样本集合$G中任意两个样本x_i, x_j的距离d_{ij}$满足\n",
    "$$\\frac 1 {n_G(n_G-1)}\\sum_{x_i\\in G} \\sum_{x_j \\in G} d_{ij} \\leq T \\\\ d_{ij} \\leq V$$\n",
    "则称$G$为一个类或簇.\n",
    "\n",
    "类的特征描述:\n",
    "\n",
    "1. 均值, 又称类的中心\n",
    "  $$\\bar x_G = \\frac 1 {n_G} \\sum_{i=1}^{n_G}x_i$$\n",
    "2. 直径(diameter), 两个样本之间的最大距离\n",
    "  $$D_G = max_{x_i, x_j \\in G} d_{ij}$$\n",
    "3. 散布矩阵(scatter matrix)\n",
    "  $$A_G = \\sum_{i=1}^{n_G}(x_i - \\bar x_G)(x_i - \\bar x_G)^T$$\n",
    "  协方差矩阵(covariance matrix)\n",
    "  $$\\begin{align} S_G  &= \\frac 1 {m-1} A_G \\\\ &= \\frac 1 {m-1} \\sum_{i=1}^{n_G}(x_i - \\bar x_G)(x_i - \\bar x_G)^T\\end{align}$$\n",
    "\n",
    "### 类与类之间的距离\n",
    "类和类之间的距离也叫做连接(linkage).\n",
    "#### 最短距离或单连接(single linkage)\n",
    "\n",
    "$$D_{pq}=\\min\\{d_{ij}|x_i\\in G_p, x_j \\in G_q\\}$$\n",
    "\n",
    "#### 最长距离, 完全距离(complete linkage)\n",
    "\n",
    "$$D_{pq}=\\max \\{d_{ij}|x_i \\in G_p, x_j \\in G_q\\}$$\n",
    "\n",
    "#### 平均距离(average linkage)\n",
    "\n",
    "$$D_{pq}=\\frac{1}{n_pn_q}\\sum\\limits_{x_i\\in G_p}\\sum\\limits_{x_j\\in G_q}d_{ij}$$\n",
    "\n",
    "#### 中心距离\n",
    "\n",
    "$$D_{pq}=d_{\\bar x_p\\bar x_q}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 性能度量\n",
    "聚类性能度量亦称\"有效性指标\"(validity index).对聚类结果, 一方面可以通过性能度量评估其结果好坏; 另一方面,可以直接将性能度量作为聚类过程中优化的目标. 直观上, 我们希望聚类结果的\"簇内相似度\"(intra-cluster similarity)高且\"簇间相似度\"(inter-cluster similarity)低.\n",
    "聚类性能度量大致有2类:\n",
    "- 外部指标(external index): 将聚类结果与某个\"参考模型\"(reference model)进行比较\n",
    "- 内部指标(internal index): 直接考察聚类结果而不利用任何参考模型\n",
    "\n",
    "![性能度量1](../img/性能度量1.png)\n",
    "![性能度量2](../img/性能度量2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 层次聚类\n",
    "层次聚类**假设**类别之间存在层次结构。\n",
    "\n",
    "层次聚类可以分成聚合聚类(自下而上)和分裂聚类(自上而下)。\n",
    "\n",
    "聚合聚类三要素：\n",
    "\n",
    "1. 距离或相似度：闵可夫斯基，马哈拉诺比斯，相关系数，余弦距离\n",
    "1. 合并规则：类间距离最小，最短，最长，中心，平均\n",
    "1. 停止条件：类的个数达到阈值，类的直径达到阈值\n",
    "\n",
    "### 算法14.1(聚合聚类算法)\n",
    "\n",
    "输入：$n$个样本组成的集合$X$\n",
    "\n",
    "输出：对样本的一个层次化聚类$C$\n",
    "\n",
    "1. 计算$n$个样本两两之间的欧氏距离$\\{d_{ij}\\}$，记作矩阵$D=[d_{ij}]_{n\\times n}$\n",
    "1. 构造$n$个类，每个类只包含一个样本\n",
    "1. 合并类间距离最小的两个类，其中最短距离为类间距离，构建一个新类。\n",
    "1. 计算新类与当前各类之间的距离。如果类的个数是1， 终止计算，否则回到步骤3。\n",
    "算法复杂度$O(n^3m)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原型聚类\n",
    "原型聚类亦称\"基于原型的聚类\"(prototype-based clustering), 此类算法假设聚类结构能通过一组原型刻画\n",
    "> \"原型\"指样本空间中具有代表性的点\n",
    "\n",
    "### KMeans聚类\n",
    "#### 模型\n",
    "给定$n$个样本的集合$X=\\{x_1, x_2, \\cdots, x_n\\}$, 每个样本由一个维数为$m$的特征向量表示.  \n",
    "$k$均值聚类的目标是将$n$个样本划分到$k$个不同的类或簇中.\n",
    "$$l=C(i)$$\n",
    "其中$i \\in \\{1, 2, \\cdots, n\\}, l \\in \\{1, 2, \\cdots, k\\}$.k均值聚类的模型是一个从样本到类的函数.\n",
    "\n",
    "#### 策略\n",
    "损失函数的最小化选取最优的划分或函数$C^*$, 距离采用的是**欧氏距离平方**\n",
    "$$d(x_i, x_j) = \\sum_{k=1}^m(x_{ki}-x_{kj})^2 \\\\ = ||x_i-x_j||^2$$\n",
    "然后, 定义样本与其所属类的中心之间的距离的总和为损失函数, 即\n",
    "$$W(C) = \\sum_{l=1}^k\\sum_{C(i)=l} ||x_i - \\bar x_l||^2$$\n",
    "式中$\\bar x = (\\bar x_{1l}, \\bar x_{2l}, \\cdots, \\bar x_{ml})^T$是第$l$个类的均值或中心, $n_l=\\sum_{i=1}^nI(C(i)=l), I(C(i)=l)$是指示函数,取值为0或1. 函数$W(C)$也称为能量, 表示相同类中的样本相似的程度.\n",
    "k均值聚类就是求解最优化问题:\n",
    "$$\n",
    "\\begin{align}C^* &= arg\\;\\mathop{min}_C W(C) \\\\\n",
    "&= arg\\;\\mathop{min}_C \\sum_{l=1}^k\\sum_{C(i)=l} ||x_i - \\bar x_l||^2\n",
    "\\end{align}\n",
    "$$\n",
    "$n个样本分到k类$, 所有可能分法的数目是:\n",
    "$$S(n,k)=\\frac 1 {k!} \\sum_{l=1}^k(-1)^{k-l}\\begin{pmatrix}k \\\\ l\\end{pmatrix}k^n$$\n",
    "是指数级的, 是一个NP难问题.现实中使用迭代的方法求解.\n",
    "\n",
    "#### 算法\n",
    "\n",
    "输入：$n$个样本的集合$X$\n",
    "\n",
    "输出：样本集合的聚类$C^*$\n",
    "\n",
    "1. 初始化。令$t=0$, 随机选择$k$个样本点作为初始聚集类中心$m^{(0)} = (m_1^{(0)},\\cdots, m_l^{(0)}, \\cdots, m_k^{(0)})$\n",
    "1. 对样本进行聚类。对固定的类中心$m^{(t)} = (m_1^{(t)},\\cdots, m_l^{(t)}, \\cdots, m_k^{(t)})$, 其中$m_l^{(t)}$为类$G_l$的中心, 计算每个样本到类中心的距离, 将每个样本指派到与其最近的中心的类中, 构成聚类结果$C^{(t)}$.\n",
    "1. 计算类的中心。对聚类结果$C^{(t)}$, 计算当前各个类中的样本均值, 作为新的类中心$m^{(t+1)} = (m_1^{(t+1)},\\cdots, m_l^{(t+1)}, \\cdots, m_k^{(t+1)})$\n",
    "1. 如果迭代收敛或符合停止条件(划分不再改变, $C^{(t+1)} = C^{(t)}$)，输出$C^*=C^{(t)}$, 否则, 令$t=t+1$返回步(2)\n",
    "\n",
    "$k$均值聚类算法的复杂度$O(mnk)$, 其中$m$是样本维数, $n$是样本个数, $k$是类别个数.\n",
    "#### 算法特性\n",
    "特性\n",
    "- 收敛性: k均值聚类属于启发式方法, 不能保证收敛到全局最优, 初始中心的选择会影响聚类结果\n",
    "- 初始选择: 可以用层次聚类对样本进行聚类, 得到k个类时停止.然后从每个类中选取一个与中心距离最近的点.\n",
    "- 类别数k的选择: 尝试使用不同的k值, 检查各自得到的聚类结果的质量, 可以用类的**平均直径衡量**. 可使用**二分查找**快速找到最优的k值.\n",
    "\n",
    "优点:\n",
    "\n",
    "- 属于无监督学习，无须准备训练集\n",
    "- 原理简单，实现起来较为容易\n",
    "- 结果可解释性较好\n",
    "\n",
    "缺点:\n",
    "\n",
    "- 需手动设置k值。 在算法开始预测之前，我们需要手动设置k值，即估计数据大概的类别个数，不合理的k值会使结果缺乏解释性\n",
    "- 可能收敛到局部最小值, 在大规模数据集上收敛较慢\n",
    "- 对于异常点、离群点敏感\n",
    "\n",
    "### 学习向量量化\n",
    "与k均值算法类似, 学习向量量化(Learnning Vector Quantization, LVQ)也是试图找到一组原型向量来刻画聚类结构, 但与一般聚类算法不同的是, LVQ假设数据样本带有类别标记, 学习过程利用样本的这些监督信息来辅助聚类\n",
    "具体内容查看, 机器学习-周志华 9.4.2 学习向量量化\n",
    "\n",
    "### 高斯混合聚类\n",
    "与k均值、LVQ用原型向量来刻画聚类结构不同， 高斯混合聚类采样概率模型来表达聚类原型。\n",
    "\n",
    "高斯混合模型(Gaussian Mixture Model)是具有如下**概率分布**的模型:\n",
    "$$\n",
    "P(y|\\theta)=\\sum\\limits^{K}_{k=1}\\alpha_k\\phi(y|\\theta_k)\n",
    "$$\n",
    "其中， $\\theta=(\\alpha_1,\\alpha_2,\\dots,\\alpha_K;\\theta_1,\\theta_2,\\dots,\\theta_K)$\n",
    "\n",
    "$\\alpha_k$是系数，$\\alpha_k\\ge0$，$\\sum\\limits^{K}_{k=1}\\alpha_k=1$, $\\phi(y|\\theta_k)$ 是**高斯分布密度**，$\\theta_k=(\\mu,\\Sigma)$\n",
    "\n",
    "多元正态分布，也叫多元高斯分布\n",
    "$$\n",
    "\\phi(y|\\theta_k)=\\frac{1}{\\sqrt{(2\\pi)^d|\\Sigma|}}\\exp\\left(-\\frac{(y-\\mu_k)^T\\Sigma^{-1}(y-\\mu_k)}{2}\\right)\n",
    "$$\n",
    "\n",
    "其中，$u$是d维均值向量,协方差矩阵$\\Sigma\\in \\mathbb{R}^{d\\times d}$\n",
    "\n",
    "上式表示第k个**分**模型。\n",
    "\n",
    "从原型聚类的角度来看, 高斯混合聚类是采用概率模型(高斯分布)对原型进行刻画, 簇划分结果则由原型对应后验概率确定(样本$x_j$由第i个高斯混合成分生成的后验概率). 具体求解参考 9.3 EM算法在高斯混合模型学习中的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 密度聚类\n",
    "基于密度的聚类(density-based clustering), 此类算法假设聚类结构能通过样分布的紧密程度确定.通常情形下目睹聚类算法从样本密度的角度考察样本直接的可连结性, 并基于可连结样本不断扩展聚类簇以获得最终的聚类结果.\n",
    "\n",
    "“具有噪声的基于密度的聚类方法”（DBSCAN）是一种著名的密度聚类算法, 它基于一组\"邻域\"(neighborhood)参数$(\\epsilon, MinPts)$来刻画样本分布的紧密程度. 给定样本数据集$D = \\{x_1, x_2, \\cdots, x_n\\}$, 定义:\n",
    "- $\\epsilon-邻域$: 对$x_j \\in D$, 其邻域包含样本集D中与$x_j$的距离不大于$\\epsilon$的样本, 即$N_{\\epsilon}(x_j)=\\{x_i \\in D |d(x_i, x_j) \\leq \\epsilon\\}$\n",
    "- 核心点(core object)：若$x_j$在其$\\epsilon-邻域$内，至少具有最小数量（MinPts）的其他点,则$x_j$是一个核心点。\n",
    "- 边界点：边界点是一个点，它不是核心点，因为它的邻域中没有足够的MinPts，但位于核心点的半径$\\epsilon$内。\n",
    "- 噪点：所有其他的点，既不是核心点也不是边界点。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}