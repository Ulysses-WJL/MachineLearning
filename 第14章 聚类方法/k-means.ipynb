{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K均值聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=16)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('testSet.txt')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:, 0], data[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, k):\n",
    "        self.k = k  # 质心数目\n",
    "    \n",
    "    def init_(self, X):\n",
    "        X_max = np.max(X, axis=0)\n",
    "        X_min = np.min(X, axis=0)\n",
    "        m0 = np.zeros((self.k, X.shape[1]))\n",
    "        for i in range(self.k):\n",
    "            m_ = np.random.rand(X.shape[1]) * (X_max - X_min) + X_min\n",
    "            m0[i] = m_\n",
    "        return m0\n",
    "      \n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        # 保存聚类的结果, (所属类编号, 样本到所属类中心距离)\n",
    "        cluster = np.zeros((n_samples, 2)) \n",
    "        # 初始化 中心\n",
    "        m = self.init_(X)\n",
    "\n",
    "        while True:\n",
    "            cluster_last = cluster.copy()\n",
    "            # 计算样本到各中心点距离, 指派到最近的点\n",
    "            for i in range(n_samples):\n",
    "                distance = np.sum(np.square(X[i] - m), axis=1)\n",
    "                min_index = np.argmin(distance)\n",
    "                cluster[i] = (min_index, distance[min_index])\n",
    "            \n",
    "            # 计算新的中心\n",
    "            # print(m)\n",
    "            for l in range(self.k):\n",
    "                m[l] = np.mean(X[cluster[:, 0] == l,:], axis=0)\n",
    "            if np.all(cluster[:, 0] == cluster_last[:, 0]):\n",
    "                break\n",
    "        return m, cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KMeans(4)\n",
    "m, cluster = clf.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[cluster[:, 0]==0, 0], data[cluster[:, 0]== 0, 1], marker='o')\n",
    "plt.scatter(data[cluster[:, 0]==1, 0], data[cluster[:, 0]== 1, 1], marker='v')\n",
    "plt.scatter(data[cluster[:, 0]==2, 0], data[cluster[:, 0]== 2, 1], marker='+')\n",
    "plt.scatter(data[cluster[:, 0]==3, 0], data[cluster[:, 0]== 3, 1], marker='x')\n",
    "plt.scatter(m[:, 0], m[:, 1], marker='s', s=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSE = np.sum(cluster[:, 1])\n",
    "SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二分k-means算法\n",
    "二分k-means算法是为了克服k-means算法收敛于局部最小值的问题,二分k-kmeans算法首先将所有点作为一个簇,然后将该簇一分为二.之后选择其中一个簇继续进行划分,选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE值.上述基于SSE的划分过程不断重复,直到得到用户指定的簇数目为止\n",
    "\n",
    "二分k-means的伪代码形式如下:\n",
    "* 将所有点看成一个簇\n",
    "* 当簇数目小于 k 时\n",
    "* 对于每一个簇\n",
    "    * 计算总误差\n",
    "    * 在给定的簇上面进行 KMeans 聚类（k=2）\n",
    "    * 计算将该簇一分为二之后的总误差\n",
    "* 选择使得误差最小的那个簇进行划分操作\n",
    "\n",
    "另一种做法是选择 SSE 最大的簇进行划分，直到簇数目达到用户指定的数目位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二分 K-Means 聚类算法\n",
    "class BinMeans:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "          \n",
    "    def choose_best_cluster(self, X, cluster, m):\n",
    "        # 选择最优的簇进行分类, \n",
    "        max_diff = 0\n",
    "        # 当前所有类别\n",
    "        m_index = np.unique(cluster[:, 0]).astype(int).tolist()\n",
    "        cluster_m = []\n",
    "        for l in m_index:\n",
    "            data = X[cluster[:, 0]==l, :]\n",
    "            # 聚类之前的SSE\n",
    "            sse_before = np.sum(cluster[cluster[:, 0]==l, 1])\n",
    "            clf = KMeans(2)\n",
    "            new_m, new_cluster = clf.fit(data)\n",
    "            # 新的类的编号\n",
    "            new_cluster[:, 0] += len(m)\n",
    "            cluster_m.append((new_cluster, new_m))\n",
    "            # 最大程度降低SSE的值\n",
    "            diff = sse_before - np.sum(new_cluster[:, 1]) \n",
    "            if diff > max_diff:\n",
    "                best_index = l\n",
    "                max_diff = diff\n",
    "        # 添加新的中心点\n",
    "        m.extend(cluster_m[m_index.index(best_index)][1])\n",
    "        j = 0\n",
    "        # 更新每个样本的类别编号和距离中心的距离\n",
    "        for i in range(len(cluster)):\n",
    "            if cluster[i, 0] == best_index:\n",
    "                cluster[i] = cluster_m[m_index.index(best_index)][0][j]\n",
    "                j += 1\n",
    "        return cluster, m\n",
    "            \n",
    "            \n",
    "    def fit(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        # 所有样本分到一个簇中\n",
    "        m_ = np.mean(X, axis=0)\n",
    "        m = [m_]\n",
    "        cluster = np.zeros((n_samples, 2))\n",
    "        cluster[:, 1] = np.sum(np.square(X - m[0]), axis=1)\n",
    "        \n",
    "        while len(np.unique(cluster[:, 0])) < self.k:\n",
    "            cluster, m = self.choose_best_cluster(X, cluster, m)\n",
    "            # 对sse最大的簇进行 K-Means聚类(k=2)\n",
    "        return np.array(m), cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BinMeans(4)\n",
    "m, cluster = clf.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSE = np.sum(cluster[:, 1])\n",
    "SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(cluster[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[cluster[:, 0]==3, 0], data[cluster[:, 0]== 3, 1], marker='o')\n",
    "plt.scatter(data[cluster[:, 0]==4, 0], data[cluster[:, 0]== 4, 1], marker='v')\n",
    "plt.scatter(data[cluster[:, 0]==5, 0], data[cluster[:, 0]== 5, 1], marker='+')\n",
    "plt.scatter(data[cluster[:, 0]==6, 0], data[cluster[:, 0]== 6, 1], marker='x')\n",
    "plt.scatter(m[3:, 0], m[3:, 1], marker='s', s=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('./testSet2.txt')\n",
    "plt.scatter(data[:, 0], data[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_k = KMeans(k=3)\n",
    "m, cluster = clf_k.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(data[cluster[:, 0]==0, 0], data[cluster[:, 0]== 0, 1], marker='o')\n",
    "plt.scatter(data[cluster[:, 0]==1, 0], data[cluster[:, 0]== 1, 1], marker='v')\n",
    "plt.scatter(data[cluster[:, 0]==2, 0], data[cluster[:, 0]== 2, 1], marker='s')\n",
    "# plt.scatter(data[cluster[:, 0]==3, 0], data[cluster[:, 0]== 3, 1], marker='x')\n",
    "plt.scatter(m[:, 0], m[:, 1], marker='+', s=100, c='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_k = BinMeans(k=3)\n",
    "m, cluster = clf_k.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(cluster[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[cluster[:, 0]==2, 0], data[cluster[:, 0]== 2, 1], marker='o')\n",
    "plt.scatter(data[cluster[:, 0]==3, 0], data[cluster[:, 0]== 3, 1], marker='v')\n",
    "plt.scatter(data[cluster[:, 0]==4, 0], data[cluster[:, 0]== 4, 1], marker='s')\n",
    "# plt.scatter(data[cluster[:, 0]==3, 0], data[cluster[:, 0]== 3, 1], marker='x')\n",
    "plt.scatter(m[2:, 0], m[2:, 1], marker='+', s=100, c='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn  KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Signature:\n",
    "make_blobs(\n",
    "    n_samples=100,\n",
    "    n_features=2,\n",
    "    centers=None,\n",
    "    cluster_std=1.0,\n",
    "    center_box=(-10.0, 10.0),\n",
    "    shuffle=True,\n",
    "    random_state=None,\n",
    ")\n",
    "Docstring:\n",
    "Generate isotropic Gaussian blobs for clustering.\n",
    "\n",
    "Read more in the :ref:`User Guide <sample_generators>`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "blobs, classes = make_blobs(500, centers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = np.array(['r', 'g', 'b']) \n",
    "plt.scatter(blobs[:, 0], blobs[:, 1], color=color_map[classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "class sklearn.cluster.KMeans(n_clusters=8, init=’k-means++’, n_init=10,\n",
    "                            max_iter=300, tol=0.0001, precompute_distances=’auto’,\n",
    "                            verbose=0, random_state=None, copy_x=True, n_jobs=None, algorithm=’auto’)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设已知有3个中心\n",
    "kmean = KMeans(n_clusters=3)\n",
    "kmean.fit(blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个样本点的预期标签, 由于 KMeans 不知道类别是什么, 与classes会不一致\n",
    "kmean.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = np.array(['r', 'g', 'b']) \n",
    "plt.scatter(blobs[:, 0], blobs[:, 1], color=color_map[classes])\n",
    "# cluster 中心点坐标\n",
    "plt.scatter(kmean.cluster_centers_[:, 0], kmean.cluster_centers_[:, 1], marker='*', s=250, color='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean.predict([[5, -1], [-8, 3], [5, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出每个点到形心的距离\n",
    "kmean.transform(blobs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化形心数量\n",
    "---\n",
    "形心难以解释，并且也难以判断是否数量正确。理解你的数据是否是未分类的十分重要，因为这会直接影响我们可用的评估手段。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs, classes = make_blobs(500, centers=3)\n",
    "kmean = KMeans(n_clusters=3)\n",
    "kmean.fit(blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = np.array(['r', 'g', 'b']) \n",
    "plt.scatter(blobs[:, 0], blobs[:, 1], color=color_map[classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = kmean.labels_\n",
    "blobs[cluster_labels==2]\n",
    "plt.scatter(blobs[cluster_labels==2][:, 0], blobs[cluster_labels==2][:, 1])\n",
    "plt.scatter(blobs[cluster_labels==1][:, 0], blobs[cluster_labels==1][:, 1])\n",
    "plt.scatter(blobs[cluster_labels==0][:, 0], blobs[cluster_labels==0][:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们查看**轮廓**（Silhouette）距离。轮廓距离是簇内不相似性、最近的簇间不相似性、以及这两个值最大值的比值。它可以看做簇间分离程度的度量。\n",
    "评估它的相似性\n",
    "$$s_i = \\frac {b_i - a_i}{max(a_i, b_i)}$$\n",
    "$s_i$是观测样本$i$的**轮廓系数**(Silhouette Coefficient), $a_i是i和同类的所有观测值之间的平均距离, a_i越小，说明样本i越应该被聚类到该簇,将a_i 称为样本i的簇内不相似度; 而b_i是i和不同类的所有观测的平均距离的最小值, 称为样本i的最近的簇间不相似性, b_i越大说明样本i越不属于其他簇$\n",
    "\n",
    "判断：\n",
    "- si接近1，则说明样本i聚类合理；\n",
    "\n",
    "- si接近-1，则说明样本i更应该分类到另外的簇；\n",
    "\n",
    "- 若si 近似为0，则说明样本i在两个簇的边界上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "silhouette_samples = metrics.silhouette_samples(blobs, kmean.labels_)\n",
    "silhouette_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.column_stack((classes[:5], silhouette_samples[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据点到形心的距离分布\n",
    "plt.hist(silhouette_samples)\n",
    "# 通常接近 1 的系数越高，分数就越高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 轮廓系数的均值通常用于描述整个模型的拟合度\n",
    "silhouette_samples.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接获取 轮廓系数的均值\n",
    "metrics.silhouette_score(blobs, kmean.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs, classes = make_blobs(500, centers=10)\n",
    "sillhouette_avgs = []\n",
    "for k in range(2, 60):\n",
    "    kmean = KMeans(n_clusters=k).fit(blobs)\n",
    "    sillhouette_avgs.append(metrics.silhouette_score(blobs, kmean.labels_))\n",
    "plt.plot(sillhouette_avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "轮廓均值随着形心数量的变化情况. 最优的数量和实际的数量不一致, 这就是聚类的实际情况，十分普遍，我们不能获得正确的簇数量，我们只能估计簇数量的近似值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估聚类的正确性\n",
    "---\n",
    "簇已知条件下的 KMeans 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs, classes = make_blobs(500, centers=3)\n",
    "kmean = KMeans(n_clusters=3).fit(blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'g', 'b']\n",
    "for i in range(3):\n",
    "    p = blobs[classes==i]\n",
    "    plt.scatter(p[:, 0], p[:, 1], label=f'Cluster {i}', color=colors[i])\n",
    "plt.legend()\n",
    "plt.scatter(kmean.cluster_centers_[:, 0],\n",
    "               kmean.cluster_centers_[:, 1], s=100,\n",
    "               color='black',\n",
    "               label='Centers') \n",
    "plt.title(\"Cluster With Ground Truth\") \n",
    "plt.savefig('2020-1-11_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始classes 变换类别编号\n",
    "labels = kmean.labels_.copy()\n",
    "labels[kmean.labels_==2] = 1  # 2->0\n",
    "labels[kmean.labels_==0] = 2  # 0->1\n",
    "labels[kmean.labels_==1] = 0  # 1->2\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚类的结果\n",
    "for i in range(3):\n",
    "    p = blobs[labels==i]\n",
    "    plt.scatter(p[:, 0], p[:, 1], label=f'Cluster {i}', color=colors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**准确度**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各类别的准确度\n",
    "for i in range(3):\n",
    "    print((classes == labels)[classes==i].astype(int).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个相似性度量是**互信息**（ mutual information score）得分。\n",
    "\n",
    "$$I(X;Y)=\\sum_x\\sum_y p(x,y) log \\frac{p(x,y)}{p(x)p(y)}$$\n",
    "归一化的互信息(Normalized Mutual Information, NMI)\n",
    "$$NMI(X;Y) = 2\\frac {I(X;Y)}{H(X)+H(Y)}$$\n",
    "分数靠近 0，就说明标签的分配可能不是按照相似过程生成的。但是分数靠近 1，就说明两个标签有很强的一致性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.normalized_mutual_info_score(classes, kmean.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**惯性**（inertia）度量. \n",
    "惯性是每个数据点和它所分配的簇的平方差之和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means 聚类手写数字\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "data = scale(digits.data)  # z = (x-u) / s  归一化的data\n",
    "digits.images[0], digits.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印图片信息\n",
    "def print_digits(images, y, max_n=10):\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1,\n",
    "                       hspace=0.05, wspace=0.005)\n",
    "    i = 0\n",
    "    while i < max_n and i < images.shape[0]:\n",
    "        p = fig.add_subplot(20, 20, i+1, xticks=[], yticks=[])\n",
    "        p.imshow(images[i], cmap=plt.cm.bone)\n",
    "        p.text(0, 14, str(y[i]))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前10张图片\n",
    "print_digits(digits.images, digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 训练集 测试集 \n",
    "X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(\n",
    "    data, digits.target, digits.images, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_digits = len(np.unique(y_train))\n",
    "labels = y_train\n",
    "n_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clf = KMeans(init='k-means++', n_clusters=10, random_state=42)\n",
    "clf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像 预测分类\n",
    "print_digits(images_train, clf.labels_, max_n=10)\n",
    "# 预测的类编号与实际图片的数值无关, 是clf自身的分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 显示每个簇的10个图像\n",
    "def print_cluster(images, y_pred, cluster_number):\n",
    "    images = images[y_pred == cluster_number]\n",
    "    y_pred = y_pred[y_pred == cluster_number]\n",
    "    print_digits(images, y_pred, max_n=10)\n",
    "for i in range(10):\n",
    "    print_cluster(images_test, y_pred, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到有些簇非常清晰, 如簇2对应数字0, 簇7对应数字6;有些簇不是那么清楚, 如簇6中有9、8和3\n",
    "\n",
    "---\n",
    "#### 评估\n",
    "我们如何评估我们的表现？精确率和所有这些东西都不起作用，因为我们没有可比较的目标类。为了评估，我们需要知道“真正的”簇，无论这意味着什么。我们可以假设，对于我们的示例，每个簇包括特定数字的每个绘图，并且仅包括该数字。知道这一点，我们可以计算我们的簇分布和预期之间的调整**兰德系数**。兰德系数是一个类似的准确率量度，但它考虑到两个分布中的类可以有不同名称的事实。也就是说，如果我们更改类名，索引不会改变。调整兰德系数试图消除偶然发生的结果的巧合。当两个集合中具有完全相同的簇时，兰德系数等于 1，而当没有簇共享数据点时，它等于零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "# adjusted rand socre\n",
    "metrics.adjusted_rand_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混淆矩阵\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "# 数字0全部分类到了簇2, 完全一致\n",
    "confusion_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数字9 5个分到簇0, 50个在簇6, 2个簇8\n",
    "confusion_matrix[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 KMeans 聚类来量化图像\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = plt.imread('01.png')\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了实际量化图像，我们需要将其转换为二维数组，长为1080x1920，宽为 RGB 值. \n",
    "\n",
    "思考它的更好的方法，是拥有一堆三维空间中的数据点，并且对点进行聚类来降低图像中的不同颜色的数量 -- 这是一个简单的量化方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = img.shape\n",
    "long_img = img.reshape(x*y, z)\n",
    "long_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5)  # 5中颜色\n",
    "kmeans.fit(long_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = kmeans.cluster_centers_ \n",
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(centers[labels].reshape(x, y, z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniBatchKMeans 小批量KMeans聚类\n",
    "---\n",
    "MiniBatchKMeans 是 KMeans 算法的一个变种，它使用小批量(mini-batches)来减少计算时间，而这多个批次仍然尝试优化相同的目标函数。小批量是输入数据的子集，在每次训练迭代中随机抽样。这些小批量大大减少了收敛到局部解所需的计算量。 与其他降低 k-means 收敛时间的算法不同，小批量 k-means 产生的结果通常只比标准算法略差。\n",
    "\n",
    "该算法在两个步骤之间进行迭代，类似于 vanilla k-means 。在第一步， b 样本是从数据集中随机抽取的，形成一个小批量。然后将它们分配到最近的质心。 在第二步，质心被更新。与 k-means 不同, 该变种算法是基于每个样本(per-sample)。对于小批量中的每个样本，通过取样本的流平均值(streaming average)和分配给该质心的所有先前样本来更新分配的质心。 这具有随时间降低质心的变化率（rate of change）的效果。执行这些步骤直到达到收敛或达到预定次数的迭代。\n",
    "\n",
    "MiniBatchKMeans 收敛速度比 KMeans快 ，但是结果的质量会降低"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_std = scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size控制每批中随机选择的观测数。 批量越大，训练过程的计算成本就越高。\n",
    "clf = MiniBatchKMeans(n_clusters=3, random_state=0, batch_size=100)\n",
    "clf.fit(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniBatchKMeans和KMeans的对比\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "\n",
    "blobs, labels = make_blobs(int(1e6), 3)\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "minibatch = MiniBatchKMeans(n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比运行时间\n",
    "%time kmeans.fit(blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time minibatch.fit(blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 类中心\n",
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "\n",
    "pairwise.pairwise_distances(kmeans.cluster_centers_[[0]], minibatch.cluster_centers_[[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.6.23 Comparison of the K-Means and MiniBatchKMeans clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "batch_size = 45\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "n_clusters = len(centers)\n",
    "X, labels_true = make_blobs(n_samples=3000, centers=centers, cluster_std=0.7)\n",
    "# #############################################################################\n",
    "# Compute clustering with Means\n",
    "k_means = KMeans(init='k-means++', n_clusters=3, n_init=10)\n",
    "t0 = time.time()\n",
    "k_means.fit(X)\n",
    "t_batch = time.time() - t0\n",
    "# #############################################################################\n",
    "# Compute clustering with MiniBatchKMeans\n",
    "mbk = MiniBatchKMeans(init='k-means++', n_clusters=3, batch_size=batch_size,\n",
    "n_init=10, max_no_improvement=10, verbose=0)\n",
    "t0 = time.time()\n",
    "\n",
    "mbk.fit(X)\n",
    "t_mini_batch = time.time() - t0\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "fig.subplots_adjust(left=0.02, right=0.98, bottom=0.05, top=0.9)\n",
    "colors = ['#4EACC5', '#FF9C34', '#4E9A06']\n",
    "# We want to have the same colors for the same cluster from the\n",
    "# MiniBatchKMeans and the KMeans algorithm. Let's pair the cluster centers per\n",
    "# closest one.\n",
    "k_means_cluster_centers = np.sort(k_means.cluster_centers_, axis=0)\n",
    "mbk_means_cluster_centers = np.sort(mbk.cluster_centers_, axis=0)\n",
    "k_means_labels = pairwise_distances_argmin(X, k_means_cluster_centers)\n",
    "mbk_means_labels = pairwise_distances_argmin(X, mbk_means_cluster_centers)\n",
    "order = pairwise_distances_argmin(k_means_cluster_centers,\n",
    "mbk_means_cluster_centers)\n",
    "# KMeans\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "for k, col in zip(range(n_clusters), colors):\n",
    "    my_members = k_means_labels == k\n",
    "    cluster_center = k_means_cluster_centers[k]\n",
    "    ax.plot(X[my_members, 0], X[my_members, 1], 'w',\n",
    "    markerfacecolor=col, marker='.')\n",
    "    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "    markeredgecolor='k', markersize=6)\n",
    "ax.set_title('KMeans')\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "plt.text(-3.5, 1.8, 'train time: %.2fs\\ninertia: %f' % (\n",
    "    t_batch, k_means.inertia_))\n",
    "\n",
    "# MiniBatchKMeans\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "for k, col in zip(range(n_clusters), colors):\n",
    "    my_members = mbk_means_labels == order[k]\n",
    "    cluster_center = mbk_means_cluster_centers[order[k]]\n",
    "    ax.plot(X[my_members, 0], X[my_members, 1], 'w',\n",
    "    markerfacecolor=col, marker='.')\n",
    "    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "    markeredgecolor='k', markersize=6)\n",
    "ax.set_title('MiniBatchKMeans')\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "plt.text(-3.5, 1.8, 'train time: %.2fs\\ninertia: %f' %\n",
    "    (t_mini_batch, mbk.inertia_))\n",
    "# Initialise the different array to all False\n",
    "different = (mbk_means_labels == 4)\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "for k in range(n_clusters):\n",
    "    different += ((k_means_labels == k) != (mbk_means_labels == order[k]))\n",
    "identic = np.logical_not(different)\n",
    "ax.plot(X[identic, 0], X[identic, 1], 'w',\n",
    "markerfacecolor='#bbbbbb', marker='.')\n",
    "ax.plot(X[different, 0], X[different, 1], 'w',\n",
    "markerfacecolor='m', marker='.')\n",
    "ax.set_title('Difference')\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
