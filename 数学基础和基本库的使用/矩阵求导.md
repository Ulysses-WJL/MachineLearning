#  矩阵求导

## 1. 布局法

### 1.1 向量对标量求导

标量$y$对标量$x$的求导，可以表示为$\frac{\partial y}{\partial x}$。

一组标量$y_i,i=1,2,...,m$来对一个标量$x$的求导,那么我们会得到一组标量求导的结果：$$\frac{\partial y_i}{\partial x}, i=1,2.,,,m$$

将标量y写成向量形式, 即得到维度为m的一个向量$\mathbf{y}$对一个标量$x$的求导，那么结果也是一个m维的向量：$\frac{\partial \mathbf{y}}{\partial x}$

向量对标量的求导，其实就是向量里的每个分量分别对标量求导，最后把求导的结果排列在一起，按一个向量表示而已。类似的结论也存在于标量对向量的求导，向量对向量的求导，向量对矩阵的求导，矩阵对向量的求导，以及矩阵对矩阵的求导等。所谓的向量矩阵求导本质上就是多元函数求导，仅仅是把把函数的自变量，因变量以及标量求导的结果排列成了向量矩阵的形式，方便表达与计算，更加简洁而已

求导的自变量用$x$表示标量，$\mathbf{x}$表示n维向量，$\mathbf{X}$表示$m \times n$维度的矩阵，求导的因变量用$y$表示标量，$\mathbf{y}$表示m维向量，$\mathbf{Y}$表示$p \times q$维度的矩阵.

### 1.2 向量求导布局

- 分子布局(numerator layout): 求导结果的维度以分子为主, 分子为列向量或者分母为行向量. 如果向量$\mathbf{y}$是一个m维的列向量，那么求导结果$\frac{\partial \mathbf{y}}{\partial x}$也是一个m维列向量。如果如果向量$\mathbf{y}$是一个m维的行向量，那么求导结果$\frac{\partial \mathbf{y}}{\partial x}$也是一个m维行向量

- 分母布局(denominator layout ): 求导结果的维度以分母为主, 分子为行向量或者分母为列向量. 如果向量$\mathbf{y}$是一个m维的列向量，那么求导结果$\frac{\partial \mathbf{y}}{\partial x}$是一个m维行向量。如果如果向量$\mathbf{y}$是一个m维的行向量，那么求导结果$\frac{\partial \mathbf{y}}{\partial x}$是一个m维的列向量。

对于分子布局和分母布局的结果来说，两者相差一个转置。

- 标量$y$对矩阵$ \mathbf{X}$求导，那么如果按分母布局，则求导结果的维度和矩阵$X$的维度$m \times n$是一致的。如果是分子布局，则求导结果的维度为$n \times m$。

- 向量对向量的求导: m维列向量$\mathbf{y}$对n维列向量$\mathbf{x}$求导。对于这2个向量求导，那么一共有$mn$个标量对标量的求导。求导的结果一般是排列为一个矩阵。

  + 如果是分子布局，则矩阵的第一个维度以分子为准，即结果是一个$m \times n$的矩阵，如下：$$ \frac{\partial  \mathbf{y}}{\partial \mathbf{x}} = \left( \begin{array}{ccc} \frac{\partial y_1}{\partial x_1}& \frac{\partial y_1}{\partial x_2}& \ldots & \frac{\partial y_1}{\partial x_n}\\  \frac{\partial y_2}{\partial x_1}& \frac{\partial y_2}{\partial x_2} & \ldots & \frac{\partial y_2}{\partial x_n}\\   \vdots&  \vdots &  \ddots & \vdots \\ \frac{\partial y_m}{\partial x_1}& \frac{\partial y_m}{\partial x_2} & \ldots & \frac{\partial y_m}{\partial x_n}  \end{array} \right)$$

    这个按分子布局的向量对向量求导的结果矩阵，我们一般叫做**雅克比 (Jacobian)**矩阵。有的资料上会使用$ \frac{\partial  \mathbf{y}}{\partial \mathbf{x^T}}$来定义雅克比矩阵。

  + 如果是按分母布局，则求导的结果矩阵的第一维度会以分母为准，即结果是一个$n \times m$的矩阵，如下：$$ \frac{\partial  \mathbf{y}}{\partial \mathbf{x}} = \left( \begin{array}{ccc} \frac{\partial y_1}{\partial x_1}& \frac{\partial y_2}{\partial x_1}& \ldots & \frac{\partial y_m}{\partial x_1}\\  \frac{\partial y_1}{\partial x_2}& \frac{\partial y_2}{\partial x_2} & \ldots & \frac{\partial y_m}{\partial x_2}\\   \vdots&  \vdots &  \ddots & \vdots \\ \frac{\partial y_1}{\partial x_n}& \frac{\partial y_2}{\partial x_n} & \ldots & \frac{\partial y_m}{\partial x_n}  \end{array} \right)$$

    这个按分母布局的向量对向量求导的结果矩阵，我们一般叫做**梯度矩阵**。有的资料上会使用$ \frac{\partial  \mathbf{y^T}}{\partial \mathbf{x}}​$来定义梯度矩阵

| 自变量\因变量      | 标量$y$                                                      | 列向量$\mathbf{y}$                                           | 矩阵$\mathbf{Y}$                                             |
| ------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 标量$x$            | /                                                            | $$\frac{\partial  \mathbf{y}}{\partial x}$$分子布局：m维列向量（默认布局）   分母布局：m维行向量 | $\frac{\partial \mathbf{Y}}{\partial x}$  分子布局：$p \times q$矩阵（默认布局）   分母布局：$q \times p$矩阵 |
| 列向量$\mathbf{x}$ | $\frac{\partial y}{\partial \mathbf{x}}$分子布局：n维行向量    分母布局：n维列向量（默认布局） | $\frac{\partial  \mathbf{y}}{\partial \mathbf{x}}$分子布局：$m \times n$雅克比矩阵（默认布局）     分母布局：$n \times m$梯度矩阵 | /                                                            |
| 矩阵$\mathbf{X}$   | $\frac{\partial y}{\partial \mathbf{X}}$分子布局：$n \times m$矩阵      分母布局：$m \times n$矩阵（默认布局） | /                                                            | /                                                            |

## 2 定义法



### 2.1 用定义法求解标量对向量求导

标量对向量求导，严格来说是实值函数对向量的求导。即定义实值函数$f: R^{n} \to R$,自变量$\mathbf{x}$是n维向量，而输出$y$是标量。对于一个给定的实值函数，如何求解$\frac{\partial y}{\partial \mathbf{x}}$呢？ 

根据矩阵求导的定义，由于所谓标量对向量的求导，其实就是标量对向量里的每个分量分别求导，最后把求导的结果排列在一起，按一个向量表示而已。那么我们可以将实值函数对向量的每一个分量来求导，最后找到规律，得到求导的结果向量。

例：$y=\mathbf{a}^T\mathbf{x}$,求解$\frac{\partial \mathbf{a}^T\mathbf{x}}{\partial \mathbf{x}}$

根据定义，我们先对$\mathbf{x}$的第i个分量进行求导，这是一个标量对标量的求导，如下：

$$\frac{\partial \mathbf{a}^T\mathbf{x}}{\partial x_i} = \frac{\partial \sum\limits_{j=1}^n a_jx_j}{\partial x_i} = \frac{\partial a_ix_i}{\partial x_i} =a_i$$

可见，对向量的第i个分量的求导结果就等于向量$\mathbf{a}$的第i个分量。由于我们是分母布局，最后所有求导结果的分量组成的是一个n维向量。那么其实就是向量$\mathbf{a}$。也就是说：$$\frac{\partial \mathbf{a}^T\mathbf{x}}{\partial \mathbf{x}} = \mathbf{a}$$

同样的思路，我们也可以直接得到：$$\frac{\partial \mathbf{x}^T\mathbf{a}}{\partial \mathbf{x}} = \mathbf{a}$$

再来看一个复杂一点点的例子：$y=\mathbf{x}^T\mathbf{A}\mathbf{x}$,求解$\frac{\partial \mathbf{x}^T\mathbf{A}\mathbf{x}}{\partial \mathbf{x}}$

我们对$\mathbf{x}$的第k个分量进行求导如下：

$$\frac{\partial \mathbf{x}^T\mathbf{A}\mathbf{x}}{\partial x_k} = \frac{\partial \sum\limits_{i=1}^n\sum\limits_{j=1}^n x_iA_{ij}x_j}{\partial x_k} = \sum\limits_{i=1}^n A_{ik}x_i + \sum\limits_{j=1}^n A_{kj}x_j $$

第一部分是矩阵$\mathbf{A}$的第k列转置后和$x$相乘得到，第二部分是矩阵$\mathbf{A}$的第k行和$x$相乘得到，排列好就是: $$\frac{\partial \mathbf{x}^T\mathbf{A}\mathbf{x}}{\partial \mathbf{x}} = \mathbf{A}^T\mathbf{x} + \mathbf{A}\mathbf{x}$$

从上面可以看出，定义法求导对于简单的实值函数是很容易的，但是复杂的实值函数就算求出了任意一个分量的导数，要排列出最终的求导结果还挺麻烦的，因此我们需要找到其他的简便一些的方法来整体求导，而不是每次都先去针对任意一个分量，再进行排列。

### 2.2 标量对向量求导的一些基本法则

1. 常量对向量的求导结果为0。
2. 线性法则：如果$f,g$都是实值函数，$c_1,c_2$为常数，则：$$\frac{\partial (c_1f(\mathbf{x})+c_2g(\mathbf{x})}{\partial \mathbf{x}} = c_1\frac{\partial f(\mathbf{x})}{\partial \mathbf{x}} +c_2\frac{\partial g(\mathbf{x})}{\partial \mathbf{x}} $$
3. 乘法法则：如果$f,g$都是**实值**函数，则：$$\frac{\partial f(\mathbf{x})g(\mathbf{x})}{\partial \mathbf{x}} = f(\mathbf{x})\frac{\partial g(\mathbf{x})}{\partial \mathbf{x}} +\frac{\partial f(\mathbf{x})}{\partial \mathbf{x}} g(\mathbf{x}) $$
	要注意的是如果不是实值函数，则不能这么使用乘法法则。

4. 除法法则：如果$f,g$都是实值函数，且$g(\mathbf{x}) \neq 0$，则：$$\frac{\partial f(\mathbf{x})/g(\mathbf{x})}{\partial \mathbf{x}} = \frac{1}{g^2(\mathbf{x})}(g(\mathbf{x})\frac{\partial f(\mathbf{x})}{\partial \mathbf{x}} - f(\mathbf{x})\frac{\partial g(\mathbf{x})}{\partial \mathbf{x}})$$

### 2.3 用定义法求解标量对矩阵求导

例: $y=\mathbf{a}^T\mathbf{X}\mathbf{b}$,求解$\frac{\partial \mathbf{a}^T\mathbf{X}\mathbf{b}}{\partial \mathbf{X}}$

其中, $\mathbf{a}$是m维向量,$\mathbf{b}$是n维向量,  $\mathbf{X}$是$m \times n$的矩阵

我们对矩阵$\mathbf{X}$的任意一个位置的$X_{ij}$求导，如下：$$\frac{\partial \mathbf{a}^T\mathbf{X}\mathbf{b}}{\partial X_{ij}} =  \frac{\partial \sum\limits_{p=1}^m\sum\limits_{q=1}^n a_pX_{pq}b_q}{\partial X_{ij}} =  \frac{\partial  a_iX_{ij}b_j}{\partial X_{ij}} = a_ib_j$$

即求导结果在$(i.j)$位置的求导结果是$\mathbf{a}$向量第i个分量和$\mathbf{b}$第j个分量的乘积，将所有的位置的求导结果排列成一个$m \times n$的矩阵，即为$ab^T$,这样最后的求导结果为：$$\frac{\partial \mathbf{a}^T\mathbf{X}\mathbf{b}}{\partial \mathbf{X}} = ab^T$$

简单的求导的确不难，但是如果是比较复杂的标量对矩阵求导，比如$y=\mathbf{a}^Texp(\mathbf{X}\mathbf{b})$,对任意标量求导容易，排列起来还是蛮麻烦的，也就是我们遇到了和标量对向量求导一样的问题，定义法比较适合解决简单的问题，复杂的求导需要更简便的方法.

### 2.4 用定义法求解向量对向量求导

例: $\mathbf{y} = \mathbf{A} \mathbf{x} $,其中$ \mathbf{A}$为$n \times m$的矩阵。$\mathbf{x}, \mathbf{y}$分别为$m,n$维向量。需要求导$\frac{\partial \mathbf{A}\mathbf{x}}{\partial \mathbf{x}}$,根据定义，结果应该是一个$n \times m$的矩阵(分子布局 雅克比矩阵)

先求矩阵的第i行和向量的内积对向量的第j分量求导，用定义法求解过程如下：$$\frac{\partial \mathbf{A}\mathbf{x}}{\partial \mathbf{x}} = \frac{\partial \sum\limits_{i=1}^m \mathbf{A_i}\mathbf{x}}{\partial \mathbf{x_j}} = \frac{\partial \sum\limits_{i=1}^m \sum\limits_{j=1}^n A_{ij}x_j}{\partial \mathbf{x_j}}= A_{ij}$$

可见矩阵 $\mathbf{A}$的第i行和向量的内积对向量的第j分量求导的结果就是矩阵 $\mathbf{A}$的$(i,j)$位置的值。排列起来就是一个矩阵了，由于我们分子布局，所以排列出的结果是$ \mathbf{A}$,而不是 $\mathbf{A}^T$

## 3 微分法

### 3.1 矩阵微分

标量的导数和微分: $df =f'(x)dx$, 如果是多变量的情况，则微分可以写成：$df=\sum\limits_{i=1}^n\frac{\partial f}{\partial x_i}dx_i = (\frac{\partial f}{\partial \mathbf{x}})^Td\mathbf{x}$

可以发现标量对向量的求导和它的向量微分有一个转置的关系.

再推广到矩阵。对于矩阵微分，我们的定义为：

$$ df=\sum\limits_{i=1}^m\sum\limits_{j=1}^n\frac{\partial f}{\partial X_{ij}}dX_{ij} = tr((\frac{\partial f}{\partial \mathbf{X}})^Td\mathbf{X}) $$

其中第二步使用了矩阵迹的性质，即迹函数等于主对角线的和。

即$$tr(A^TB) = \sum\limits_{i,j}A_{ij}B_{ij}$$

从上面矩阵微分的式子，我们可以看到矩阵微分和它的导数也有一个转置的关系，不过在外面套了一个迹函数而已。由于标量的迹函数就是它本身，那么矩阵微分和向量微分可以统一表示，即：$$df= tr((\frac{\partial f}{\partial \mathbf{X}})^Td\mathbf{X})\;\; \;df= tr((\frac{\partial f}{\partial \mathbf{x}})^Td\mathbf{x})$$

### 3.2 矩阵微分的性质

- 微分加减法：$d(X+Y) =dX+dY, d(X-Y) =dX-dY$
- 微分乘法：$d(XY) =(dX)Y + X(dY)$
- 微分转置：$d(X^T) =(dX)^T$
- 微分的迹：$dtr(X) =tr(dX)$
- 微分哈达马乘积： $d(X \odot Y) = X\odot dY + dX \odot Y$
- 逐元素求导：$d \sigma(X) =\sigma'(X) \odot dX$
- 逆矩阵微分：$d X^{-1}= -X^{-1}dXX^{-1}$
- 行列式微分：$d |X|= |X|tr(X^{-1}dX)$　

### 3.3 微分法求解矩阵向量求导

若标量函数$f$是矩阵$X$经加减乘法、逆、行列式、逐元素函数等运算构成，则使用相应的运算法则对$f$求微分，再使用迹函数技巧给$df$套上迹并将其它项交换至$dX$左侧,那么对于迹函数里面在$dX$左边的部分，我们只需要加一个转置就可以得到导数了。

迹函数的技巧:

- 标量的迹等于自己：$tr(x) =x$
-  转置不变：$tr(A^T) =tr(A)$
-  交换率：$tr(AB) =tr(BA)$,需要满足$A,B^T$同维度。
- 加减法：$tr(X+Y) =tr(X)+tr(Y), tr(X-Y) =tr(X)-tr(Y)$
- 矩阵乘法和迹交换：$tr((A\odot B)^TC)= tr(A^T(B \odot C))$,需要满足$A,B,C$同维度。

例1: $$y=\mathbf{a}^T\mathbf{X}\mathbf{b}, \frac{\partial y}{\partial \mathbf{X}}$$

首先，我们使用微分乘法的性质对$f$求微分，得到：$$dy = d\mathbf{a}^T\mathbf{X}\mathbf{b} + \mathbf{a}^Td\mathbf{X}\mathbf{b} + \mathbf{a}^T\mathbf{X}d\mathbf{b} = \mathbf{a}^Td\mathbf{X}\mathbf{b}$$

第二步，就是两边套上迹函数，即：$$dy =tr(dy) = tr(\mathbf{a}^Td\mathbf{X}\mathbf{b}) = tr(\mathbf{b}\mathbf{a}^Td\mathbf{X})$$

根据我们矩阵导数和微分的定义，迹函数里面在$dX$左边的部分$\mathbf{b}\mathbf{a}^T$,加上一个转置即为我们要求的导数，即：$$\frac{\partial f}{\partial \mathbf{X}} = (\mathbf{b}\mathbf{a}^T)^T =ab^T$$

以上就是微分法的基本流程，先求微分再做迹函数变换，最后得到求导结果。比起定义法，我们现在不需要去对矩阵中的单个标量进行求导了。

例2: $$y=\mathbf{a}^Texp(\mathbf{X}\mathbf{b}), \frac{\partial y}{\partial \mathbf{X}}$$

　$$dy =tr(dy) = tr(\mathbf{a}^Tdexp(\mathbf{X}\mathbf{b})) = tr(\mathbf{a}^T (exp(\mathbf{X}\mathbf{b}) \odot d(\mathbf{X}\mathbf{b}))) = tr((\mathbf{a}  \odot exp(\mathbf{X}\mathbf{b}) )^T d\mathbf{X}\mathbf{b})   \\ =   tr(d\mathbf{X} \mathbf{b}(\mathbf{a}  \odot exp(\mathbf{X}\mathbf{b}) )^T ) =  tr(\mathbf{b}(\mathbf{a}  \odot exp(\mathbf{X}\mathbf{b}) )^T d\mathbf{X}) $$

其中第3步到第4步使用了上面迹函数的性质5. 这样我们的求导结果为：$$\frac{\partial y}{\partial \mathbf{X}} =(\mathbf{a}  \odot exp(\mathbf{X}\mathbf{b}) )b^T $$

以上就是微分法的基本思路。

### 3.4  迹函数对向量矩阵求导

由于微分法使用了迹函数的技巧，那么迹函数对对向量矩阵求导这一大类问题，使用微分法是最简单直接的.

- $\frac{\partial tr(AB)}{\partial A} = B^T, \frac{\partial tr(AB)}{\partial B} =A^T$, 直接根据矩阵微分的定义即可得到
- $\frac{\partial tr(W^TAW)}{\partial W}$:$$d(tr(W^TAW)) = tr(dW^TAW +W^TAdW) = tr(dW^TAW)+tr(W^TAdW) = tr((dW)^TAW) + tr(W^TAdW) \\ = tr(W^TA^TdW) +  tr(W^TAdW) = tr(W^T(A+A^T)dW) $$可以得到：$$\frac{\partial tr(W^TAW)}{\partial W} = (A+A^T)W$$
- $\frac{\partial tr(B^TX^TCXB)}{\partial X} $: $$d(tr(B^TX^TCXB)) = tr(B^TdX^TCXB) + tr(B^TX^TCdXB) = tr((dX)^TCXBB^T) + tr(BB^TX^TCdX) \\ = tr(BB^TX^TC^TdX) + tr(BB^TX^TCdX) = tr((BB^TX^TC^T + BB^TX^TC)dX)$$
  $$\frac{\partial tr(B^TX^TCXB)}{\partial X}= (C+C^T)XBB^T$$

## 4. 矩阵向量求导的链式法则

### 4.1 向量对向量求导的链式法则

假设多个向量存在依赖关系，比如三个向量$\mathbf{x} \to \mathbf{y} \to \mathbf{z}$存在依赖关系，则我们有下面的链式求导法则：

$$\frac{\partial \mathbf{z}}{\partial \mathbf{x}} = \frac{\partial \mathbf{z}}{\partial \mathbf{y}} \frac{\partial \mathbf{y}}{\partial \mathbf{x}}$$

该法则也可以推广到更多的向量依赖关系。但是要注意的是要求所有有依赖关系的变量都是向量，如果有一个$\mathbf{Y}$是矩阵，比如是$\mathbf{x} \to \mathbf{Y} \to \mathbf{z}$， 则上式并不成立.

从矩阵维度相容的角度也很容易理解上面的链式法则，假设$\mathbf{x} , \mathbf{y} ,\mathbf{z}$分别是$m,n.p$维向量，则求导结果$\frac{\partial \mathbf{z}}{\partial \mathbf{x}}$是一个$p \times m$的雅克比矩阵，而右边$\frac{\partial \mathbf{z}}{\partial \mathbf{y}}$是一个$p \times n$的雅克比矩阵，$\frac{\partial \mathbf{y}}{\partial \mathbf{x}}$是一个$n \times m$的矩阵，两个雅克比矩阵的乘积维度刚好是$p \times m$，和左边相容。

### 4.2 标量对多个向量的链式求导法则

在机器学习算法中, 优化目标一般是一个标量损失函数，因此最后求导的目标是标量，无法使用上一节的链式求导法则，比如2向量，最后到1标量的依赖关系：$\mathbf{x} \to \mathbf{y} \to z$，此时很容易发现维度不相容。

假设$\mathbf{x} , \mathbf{y} $分别是$m,n$维向量, 那么$\frac{\partial z}{\partial \mathbf{x}}$的求导结果是一个$m \times 1$的向量, 而$\frac{\partial z}{\partial \mathbf{y}}$是一个$n \times 1$的向量，$\frac{\partial \mathbf{y}}{\partial \mathbf{x}}$是一个$n \times m$的雅克比矩阵,右边的向量和矩阵是没法直接乘的。

但是假如我们把标量求导的部分都做一个转置，那么维度就可以相容了，也就是：$$(\frac{\partial z}{\partial \mathbf{x}})^T = (\frac{\partial z}{\partial \mathbf{y}})^T\frac{\partial \mathbf{y}}{\partial \mathbf{x}} $$

可以得到标量对多个向量求导的链式法则：$$\frac{\partial z}{\partial \mathbf{x}} = (\frac{\partial \mathbf{y}}{\partial \mathbf{x}} )^T\frac{\partial z}{\partial \mathbf{y}}$$

如果是标量对更多的向量求导,比如$\mathbf{y_1} \to \mathbf{y_2}  \to ...\to  \mathbf{y_n} \to z$，则其链式求导表达式可以表示为：$$\frac{\partial z}{\partial \mathbf{y_1}} = (\frac{\partial \mathbf{y_n}}{\partial \mathbf{y_{n-1}}} \frac{\partial \mathbf{y_{n-1}}}{\partial \mathbf{y_{n-2}}} ...\frac{\partial \mathbf{y_2}}{\partial \mathbf{y_1}})^T\frac{\partial z}{\partial \mathbf{y_n}}$$

例如最小二乘法优化的目标是最小化如下损失函数:$$l=(X\theta - y)^T(X\theta - y)$$

我们优化的损失函数$l$是一个标量，而模型参数$\theta$是一-个向量，期望L对$\theta$求导，并求出导数等于0时候的极值点。我们假设向量$z = X\theta - y$, 则$l=z^Tz$， $\theta \to z \to l$存在链式求导的关系，因此：$$\frac{\partial l}{\partial \mathbf{\theta}} = (\frac{\partial z}{\partial \theta} )^T\frac{\partial l}{\partial \mathbf{z}} = X^T(2z) =2X^T(X\theta - y)$$

### 4.3 标量对多个矩阵的链式求导法则

标量对多个矩阵的链式求导法则，假设有这样的依赖关系：$\mathbf{X} \to \mathbf{Y} \to z$,那么我们有：$$\frac{\partial z}{\partial x_{ij}} = \sum\limits_{k,l}\frac{\partial z}{\partial Y_{kl}} \frac{\partial Y_{kl}}{\partial X_{ij}} =tr((\frac{\partial z}{\partial Y})^T\frac{\partial Y}{\partial X_{ij}})$$

虽然我们没有全局的标量对矩阵的链式求导法则，但是对于一些线性关系的链式求导，我们还是可以得到一些有用的结论的。

一个常见问题：$A,X,B,Y$都是矩阵，$z$是标量，其中$z= f(Y), Y=AX+B$,我们要求出$\frac{\partial z}{\partial X}$,这个问题在机器学习中是很常见的。此时，我们并不能直接整体使用矩阵的链式求导法则，因为矩阵对矩阵的求导结果不好处理。

使用定义法试一试,先使用上面的标量链式求导公式：$$\frac{\partial z}{\partial x_{ij}}  = \sum\limits_{k,l}\frac{\partial z}{\partial Y_{kl}} \frac{\partial Y_{kl}}{\partial X_{ij}}$$

再来看看后半部分的导数：

$$ \frac{\partial Y_{kl}}{\partial X_{ij}} =  \frac{\partial \sum\limits_s(A_{ks}X_{sl})}{\partial X_{ij}} =   \frac{\partial A_{ki}X_{il}}{\partial X_{ij}} =A_{ki}\delta_{lj}$$

其中$\delta_{lj}​$在$l=j​$时为1，否则为0

那么最终的标签链式求导公式转化为：$$\frac{\partial z}{\partial x_{ij}}  = \sum\limits_{k,l}\frac{\partial z}{\partial Y_{kl}} A_{ki}\delta_{lj} =  \sum\limits_{k}\frac{\partial z}{\partial Y_{kj}} A_{ki}$$

即矩阵$A^T$的第i行和$\frac{\partial z}{\partial Y} $的第j列的内积。排列成矩阵即为：$$\frac{\partial z}{\partial X} = A^T\frac{\partial z}{\partial Y}$$

总结下就是：$$z= f(Y), Y=AX+B \to \frac{\partial z}{\partial X} = A^T\frac{\partial z}{\partial Y}$$

这结论在$\mathbf{x}$是一个向量的时候也成立，即：$$z= f(\mathbf{y}), \mathbf{y}=A\mathbf{x}+\mathbf{b} \to \frac{\partial z}{\partial \mathbf{x}} = A^T\frac{\partial z}{\partial \mathbf{y}}$$

如果要求导的自变量在左边，线性变换在右边，也有类似稍有不同的结论如下，证明方法是类似的，这里直接给出结论：$$z= f(Y), Y=XA+B \to \frac{\partial z}{\partial X} = \frac{\partial z}{\partial Y}A^T$$ $$z= f(\mathbf{y}), \mathbf{y}=X\mathbf{a}+\mathbf{b} \to \frac{\partial z}{\partial \mathbf{X}} = \frac{\partial z}{\partial \mathbf{y}}a^T$$

## 5 矩阵对矩阵求导

###  5.1 矩阵对矩阵求导的定义

假设我们有一个$p \times q$的矩阵$F$要对$m \times n$的矩阵$X$求导，那么根据我们第一篇求导的定义，矩阵$F$中的$pq$个值要对矩阵$X$中的$mn$个值分别求导，那么求导的结果一共会有$mnpq$个。那么求导的结果如何排列呢？

- 第一种是矩阵$F$对矩阵$X$中的每个值$X_{ij}$求导，这样对于矩阵$X$每一个位置(i,j)求导得到的结果是一个矩阵$\frac{\partial F}{\partial X_{ij}}$,可以理解为矩阵$X$的每个位置都被替换成一个$p \times q$的矩阵，最后我们得到了一个$mp \times nq$的矩阵。
- 第二种和第一种类似，可以看做矩阵$F$中的每个值$F_{kl}$分别对矩阵$X$求导，这样矩阵$F$每一个位置(k,l)对矩阵$X$求导得到的结果是一个矩阵$\frac{\partial F_{kl}}{\partial X}$, 可以理解为矩阵$F$的每个位置都被替换成一个$m \times n$的矩阵，最后我们得到了一个$mp \times nq$的矩阵。

这两种定义虽然没有什么问题，但是很难用于实际的求导. 目前主流的矩阵对矩阵求导定义是对矩阵先做向量化，然后再使用向量对向量的求导。而这里的向量化一般是使用列向量化。也就是说，现在我们的矩阵对矩阵求导可以表示为：$$\frac{\partial F}{\partial X} = \frac{\partial vec(F)}{\partial vec(X)}$$

对于矩阵$F$，列向量化后，$vec(F)$的维度是$pq \times 1$的向量，同样的，$vec(X)$的维度是$mn \times 1$的向量。最终求导的结果，这里我们使用分母布局，得到的是一个$mn \times pq$的矩阵。

### 5.2 矩阵对矩阵求导的微分法

标量对向量矩阵求导的微分法里，我们有：$$df= tr((\frac{\partial f}{\partial \mathbf{X}})^Td\mathbf{X})$$

矩阵对矩阵求导的微分法，也有一些法则可以直接使用。主要集中在矩阵向量化后的运算法则，以及向量化和克罗内克积之间的关系.

矩阵向量化的主要运算法则有：

- 线性性质：$vec(A+B) =vec(A) +vec(B)$
- 矩阵乘法：$vec(AXB)= (B^T \bigotimes A)vec(X)$,其中$\bigotimes$是克罗内克积。
-  矩阵转置：$vec(A^T) =K_{mn}vec(A)$,其中$A$是$m \times n$的矩阵，$K_{mn}$是$mn \times mn$的交换矩阵，用于矩阵列向量化和行向量化之间的转换。
-  逐元素乘法：$vec(A \odot X) = diag(A)vec(X)$, 其中$diag(A)$是$mn \times mn$的对角矩阵，对角线上的元素是矩阵$A$按列向量化后排列出来的。

克罗内克积的主要运算法则有：

- $(A \bigotimes B)^T = A^T \bigotimes B^T$
- $vec(ab^T) = b \bigotimes a$
- $(A \bigotimes B)(C \bigotimes D )=AC \bigotimes BD$
- $K_{mn} = K_{nm}^T, K_{mn}K_{nm}=I$

　使用上面的性质，求出$vec(dF)$关于$ vec(dX)$的表达式，则表达式左边的转置即为我们要求的$\frac{\partial vec(F)}{\partial vec(X)} $,或者说$\frac{\partial F}{\partial X} $

### 5.3 矩阵对矩阵求导实例

 $\frac{\partial AXB}{\partial X}$, 假设A,X,B都是矩阵，X是$m \times n$的矩阵。

首先求$dF$: $$dF =AdXB$$

然后我们两边列向量化(之前的微分法是套上迹函数), 得到：$$vec(dF) = vec(AdXB) = (B^T \bigotimes A)vec(dX)$$

这样，我们就得到了求导结果为：$$\frac{\partial AXB}{\partial X} =  (B^T \bigotimes A)^T = B \bigotimes A^T$$

利用上面的结果我们也可以得到：

$$\frac{\partial AX}{\partial X} =  I_n \bigotimes A^T$$
$$\frac{\partial XB}{\partial X} =  B \bigotimes I_m$$

更复杂的矩阵对矩阵求导:$\frac{\partial Aexp(BXC)D}{\partial X}$

首先求微分得到：$$dF =A [dexp(BXC)]D = A[exp(BXC) \odot (BdXC)]D  $$

两边矩阵向量化，我们有：

$$vec(dF) = (D^T \bigotimes A) vec[exp(BXC) \odot (BdXC)]  \\ =  (D^T \bigotimes A) diag(exp(BXC))vec(BdXC)  \\ =  (D^T \bigotimes A) diag(exp(BXC))(C^T\bigotimes B)vec(dX) $$



由于矩阵对矩阵求导的结果包含克罗内克积，因此和之前我们讲到的其他类型的矩阵求导很不同，在机器学习算法优化中中，我们一般不在推导的时候使用矩阵对矩阵的求导，除非只是做定性的分析。

来源:

- [机器学习中的矩阵求导](https://www.cnblogs.com/pinard/p/10750718.html)
- [通过一个例子快速上手矩阵求导](https://blog.csdn.net/nomadlx53/article/details/50849941)
- [矩阵求导、几种重要的矩阵及常用的矩阵求导公式](https://blog.csdn.net/daaikuaichuan/article/details/80620518)
