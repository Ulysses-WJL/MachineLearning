# 概率论相关

## 基本概念

**概率分布(probability distribution)**
随机变量可以分为离散型随机变量和连续型随机变量。

相应的描述其概率分布的函数是

概率质量函数(Probability Mass Function, PMF):描述离散型随机变量的概率分布，通常用大写字母 $P$表示。

概率密度函数(Probability Density Function, PDF):描述连续型随机变量的概率分布，通常用小写字母$p$表示。

PMF 可以同时作用于多个随机变量，即**联合概率分布(joint probability distribution)** $P(X=x,Y=y)$表示 $X=x$和$Y=y$同时发生的概率，也可以简写成 $P(x,y)$

**边缘概率**

求和法则计算P(x):
$$
\forall x \in \mathrm x, P(\mathrm x = x)=\sum_yP(\mathrm x=x,\mathrm y=y)
$$
**条件概率**
设A, B是两个事件, 且$P(A) \geq 0$,称
$$P(B|A) = \frac {P(AB)}{P(A)}$$
为在事件A发生条件下事件B发生的条件概率

**独立性**
两个随机变量$x$和$y$，概率分布表示成两个因子乘积形式，一个因子只包含$x$，另一个因子只包含$y$，两个随机变量相互独立(independent)。
条件有时为不独立的事件之间带来独立，有时也会把本来独立的事件，因为此条件的存在，而失去独立性。
举例：$P(XY)=P(X)P(Y)$, 事件$X$和事件$Y$独立。此时给定$Z$，
$$
P(X,Y|Z) \not = P(X|Z)P(Y|Z)
$$
事件独立时，联合概率等于概率的乘积。这是一个非常好的数学性质，然而不幸的是，无条件的独立是十分稀少的，因为大部分情况下，事件之间都是互相影响的。

**条件独立性**
​给定$Z$的情况下,$X$和$Y$条件独立，当且仅当
$$
X\bot Y|Z \iff P(X,Y|Z) = P(X|Z)P(Y|Z)
$$
$X$和$Y$的关系依赖于$Z$，而不是直接产生。

> **举例**定义如下事件：
> $X$：明天下雨；
> $Y$：今天的地面是湿的；
> $Z$：今天是否下雨；
> $Z$事件的成立，对$X$和$Y$均有影响，然而，在$Z$事件成立的前提下，今天的地面情况对明天是否下雨没有影响。

**独立同分布（iid，independently identically distribution)**，指随机过程中，任何时刻的取值都为随机变量，如果这些随机变量服从同一分布，并且互相独立，那么这些随机变量是独立同分布。

**全概率公式**
$$P(A) = \sum_{i=1}^n P(A|B_i)P(B_i), \quad i=1, 2, \dots, n$$
**贝叶斯公式**
$$P(B_i|A) = \frac {P(A|B_i)P(B_i)}{\sum_{i=1}^n P(A|B_j)P(B_j)}, \quad i=1, 2, \dots, n$$

**频率派概率(frequentist probability)**
概率直接与事件发生的频率相联系: 某一事件发生的概率为p, 意味着如果反复实验无数次, 有p比例的可能会导致这样的结果.
**贝叶斯概率(Bayesian probability)**
用概率表示一种**信任度**(degree of belief): 如判断肿瘤是否为恶性.
[贝叶斯统计](https://blog.csdn.net/jackxu8/article/details/70332331)

## 2 随机变量及其分布

### 2.1 分布函数 概率密度函数
**随机变量的分布函数**

设$X$是一个随机变量, x是任意实数, 函数
$$F(x) = P(X \leq x), -\infty < x < \infty$$
称为$X$的**分布函数**.

**连续随机变量及其概率密度函数**
如果对于随机变量$X$的分布函数$F(x)$, 存在非负可积函数$f(x)$, 对任意实数$x$有
$$F(x)= \int_{-\infty}^x f(t)dt, $$
则称$X$为**连续随机变量**, $f(x)$称为$X$的**概率密度函数**, 简称**概率密度**.

### 2.2 常见概率分布

**Bernoulli分布 (0, 1)分布**是单个二值随机变量分布, 单参数$\phi$∈[0,1]控制,$\phi$给出随机变量等于1的概率. 主要性质有:
$$
\begin{aligned}
P(x=1) &= \phi \\
P(x=0) &= 1-\phi  \\
P(x=x) &= \phi^x(1-\phi)^{1-x} \\
\end{aligned}
$$
其期望和方差为：
$$
\begin{aligned}
E_x[x] &= \phi \\
Var_x(x) &= \phi{(1-\phi)}
\end{aligned}
$$
**Multinoulli分布**也叫**范畴分布**, 是单个*k*值随机分布,经常用来表示**对象分类的分布**. 其中$k$是有限值.Multinoulli分布由向量$\vec{p}\in[0,1]^{k-1}$参数化,每个分量$p_i$表示第$i$个状态的概率, 且$p_k=1-1^Tp$.


**二项分布（Binomial Distribution）**
即重复n次的伯努利试验（Bernoulli Experiment)(每次试验概率不变， 各次试验的结果互不影响)。
如果A事件发生的概率是p, N次独立重复试验中A发生K次的概率是
$$\begin{aligned}P(X=K) &= \begin{pmatrix}n \\ k\end{pmatrix}p^k(1-p)^{n-k} \\
&= C_n^k p^k(1-p)^{n-k} \\ &= \frac {n!} {k!(n-k)!}p^k(1-p)^{n-k}
\end{aligned}, \quad k=0, 1, 2, \cdots, n.$$
期望：$$E(X) = np$$
方差：$$Var(X) = np(1-p)$$

**高斯分布**

高斯分布也叫**正态分布(Normal Distribution)**, 概率度函数如下:
$$
N(x;\mu,\sigma^2) = \sqrt{\frac{1}{2\pi\sigma^2}}exp\left ( -\frac{1}{2\sigma^2}(x-\mu)^2 \right )
$$
其中, $\mu​$和$\sigma​$分别是均值和方差, 中心峰值x坐标由$\mu​$给出, 峰的宽度受$\sigma​$控制, 最大点在$x=\mu​$处取得, 拐点为$x=\mu\pm\sigma​$

正态分布中，±1$\sigma$、±2$\sigma$、±3$\sigma$下的概率分别是68.3%、95.5%、99.73%，这3个数最好记住。

此外, 令$\mu=0,\sigma=1​$高斯分布即简化为标准正态分布:
$$
N(x;\mu,\sigma^2) = \sqrt{\frac{1}{2\pi}}exp\left ( -\frac{1}{2}x^2 \right )
$$
对概率密度函数高效求值:
$$
N(x;\mu,\beta^{-1})=\sqrt{\frac{\beta}{2\pi}}exp\left(-\frac{1}{2}\beta(x-\mu)^2\right)
$$


其中，$\beta=\frac{1}{\sigma^2}$通过参数$\beta∈（0，\infty）​$来控制分布精度。

**何时采用正态分布**

问: 何时采用正态分布?
答: 缺乏实数上分布的先验知识, 不知选择何种形式时, 默认选择正态分布总是不会错的, 理由如下:

1. 中心极限定理告诉我们, 很多独立随机变量均近似服从正态分布, 现实中很多复杂系统都可以被建模成正态分布的噪声, 即使该系统可以被结构化分解.
2. 正态分布是具有相同方差的所有概率分布中, 不确定性最大的分布, 换句话说, 正态分布是对模型加入先验知识最少的分布.

正态分布的推广:
正态分布可以推广到$R^n$空间, 此时称为**多维正态分布**, 其参数是一个正定对称矩阵$\Sigma$:
$$
N(x;\vec\mu,\Sigma)=\sqrt{\frac{1}{(2\pi)^ndet(\Sigma)}}exp\left(-\frac{1}{2}(\vec{x}-\vec{\mu})^T\Sigma^{-1}(\vec{x}-\vec{\mu})\right)
$$
对多为正态分布概率密度高效求值:
$$
N(x;\vec{\mu},\vec\beta^{-1}) = \sqrt{det(\vec\beta)}{(2\pi)^n}exp\left(-\frac{1}{2}(\vec{x}-\vec\mu)^T\beta(\vec{x}-\vec\mu)\right)
$$
此处，$\vec\beta$是一个精度矩阵。

**Laplace 分布**

一个联系紧密的概率分布是 Laplace 分布（Laplace distribution），它允许我们在任意一点 $\mu$处设置概率质量的峰值
$$
Laplace(x;\mu;\gamma)=\frac{1}{2\gamma}exp\left(-\frac{|x-\mu|}{\gamma}\right)
$$

**Dirac分布和经验分布**

Dirac分布可保证概率分布中所有质量都集中在一个点上. Diract分布的狄拉克$\delta​$函数(也称为**单位脉冲函数**)定义如下:
$$
p(x)=\delta(x-\mu), x\neq \mu
$$

$$
\int_{a}^{b}\delta(x-\mu)dx = 1, a < \mu < b
$$

Dirac 分布经常作为 经验分布（empirical distribution）的一个组成部分出现
$$
\hat{p}(\vec{x})=\frac{1}{m}\sum_{i=1}^{m}\delta(\vec{x}-{\vec{x}}^{(i)})
$$
, 其中, m个点$x^{1},...,x^{m}$是给定的数据集, **经验分布**将概率密度$\frac{1}{m}​$赋给了这些点.

当我们在训练集上训练模型时, 可以认为从这个训练集上得到的经验分布指明了**采样来源**.

**适用范围**: 狄拉克δ函数适合对**连续型**随机变量的经验分布.

**均匀分布**
连续随机变量$X$具有概率密度
$$f(x) = \begin{cases}\frac 1 {b-a}, \quad a<x<b, \\
0, \quad 其他\end{cases}$$
则称$X$在区间$(a, b)$上服从均匀分布

**指数分布**
$$f(x) = \begin{cases}\frac 1 \theta e^{-\frac x {\theta}}, x>0, \\
0, \quad 其他\end{cases}$$
$\theta > 0$为常数
深度学习中, 指数分布用来描述在$x=0​$点处取得边界点的分布, 指数分布定义如下:
$$
p(x;\lambda)=\lambda I_{x\geq 0}exp(-\lambda{x})
$$
指数分布用指示函数$I_{x\geq 0}​$来使$x​$取负值时的概率为零。

## 3 随机变量的数字特征

### 期望

在概率论和统计学中，数学期望（或均值，亦简称期望）是试验中每次可能结果的概率乘以其结果的总和。它反映随机变量平均取值的大小。

**定义**

设离散型随机变量$X$的分布律是:
$$P\{ X=x_k\}=p_k, \quad k=1, 2, \cdots.$$
若级数$\sum_{k=1}^{\infty}x_kp_k$绝对收敛, 则称级数$\sum_{k=1}^{\infty}x_kp_k$的和为随机变量$X$的数学期望:
$$E(X)=\sum_{k=1}^{\infty}x_kp_k$$
设连续型随机变量$X$的概率密度为$f(x)$,若积分$\int_{-\infty}^{+\infty}xf(x)dx$绝对收敛, 则称积分$\int_{-\infty}^{+\infty}xf(x)dx$为随机变量$X$的数学期望:
$$E(X) = \int_{-\infty}^{+\infty}xf(x)dx$$


- 线性运算： $E(ax+by+c) = aE(x)+bE(y)+c$
- 推广形式： $E(\sum_{k=1}^{n}{a_ix_i+c}) = \sum_{k=1}^{n}{a_iE(x_i)+c}$
- 函数期望：设$f(x)$为$x$的函数，则$f(x)$的期望为
    - 离散函数： $E(f(x))=\sum_{k=1}^{n}{f(x_k)P(x_k)}$
    - 连续函数： $E(f(x))=\int_{-\infty}^{+\infty}{f(x)p(x)dx}$

> 注意：
>
> - 函数的期望大于等于期望的函数（Jensen不等式），即$E(f(x))\geqslant f(E(x))$
> - 一般情况下，乘积的期望不等于期望的乘积。
> - 如果$X$和$Y$相互独立，则$E(xy)=E(x)E(y)​$。

### 方差

概率论中方差用来度量随机变量和其数学期望（即均值）之间的偏离程度。方差是一种特殊的期望。定义为：

$$
Var(x) = E((x-E(x))^2)
$$
对于离散型随机变量, 有:
$$Var(X) = \sum_{k=1}^{\infty}[x_k - E(X)]^2p_k$$
对于连续型随机变量, 有:
$$Var(X) = \int_{-\infty}^{\infty}[x_k - E(X)]^2f(x)dx$$
> 方差性质：
>
> 1）$Var(x) = E(x^2) -E(x)^2$
> 2）常数的方差为0;
> 3）方差不满足线性性质;
> 4）如果$X$和$Y$相互独立, $Var(ax+by)=a^2Var(x)+b^2Var(y)$

### 协方差

协方差是衡量两个变量线性相关性强度及变量尺度。  两个随机变量的协方差定义为：
$$
Cov(x,y)=E((x-E(x))(y-E(y)))
$$

方差是一种特殊的协方差。当$X=Y$时，$Cov(x,y)=Var(x)=Var(y)$。

> 协方差性质：
>
> 1）独立变量的协方差为0。
> 2）协方差计算公式：

$$
Cov(\sum_{i=1}^{m}{a_ix_i}, \sum_{j=1}^{m}{b_jy_j}) = \sum_{i=1}^{m} \sum_{j=1}^{m}{a_ib_jCov(x_iy_i)}
$$

>
> 3）特殊情况：

$$
Cov(a+bx, c+dy) = bdCov(x, y)
$$

### 相关系数

相关系数是研究变量之间线性相关程度的量。两个随机变量的相关系数定义为：
$$
Corr(x,y) = \frac{Cov(x,y)}{\sqrt{Var(x)Var(y)}}
$$

> 相关系数的性质：
> 1）有界性。相关系数的取值范围是 [-1,1]，可以看成无量纲的协方差。
> 2）值越接近1，说明两个变量正相关性（线性）越强。越接近-1，说明负相关性越强，当为0时，表示两个变量没有相关性。

### 特征函数

特征函数相关:
https://www.zhihu.com/question/23686709/answer/383239222
https://blog.csdn.net/shayashi/article/details/82500031


## 4 大数定律和中心极限定理

**大数定律**
频率的稳定性: 随机事件A的频率$f_n(A)$当重复试验的次数n增大时总呈现出稳定性, 稳定在一个常数附近.

随机变量序列的前一些项的算数平均值在某种条件下收敛到这些项的均值的算数平均值

**中心极限定理**

在客观实际中有很多随机变量, 它们是由大量的相互独立的随机因素的综合影响所形成的. 而其中每一个因素在总的影响中所起的作用都是微小的. 这种随机变量往往近似地服从正态分布.


## 5 参数估计

### 5.1 点估计

设总体$X$的分布函数的形式已知, 但它的一个或多个参数未知, 借助于总体$X$的一个样本来估计总体的未知参数的值的问题称为参数的**点估计**问题.

点估计问题的一般提法如下: 设总体$X$的分布函数$F(x;\theta)$的形式为已知, $\theta$为待估参数. $X_1, X_2, \dots, X_n$是$X$的一个样本, $x_1, x_2, \dots, x_n$是相应的一个样本值. 点估计问题就是要构造一个适当的统计量$\hat \theta(X_1, X_2, \dots, X_n)$, 用它的观察值$\hat \theta(x_1, x_2, \dots, x_n)$作为未知参数$\theta$的近似值. 我们称$\hat \theta(X_1, X_2, \dots, X_n)$为$\theta$的**估计量**, 称$\hat \theta(x_1, x_2, \dots, x_n)$为$\theta$的**估计值**.

### 5.2 最大似然估计
若总体$X$数离散型, 其分布律$P\{X=x\}=p(x;\theta), \theta \in \Theta$的形式为已知, $\theta$为待估参数, $\Theta$为$\theta$可能的取值范围. 设$X_1, X_2, \dots, X_n$是$X$的一个样本, 则$X_1, X_2, \dots, X_n$的联合分布律为
$$\prod_{i=1}^{n}p(x_i;\theta)$$
又设$x_1, x_2, \dots, x_n$是对应样本$X_1, X_2, \dots, X_n$的一个样本值(一系列已知的常数), 易知样本$X_1, X_2, \dots, X_n$取到观察值$x_1, x_2, \dots, x_n$的概率, 即事件$\{X_1=x_1, X_2=x_2, \dots, X_n=x_n\}$发生的概率为
$$L(\theta) = L(x_1, x_2, \dots, x_n;\theta) = \prod_{i=1}^{n}p(x_i;\theta), \quad \theta \in \Theta$$
这一概率随着$\theta$的取值而改变, 它是$\theta$的函数, $L(\theta)$称为样本的**似然函数(likelihood function)**

固定样本的取值$x_1, x_2, \dots, x_n$, 在$\theta$的取值范围$\Theta$内挑选使似然函数$L(x_1, x_2, \dots, x_n;\theta)$达到最大的参数$\hat \theta$, 作为参数$\theta$的估计值. 即取$\hat \theta$使
$$L(x_1, x_2, \dots, x_n;\hat \theta) = \mathbb{max}_{\theta \in \Theta} L(x_1, x_2, \dots, x_n;\theta)$$
这样得到的$\hat \theta$与样本值$x_1, x_2, \dots, x_n$有关, 常记为$\hat \theta(x_1, x_2, \dots, x_n)$, 称为参数的**最大似然估计**

## 6 信息论

信息论的基本想法是一个不太可能的事情发生了, 要比一个非常可能的事件发生, 能提供更多的信息.

定义一个事件$\mathrm x = x$的**自信息**(self-information)为
$$
I(x) = -logP(x)
$$

> log表示自然对数, 底数为e. I(x)的单位为**奈特**(nats).一奈特是以$\frac 1 e$的概率观测到一个事件时获得的信息量. 若使用
>
> 度数为2的对数, 则单位为**比特**(bit)或**香农**(shannous)

自信息只处理单个的输出.我们可以用**香农熵**(Shannou entropy)来对整个概率分布中的不确定性总量进行量化:
$$
H(x) = E_{x \sim P}[I(x)] = -E_{x \sim P}[logP(x)] = -\sum_xP(x)logP(x)
$$
也记作H(P).一个分布的香农熵是指遵循这个分布的事件所产生的期望信息的总量.那些接近确定性的分布(输出几乎可以确定)具有较低的熵; 那些接近均匀分布的概率分布具有较高的熵.

如果对同一个随机变量x有两个单独的概率分布P(x)和Q(x), 可以使用**KL散度**来衡量这两个分布的差异:
$$
D_{KL}(p||q) = \sum_x p(x)log(\frac {p(x)}{q(x)})
$$
KL散度是不对称的, 非负的.KL散度为0, 当且仅当P和Q在离散型变量的情况下是相同分布, 或连续型变量情况下"几乎处处"相同.

**交叉熵**:
$$
H(p||q) = H(p) + D_{KL}(p||q)
$$


