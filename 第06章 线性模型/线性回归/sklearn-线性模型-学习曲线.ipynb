{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 普通最小二乘法回归\n",
    "---\n",
    "最标准的线性模型是“普通最小二乘回归”，通常简称为“线性回归”。 它没有对coef_施加任何额外限制，因此当特征数量很大时，它会变得行为异常，并且模型会过拟合。\n",
    "$$\\underset{\\theta}{min} {|| X\\theta - y||_2}^2$$\n",
    "通过回归方程求导得到的最佳系数$\\hat\\theta = (X^TX)^{-1}X^Ty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=16)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**make_regression**\n",
    "```\n",
    "Generate a random regression problem.\n",
    "    n_samples=100,\n",
    "    n_features=100,\n",
    "    n_informative=10,\n",
    "    n_targets=1,\n",
    "    bias=0.0,\n",
    "    effective_rank=None,\n",
    "    tail_strength=0.5,\n",
    "    noise=0.0,\n",
    "    shuffle=True,\n",
    "    coef=False,\n",
    "    random_state=None,\n",
    "```\n",
    "```\n",
    "Returns\n",
    "-------\n",
    "X : array of shape [n_samples, n_features]\n",
    "    The input samples.\n",
    "\n",
    "y : array of shape [n_samples] or [n_samples, n_targets]\n",
    "    The output values.\n",
    "\n",
    "coef : array of shape [n_features] or [n_features, n_targets], optional\n",
    "    The coefficient of the underlying linear model. It is returned only if\n",
    "    coef is True.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 30)\n",
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "# 只有10个是有用的特征 , 添加了噪声\n",
    "X, y, true_coef = make_regression(n_samples=200, n_features=30, n_informative=10, noise=100, coef=True, random_state=5)  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5, train_size=60, test_size=140)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R2 score, 用来计算[可决系数](https://baike.baidu.com/item/%E5%8F%AF%E5%86%B3%E7%B3%BB%E6%95%B0)(the coefficient of determination)**\n",
    "$$R^2(y, \\hat y) = 1 - \\frac {\\sum_{i=1}^{n\\_sample}(y_i - \\hat y_i)^2}{\\sum_{i=1}^{n\\_sample}(y_i - \\bar y)^2}$$\n",
    "其中$y_i$表示第i个样本的真实值, $\\hat y_i$ 为预测值,   \n",
    "$\\bar y = \\frac 1 {n\\_sample}\\sum_{i=1}^{n\\_sample} y_i$为实际值的均值\n",
    "R2 score的值为1.0时最佳, 也可能为负数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 on training set: 0.878011\n",
      "R^2 on test set: 0.216332\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"R^2 on training set: %f\" % lr.score(X_train, y_train))\n",
    "print(\"R^2 on test set: %f\" % lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5985284495875146"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# 整体集合的R2 score\n",
    "r2_score(np.dot(X, true_coef), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f13342ce7f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "# 系数从大到小排序 画图比较\n",
    "coefficient_sorting = np.argsort(true_coef)[::-1]\n",
    "plt.plot(true_coef[coefficient_sorting], \"o\", label=\"true\")\n",
    "plt.plot(lr.coef_[coefficient_sorting], \"o\", label=\"linear regression\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**learning curve 学习曲线**\n",
    "---\n",
    "A learning curve shows the validation and training score of an estimator for varying numbers of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 16,  52,  88, 124, 160])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    LinearRegression(), X, y, train_sizes=np.linspace(.1, 1, 5), cv=5)\n",
    "train_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.    , 1.    , 1.    , 1.    , 1.    ],\n",
       "       [0.8361, 0.8313, 0.8395, 0.8395, 0.8395],\n",
       "       [0.8266, 0.7631, 0.7211, 0.7461, 0.7461],\n",
       "       [0.8401, 0.7952, 0.7754, 0.7855, 0.7735],\n",
       "       [0.8028, 0.7629, 0.7478, 0.7579, 0.7798]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4851,  0.2807,  0.2403, -0.1845,  0.4229],\n",
       "       [ 0.0926,  0.2826,  0.4763, -0.0515,  0.2072],\n",
       "       [ 0.1387,  0.6938,  0.7296,  0.5487,  0.5216],\n",
       "       [ 0.17  ,  0.6873,  0.7396,  0.6146,  0.6083],\n",
       "       [ 0.288 ,  0.7235,  0.749 ,  0.6684,  0.6191]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(est, X, y):\n",
    "    plt.figure()\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        LinearRegression(), X, y, train_sizes=np.linspace(.1, 1, 20), cv=5)\n",
    "    estimator_name = est.__class__.__name__\n",
    "    # 训练集的 训练集大小-socre 分数 曲线\n",
    "    line = plt.plot(train_sizes, train_scores.mean(axis=1), '--', label=f\"train scores {estimator_name}\")\n",
    "    plt.plot(train_sizes, test_scores.mean(axis=1), '-', label=f\"test scores {estimator_name}\")\n",
    "    plt.xlabel(\"Training set size\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylim(-0.1, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(LinearRegression(), X, y)\n",
    "# 不同训练集大小 训练出来的 结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**普通最小二乘法的复杂度**\n",
    "\n",
    "该方法使用 X 的奇异值分解来计算最小二乘解。如果 X 是一个形状为 (n_samples, n_features)的矩阵，设$$n_{samples} \\geq n_{features}$$, 则该方法的复杂度为$$O(n_{samples} n_{fearures}^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 岭回归（L2 惩罚）\n",
    "---\n",
    "岭估计器是普通LinearRegression的简单正则化（称为 l2 惩罚）。 特别是，它具有的优点是，在计算上不比普通的最小二乘估计更昂贵。\n",
    "$$\\underset {\\theta}{min} ||X\\theta - y||_2^2 + \\alpha ||\\theta||_2^2$$\n",
    "其中， $\\alpha \\geq 0$ 是控制系数收缩量的复杂性参数： $\\alpha$ 的值越大，收缩量越大，模型对共线性的鲁棒性也更强。\n",
    "\n",
    "最优的$\\hat \\theta = (X^TX+\\alpha I)^{-1}X^Ty$, 它是一个关于$\\alpha$的函数.  \n",
    "**岭回归的复杂度**\n",
    "这种方法与 `普通最小二乘法` 的复杂度是相同的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们加载一个不满秩（low effective rank）数据集来比较岭回归和线性回归。秩是矩阵线性无关组的数量，满秩是指一个$m \\times n$矩阵中行向量或列向量中现行无关组的数量等于$min(m,n)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0119,  0.0026,  0.0078],\n",
       "       [ 0.0036, -0.0087,  0.0114],\n",
       "       [-0.026 , -0.0295, -0.0108],\n",
       "       ...,\n",
       "       [ 0.0076,  0.0056, -0.0014],\n",
       "       [ 0.0178,  0.0077,  0.0094],\n",
       "       [ 0.0128, -0.0112, -0.004 ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 建一个有3个自变量的数据集，但是其秩为2，因此3个自变量中有两个自变量存在相关性\n",
    "X, y = make_regression(n_samples=2000, n_features=3, effective_rank=2, noise=10)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression(lr, X, y):\n",
    "    n_sample, n_feature = X.shape\n",
    "    n_bootstraps = 1000 # 1000次\n",
    "    coefs = np.zeros((n_bootstraps, n_feature))\n",
    "    scores = np.zeros((n_bootstraps, 2))\n",
    "    \n",
    "    for i in range(n_bootstraps):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25)\n",
    "        lr.fit(X_train, y_train)\n",
    "        scores[i] = (lr.score(X_train, y_train), lr.score(X_test, y_test))\n",
    "        coefs[i] = lr.coef_\n",
    "    f, axes = plt.subplots(nrows=n_feature, sharex=True, sharey=True, figsize=(10, 8))\n",
    "    for i, ax in enumerate(axes):\n",
    "        # 频率分布直方图\n",
    "        ax.hist(coefs[:, i], alpha=.5)\n",
    "        ax.set_title(\"Coef {}\".format(i))\n",
    "    plt.show()\n",
    "    return coefs, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 普通的线性回归\n",
    "coefs_lr, scores_lr = plot_regression(LinearRegression(), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 岭回归\n",
    "coefs_ridge, scores_ridge = plot_regression(Ridge(), X, y)  # 正则化系数 alpha 默认1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然, 岭回归的系数更接近0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-42.537 , -51.3064, -46.7559])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(coefs_ridge - coefs_lr, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从均值上看，线性回归比岭回归的系数要大很多。均值显示的差异其实是线性回归的系数隐含的偏差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41.143 , 76.4322, 56.9921])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs_lr.var(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.3656, 5.2234, 5.764 ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs_ridge.var(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0546, 0.0635],\n",
       "       [0.0486, 0.0744],\n",
       "       [0.0631, 0.0356],\n",
       "       ...,\n",
       "       [0.0598, 0.0482],\n",
       "       [0.0539, 0.059 ],\n",
       "       [0.0608, 0.0424]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0579, 0.0507]), array([0.0343, 0.0304]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_lr.mean(0), scores_ridge.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "岭回归的系数方差也会小很多。这就是机器学习里著名的偏差-方差均衡(Bias-Variance Trade-off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化岭回归参数 \n",
    "---\n",
    "用OLS（普通最小二乘法）做回归也许可以显示两个变量之间的某些关系；但是，当alpha参数正则化之后，那些关系就会消失.\n",
    "\n",
    "在linear_models模块中，有一个对象叫RidgeCV，表示**岭回归交叉检验**（ridge cross-validation）。这个交叉检验类似于**留一交叉验证法**（leave-one-out cross-validation，LOOCV）\n",
    "\n",
    "指定cv属性的值将触发(通过GridSearchCV的)交叉验证。例如，cv=10将触发10折的交叉验证，而不是广义交叉验证(GCV)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([     0.    ,      0.0001,      0.001 ,      0.01  ,      0.1   ,\n",
       "            1.    ,     10.    ,    100.    ,   1000.    ,  10000.    ,\n",
       "       100000.    ]),\n",
       "        cv=None, fit_intercept=True, gcv_mode=None, normalize=False,\n",
       "        scoring=None, store_cv_values=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=2, effective_rank=1, noise=10, random_state=12345)\n",
    "rcv = RidgeCV(alphas=np.logspace(-5, 5, 11))\n",
    "rcv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 拟合模型之后，alpha参数就是最优参数：\n",
    "rcv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看0.1附近更优的alpha值\n",
    "rcv = RidgeCV(alphas=np.linspace(.05, .2, 16))\n",
    "rcv.fit(X, y)\n",
    "rcv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([0.001, 0.002, 0.003, ..., 0.998, 0.999, 1.   ]), cv=None,\n",
       "        fit_intercept=True, gcv_mode=None, normalize=False, scoring=None,\n",
       "        store_cv_values=True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_list = np.linspace(0.001, 1, 1000)\n",
    "rcv = RidgeCV(alphas=alpha_list, store_cv_values=True)  # 保存交叉检验的数据\n",
    "rcv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1000)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.cv_values_.shape  # 100次交叉验证  1000个不同alpha 的均方根误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_alpha_idx = rcv.cv_values_.mean(0).argmin()\n",
    "min_alpha_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036000000000000004"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_list[min_alpha_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.036000000000000004"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_ridgecv(alpha_list, X, y):\n",
    "    rcv = RidgeCV(alphas=alpha_list, store_cv_values=True)  # 保存交叉检验的数据\n",
    "    rcv.fit(X, y)\n",
    "    min_alpha_idx = rcv.cv_values_.mean(0).argmin()\n",
    "    f, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.set_title(r\"Various values of $\\alpha$\")\n",
    "    xy = (alpha_list[min_alpha_idx], rcv.cv_values_.mean(axis=0)[min_alpha_idx])\n",
    "    xytext = (xy[0] + .01, xy[1] + .1)\n",
    "    \n",
    "    ax.annotate(r'Chosen $\\alpha$', xy=xy, xytext=xytext,\n",
    "            arrowprops=dict(facecolor='black', shrink=0, width=0)\n",
    "            )\n",
    "    ax.plot(alpha_list, rcv.cv_values_.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_ridgecv(np.linspace(0.001, 0.1, 100), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeCV(alphas=array([0.01  , 0.0109, 0.0118, ..., 0.0982, 0.0991, 0.1   ]),\n",
       "        cv=None, fit_intercept=True, gcv_mode=None, normalize=False,\n",
       "        scoring=make_scorer(mad, greater_is_better=False),\n",
       "        store_cv_values=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "# 使用自定义的函数(偏差的绝对值的均值)   作为评分函数\n",
    "def mad(y, y_pred):\n",
    "    return np.abs(y - y_pred).mean()\n",
    "MAD = make_scorer(mad, greater_is_better=False)  # \n",
    "rcv = RidgeCV(alphas=np.linspace(0.01, 0.1, 100), store_cv_values=True, scoring=MAD)\n",
    "rcv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rcv.cv_values_.mean(0).argmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO 正则化\n",
    "---\n",
    "LASSO（ least absolute shrinkage and selection operator，最小绝对值收缩和选择算子）方法与岭回归和LARS（least angle regression，最小角回归）很类似。与岭回归类似，它也是通过增加惩罚函数来判断、消除特征间的共线性。与LARS相似的是它也可以用作参数选择，通常得出一个相关系数的稀疏向量。\n",
    "\n",
    "`Lasso` 类的实现使用了 coordinate descent （坐标下降算法）来拟合系数\n",
    "\n",
    "其最小化的目标函数是\n",
    "$$\\underset {\\theta} {min} ||X\\theta-y||^2 + \\alpha||\\theta||_1$$\n",
    "最小化残差平方和的另一种表达方式是：\n",
    "\n",
    "$$ RSS(\\theta),其中 {\\begin{Vmatrix} \\theta \\end{Vmatrix}}_1 \\lt \\theta $$\n",
    "这个约束会让数据稀疏。LASSO回归的约束创建了围绕原点的超立方体（相关系数是轴），也就意味着大多数点都在各个顶点上，那里相关系数为0。而岭回归创建的是超平面，因为其约束是L2范数，少一个约束，但是即使有限制相关系数也不会变成0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500个特征中只有5个是有用的\n",
    "X, y = make_regression(n_samples=200, n_features=500, n_informative=5, noise=5, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n",
       "      normalize=False, positive=False, precompute=False, random_state=None,\n",
       "      selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso()\n",
    "lasso.fit(X, y)  # 默认alpha 也是1. 设置为0时就变成普通线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(lasso.coef_ != 0)   # 其余系数都是0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-58-9c7c09cafaf7>:3: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lasso_0.fit(X, y)\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/coordinate_descent.py:473: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果使用线性回归, 就有500个系数\n",
    "lasso_0 = Lasso(alpha=0)\n",
    "lasso_0.fit(X, y)\n",
    "np.sum(lasso_0.coef_ != 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LASSO交叉验证**  \n",
    "操作上与岭回归的交叉验证差不多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LassoCV(alphas=None, copy_X=True, cv='warn', eps=0.001, fit_intercept=True,\n",
       "        max_iter=1000, n_alphas=100, n_jobs=None, normalize=False,\n",
       "        positive=False, precompute='auto', random_state=None,\n",
       "        selection='cyclic', tol=0.0001, verbose=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "alpha_list = np.logspace(-5, 5, 11)\n",
    "lasso_cv = LassoCV()\n",
    "lasso_cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8827526920440889"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_cv.alpha_  # 通过交叉验证得到最优的正则化系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(lasso_cv.coef_ != 0)  # 交叉验证之后的非零系数数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LASSO特征选择**\n",
    "---\n",
    "LASSO通常用来为其他方法所特征选择([L1-based feature selection](https://github.com/apachecn/sklearn-doc-zh/blob/master/docs/0.21.3/14.md#11341-%E5%9F%BA%E4%BA%8E-l1-%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E5%8F%96))。例如，你可能会用LASSO回归获取适当的特征变量，然后在其他算法中使用。\n",
    "\n",
    "要获取想要的特征，需要创建一个非零相关系数的列向量，然后再其他算法拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 11)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = lasso_cv.coef_ != 0\n",
    "X_new = X[:, mask]\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LARS (Least Angle Regression)\n",
    "---\n",
    "最小角回归 （LARS） 是对高维数据的回归算法， 由 Bradley Efron, Trevor Hastie, Iain Johnstone 和 Robert Tibshirani 开发完成。 LARS 和逐步回归很像。在每一步，它都寻找与响应最有关联的预测。当有很多预测有相同的关联时，它并不会继续利用相同的预测，而是在这些预测中找出应该等角的方向。\n",
    "\n",
    "LARS的优点:\n",
    "\n",
    "- 当 p(n_feature) >> n(n_sample)，该算法数值运算上非常有效。(例如当维度的数目远超点的个数)\n",
    "- 它在计算上和前向选择一样快，和普通最小二乘法有相同的运算复杂度。\n",
    "- 它产生了一个完整的分段线性的解决路径，在交叉验证或者其他相似的微调模型的方法上非常有用。\n",
    "- 如果两个变量对响应几乎有相等的联系，则它们的系数应该有相似的增长率。因此这个算法和我们直觉 上的判断一样，而且还更加稳定。\n",
    "- 它很容易修改并为其他估算器生成解，比如Lasso。\n",
    "\n",
    "LARS 的缺点:\n",
    "- 因为 LARS 是建立在循环拟合剩余变量上的，所以它对噪声非常敏感。\n",
    "\n",
    "[算法的具体描述](https://blog.csdn.net/u014664226/article/details/52240272/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=200, n_features=500, \n",
    "                       n_informative=10, noise=2, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们用了10个信息特征，因此我们还要为LARS设置10个非0的相关系数。我们事先可能不知道信息特征的准确数量，但是出于试验的目的是可行的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,\n",
       "     n_nonzero_coefs=10, normalize=True, positive=False, precompute='auto',\n",
       "     verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lars\n",
    "\n",
    "lars = Lars(n_nonzero_coefs=10)\n",
    "train_n = 100\n",
    "lars.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(lars.coef_ != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,\n",
       "     n_nonzero_coefs=12, normalize=True, positive=False, precompute='auto',\n",
       "     verbose=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lars_12 = Lars(n_nonzero_coefs=12)  # 10左右估计一个值\n",
    "lars_12.fit(X[:train_n], y[:train_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=9.508e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=9.161e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.910e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.801e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.668e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.599e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.535e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.334e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.110e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.936e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.918e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.839e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.838e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.764e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.580e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.732e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.559e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.552e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.479e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.431e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.401e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.397e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.312e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.306e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.273e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.220e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.219e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.163e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.162e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.011e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.007e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.930e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.902e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.780e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.758e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.744e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.742e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.730e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.653e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.643e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.609e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.605e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.530e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.443e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.403e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 9.246e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.391e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.388e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.330e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.320e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.308e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.295e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.278e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.271e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.266e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.229e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.206e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.198e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.170e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.165e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.155e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.133e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.132e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.121e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.148e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.144e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.105e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.918e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.875e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.868e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.806e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.767e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.762e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.689e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.643e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.603e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.599e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.598e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.560e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.509e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.496e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.488e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.473e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.465e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.449e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.415e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.380e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.325e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.271e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.263e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.254e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.247e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.237e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.237e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.228e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.225e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.174e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.173e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.151e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.129e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.103e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.096e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.089e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.071e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.026e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.991e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.978e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.976e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.972e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.966e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.933e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.928e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.903e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.888e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.848e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.793e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.781e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.748e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.722e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.634e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.626e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.572e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.543e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.518e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.510e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.486e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.451e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.428e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.408e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.377e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.352e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.289e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.280e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.267e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.264e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.251e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.244e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.222e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.220e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.214e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.211e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.184e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.143e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.125e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.070e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.066e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.055e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.013e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.007e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.992e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.953e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.953e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.943e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.942e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.895e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.886e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.850e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.848e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.806e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.804e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.790e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.765e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.718e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.718e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.705e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.697e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.687e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.674e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.661e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.601e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.598e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.597e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.576e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.570e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.568e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.568e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.550e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.542e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.519e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.493e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.468e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.421e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.386e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.317e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.314e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.295e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.271e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.261e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.251e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.247e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.227e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.226e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.213e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.212e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.210e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.178e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.174e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.154e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.144e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.122e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.116e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.108e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.104e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.073e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 9.884e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.063e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.049e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.035e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.035e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.027e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.026e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.024e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.015e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.996e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.990e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.946e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.940e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.933e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.899e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.890e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.865e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.862e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.844e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.810e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.808e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.792e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.755e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.741e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.737e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.734e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.732e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.725e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.720e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.706e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.706e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.686e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.674e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.668e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.647e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.626e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.614e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.592e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.589e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.572e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.539e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.502e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.486e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.481e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.454e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.451e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.448e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.445e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.434e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.427e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.407e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.363e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.348e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.312e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.275e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.253e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.240e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.224e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.216e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.203e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.197e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.178e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.177e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.175e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.170e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.159e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.159e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.130e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.063e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.053e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.044e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.020e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.010e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.002e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.997e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.995e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.993e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.971e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.963e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.950e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.928e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.928e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.898e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.891e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.875e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.868e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.860e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.817e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.814e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.796e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.774e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.751e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.749e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.739e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.692e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.673e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.665e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.622e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.621e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.602e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.583e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.575e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.555e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.555e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.532e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.510e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.471e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.429e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.413e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.391e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.378e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.374e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.349e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.303e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.296e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.259e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.217e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.178e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.144e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.125e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.121e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.117e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.115e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.103e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.084e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.083e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.071e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.056e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.051e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.033e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.003e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=9.877e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=9.863e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=9.506e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=9.412e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=9.285e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=9.284e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=9.186e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.833e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.779e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.711e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.638e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.435e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.370e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.283e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.272e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.257e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.232e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.215e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.159e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.021e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=7.939e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=7.313e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.933e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.913e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.764e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.354e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.238e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.187e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.865e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.806e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.795e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.677e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.651e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.366e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.140e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.021e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.841e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.719e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.707e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.626e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.621e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.546e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.309e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.212e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.210e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=4.077e-03, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=4.042e-03, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=3.686e-03, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=3.599e-03, with an active set of 102 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=3.340e-03, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=3.215e-03, with an active set of 102 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=3.189e-03, with an active set of 102 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,\n",
       "     n_nonzero_coefs=500, normalize=True, positive=False, precompute='auto',\n",
       "     verbose=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lars_500 = Lars()  # 默认所有系数非零\n",
    "lars_500.fit(X[:train_n], y[:train_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.156478326549475"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(y[:train_n]-lars_12.predict(X[:train_n]), 2).mean()  # 12 训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376.6855131789647"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(y[:train_n]-lars_500.predict(X[:train_n]), 2).mean()  # 500 训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.010395792572443"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(y[train_n:]-lars.predict(X[train_n:]), 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.26807154043368"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(y[train_n:]-lars_12.predict(X[train_n:]), 2).mean()  # 12 测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.91887311386876e+31"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(y[train_n:]-lars_500.predict(X[train_n:]), 2).mean()  # 500 测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集的误差明显高很多。高维数据集问题就在于此；通常面对大量的特征时，想找出一个对训练集拟合很好的模型并不难，但是拟合过度却是更大的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯回归\n",
    "---\n",
    "贝叶斯回归可以用于在预估阶段的参数正则化: 正则化参数的选择不是通过人为的选择，而是通过手动调节数据值来实现。\n",
    "上述过程可以通过引入 无信息先验 到模型中的超参数来完成。 在`岭回归`中使用的 $\\ell_{2}$ 正则项相当于在 w 为高斯先验条件，且此先验的精确度为 $\\lambda^{-1}$ 时，求最大后验估计。在这里，我们没有手工调参数 lambda ，而是让他作为一个变量，通过数据中估计得到。\n",
    "\n",
    "为了得到一个全概率模型，输出y也被认为是关于 X w 的高斯分布。\n",
    "$$p(y|X,w,\\alpha) = \\mathcal{N}(y|X w,\\alpha)$$\n",
    "\n",
    "**贝叶斯岭回归**\n",
    "\n",
    "`BayesianRidge `利用概率模型估算了上述的回归问题，其先验参数$\\theta$是由以下球面高斯公式得出的\n",
    "$$p(\\theta|\\lambda) =\\mathcal{N}(\\theta|0,\\lambda^{-1}{I_{p}})$$\n",
    "\n",
    "> $I_{p}$为指示函数\n",
    "\n",
    "先验参数 $\\alpha $和 $\\lambda$ 一般是服从[Gamma](https://blog.csdn.net/chenshulong/article/details/79027103)分布 ，这个分布与高斯成共轭先验关系。 得到的模型一般称为`贝叶斯岭回归`，并且这个与传统的 Ridge 非常相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=2, noise=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
       "              fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
       "              normalize=False, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "br = BayesianRidge()\n",
    "br.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有两组相关系数，分别是alpha_1 / alpha_2和lambda_1 / lambda_2。其中，alpha_*是先验概率分布的$\\alpha$超参数，lambda_*是先验概率分布的$\\lambda$超参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0317, -0.544 , -0.6689, 85.7597, -0.5359,  0.1003, -0.7926,\n",
       "       93.5907, -0.4476,  1.0632])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.033 , -0.5474, -0.6639, 85.7153, -0.5356,  0.1004, -0.7896,\n",
       "       93.5411, -0.4445,  1.0605])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们来调整超参数，注意观察相关系数的变化\n",
    "br_alphas = BayesianRidge(alpha_1=10, lambda_1=10)\n",
    "br_alphas.fit(X, y)\n",
    "br_alphas.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 内核岭回归\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
