{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 普通最小二乘法回归\n",
    "---\n",
    "最标准的线性模型是“普通最小二乘回归”，通常简称为“线性回归”。 它没有对coef_施加任何额外限制，因此当特征数量很大时，它会变得行为异常，并且模型会过拟合。\n",
    "$$\\underset{\\theta}{min} {|| X\\theta - y||_2}^2$$\n",
    "通过回归方程求导得到的最佳系数$\\hat\\theta = (X^TX)^{-1}X^Ty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.set_printoptions(precision=4, suppress=True, threshold=16)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**make_regression**\n",
    "```\n",
    "Generate a random regression problem.\n",
    "    n_samples=100,\n",
    "    n_features=100,\n",
    "    n_informative=10,\n",
    "    n_targets=1,\n",
    "    bias=0.0,\n",
    "    effective_rank=None,\n",
    "    tail_strength=0.5,\n",
    "    noise=0.0,\n",
    "    shuffle=True,\n",
    "    coef=False,\n",
    "    random_state=None,\n",
    "```\n",
    "```\n",
    "Returns\n",
    "-------\n",
    "X : array of shape [n_samples, n_features]\n",
    "    The input samples.\n",
    "\n",
    "y : array of shape [n_samples] or [n_samples, n_targets]\n",
    "    The output values.\n",
    "\n",
    "coef : array of shape [n_features] or [n_features, n_targets], optional\n",
    "    The coefficient of the underlying linear model. It is returned only if\n",
    "    coef is True.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(60, 30)\n(60,)\n"
    }
   ],
   "source": [
    "# 只有10个是有用的特征 , 添加了噪声\n",
    "X, y, true_coef = make_regression(n_samples=200, n_features=30, n_informative=10, noise=100, coef=True, random_state=5)  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5, train_size=60, test_size=140)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 0.    ,  0.    ,  2.1223, ...,  0.    , 80.8129,  0.    ])"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "true_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R2 score, 用来计算[可决系数](https://baike.baidu.com/item/%E5%8F%AF%E5%86%B3%E7%B3%BB%E6%95%B0)(the coefficient of determination)**\n",
    "$$R^2(y, \\hat y) = 1 - \\frac {\\sum_{i=1}^{n\\_sample}(y_i - \\hat y_i)^2}{\\sum_{i=1}^{n\\_sample}(y_i - \\bar y)^2}$$\n",
    "其中$y_i$表示第i个样本的真实值, $\\hat y_i$ 为预测值,   \n",
    "$\\bar y = \\frac 1 {n\\_sample}\\sum_{i=1}^{n\\_sample} y_i$为实际值的均值\n",
    "R2 score的值为1.0时最佳, 也可能为负数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "R^2 on training set: 0.878011\nR^2 on test set: 0.216332\n"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"R^2 on training set: %f\" % lr.score(X_train, y_train))\n",
    "print(\"R^2 on test set: %f\" % lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5985284495875146"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# 整体集合的R2 score\n",
    "r2_score(np.dot(X, true_coef), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x7f82bf6e5690>"
     },
     "metadata": {},
     "execution_count": 6
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 720x360 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"302.878125pt\" version=\"1.1\" viewBox=\"0 0 600.504688 302.878125\" width=\"600.504688pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 302.878125 \nL 600.504688 302.878125 \nL 600.504688 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 35.304688 279 \nL 593.304688 279 \nL 593.304688 7.2 \nL 35.304688 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"me80233b758\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.668324\" xlink:href=\"#me80233b758\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(57.487074 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"148.129139\" xlink:href=\"#me80233b758\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(144.947889 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"235.589954\" xlink:href=\"#me80233b758\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(229.227454 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"323.050769\" xlink:href=\"#me80233b758\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(316.688269 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"410.511584\" xlink:href=\"#me80233b758\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(404.149084 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"497.972399\" xlink:href=\"#me80233b758\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(491.609899 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"585.433214\" xlink:href=\"#me80233b758\" y=\"279\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 30 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(579.070714 293.598437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"md0ebcbfad2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#md0ebcbfad2\" y=\"268.446891\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- −75 -->\n      <defs>\n       <path d=\"M 10.59375 35.5 \nL 73.1875 35.5 \nL 73.1875 27.203125 \nL 10.59375 27.203125 \nz\n\" id=\"DejaVuSans-8722\"/>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(7.2 272.246109)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#md0ebcbfad2\" y=\"234.70635\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- −50 -->\n      <g transform=\"translate(7.2 238.505569)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#md0ebcbfad2\" y=\"200.965809\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- −25 -->\n      <g transform=\"translate(7.2 204.765028)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-8722\"/>\n       <use x=\"83.789062\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"147.412109\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#md0ebcbfad2\" y=\"167.225268\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0 -->\n      <g transform=\"translate(21.942187 171.024487)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#md0ebcbfad2\" y=\"133.484728\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(15.579687 137.283946)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#md0ebcbfad2\" y=\"99.744187\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 50 -->\n      <g transform=\"translate(15.579687 103.543406)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#md0ebcbfad2\" y=\"66.003646\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 75 -->\n      <g transform=\"translate(15.579687 69.802865)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"35.304688\" xlink:href=\"#md0ebcbfad2\" y=\"32.263105\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 100 -->\n      <g transform=\"translate(9.217187 36.062324)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_16\">\n    <defs>\n     <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m69c4aae24a\" style=\"stroke:#1f77b4;\"/>\n    </defs>\n    <g clip-path=\"url(#p844f7c2702)\">\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"60.668324\" xlink:href=\"#m69c4aae24a\" y=\"48.219201\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"78.160487\" xlink:href=\"#m69c4aae24a\" y=\"58.158412\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"95.65265\" xlink:href=\"#m69c4aae24a\" y=\"71.594252\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"113.144813\" xlink:href=\"#m69c4aae24a\" y=\"80.081216\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"130.636976\" xlink:href=\"#m69c4aae24a\" y=\"100.749549\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"148.129139\" xlink:href=\"#m69c4aae24a\" y=\"120.947259\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"165.621302\" xlink:href=\"#m69c4aae24a\" y=\"123.645203\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"183.113465\" xlink:href=\"#m69c4aae24a\" y=\"146.878596\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"200.605628\" xlink:href=\"#m69c4aae24a\" y=\"155.868367\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"218.097791\" xlink:href=\"#m69c4aae24a\" y=\"164.361005\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"235.589954\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"253.082117\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"270.57428\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"288.066443\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"305.558606\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"323.050769\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"340.542932\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"358.035095\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"375.527258\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"393.019421\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"410.511584\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"428.003747\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"445.49591\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"462.988073\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"480.480236\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"497.972399\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"515.464562\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"532.956725\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"550.448888\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"567.941051\" xlink:href=\"#m69c4aae24a\" y=\"167.225268\"/>\n    </g>\n   </g>\n   <g id=\"line2d_17\">\n    <defs>\n     <path d=\"M -3 0 \nL 3 0 \nM 0 3 \nL 0 -3 \n\" id=\"m05d62969f5\" style=\"stroke:#ff7f0e;\"/>\n    </defs>\n    <g clip-path=\"url(#p844f7c2702)\">\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"60.668324\" xlink:href=\"#m05d62969f5\" y=\"55.838358\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"78.160487\" xlink:href=\"#m05d62969f5\" y=\"95.420895\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"95.65265\" xlink:href=\"#m05d62969f5\" y=\"19.554545\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"113.144813\" xlink:href=\"#m05d62969f5\" y=\"112.02917\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"130.636976\" xlink:href=\"#m05d62969f5\" y=\"75.13074\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"148.129139\" xlink:href=\"#m05d62969f5\" y=\"147.580859\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"165.621302\" xlink:href=\"#m05d62969f5\" y=\"110.208789\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"183.113465\" xlink:href=\"#m05d62969f5\" y=\"97.20461\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"200.605628\" xlink:href=\"#m05d62969f5\" y=\"179.809301\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"218.097791\" xlink:href=\"#m05d62969f5\" y=\"212.165602\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"235.589954\" xlink:href=\"#m05d62969f5\" y=\"148.056995\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"253.082117\" xlink:href=\"#m05d62969f5\" y=\"169.965227\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"270.57428\" xlink:href=\"#m05d62969f5\" y=\"183.230403\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"288.066443\" xlink:href=\"#m05d62969f5\" y=\"151.152238\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"305.558606\" xlink:href=\"#m05d62969f5\" y=\"157.645805\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"323.050769\" xlink:href=\"#m05d62969f5\" y=\"139.600931\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"340.542932\" xlink:href=\"#m05d62969f5\" y=\"169.280697\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"358.035095\" xlink:href=\"#m05d62969f5\" y=\"138.030338\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"375.527258\" xlink:href=\"#m05d62969f5\" y=\"266.645455\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"393.019421\" xlink:href=\"#m05d62969f5\" y=\"134.517707\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"410.511584\" xlink:href=\"#m05d62969f5\" y=\"139.152418\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"428.003747\" xlink:href=\"#m05d62969f5\" y=\"195.648335\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"445.49591\" xlink:href=\"#m05d62969f5\" y=\"148.127047\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"462.988073\" xlink:href=\"#m05d62969f5\" y=\"184.293711\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"480.480236\" xlink:href=\"#m05d62969f5\" y=\"176.321383\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"497.972399\" xlink:href=\"#m05d62969f5\" y=\"162.659011\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"515.464562\" xlink:href=\"#m05d62969f5\" y=\"160.244354\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"532.956725\" xlink:href=\"#m05d62969f5\" y=\"190.834832\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"550.448888\" xlink:href=\"#m05d62969f5\" y=\"148.208606\"/>\n     <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"567.941051\" xlink:href=\"#m05d62969f5\" y=\"199.661943\"/>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 35.304688 279 \nL 35.304688 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 593.304688 279 \nL 593.304688 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 35.304688 279 \nL 593.304688 279 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 35.304688 7.2 \nL 593.304688 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 470.317187 44.55625 \nL 586.304688 44.55625 \nQ 588.304688 44.55625 588.304688 42.55625 \nL 588.304688 14.2 \nQ 588.304688 12.2 586.304688 12.2 \nL 470.317187 12.2 \nQ 468.317187 12.2 468.317187 14.2 \nL 468.317187 42.55625 \nQ 468.317187 44.55625 470.317187 44.55625 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_18\"/>\n    <g id=\"line2d_19\">\n     <g>\n      <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"482.317187\" xlink:href=\"#m69c4aae24a\" y=\"20.298437\"/>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- true -->\n     <defs>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     </defs>\n     <g transform=\"translate(500.317187 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"80.322266\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"143.701172\" xlink:href=\"#DejaVuSans-101\"/>\n     </g>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"line2d_21\">\n     <g>\n      <use style=\"fill:#ff7f0e;stroke:#ff7f0e;\" x=\"482.317187\" xlink:href=\"#m05d62969f5\" y=\"34.976562\"/>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- linear regression -->\n     <defs>\n      <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n     </defs>\n     <g transform=\"translate(500.317187 38.476562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"55.566406\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"118.945312\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"180.46875\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"241.748047\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"282.861328\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"314.648438\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"355.730469\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"417.253906\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"480.730469\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"521.8125\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"583.335938\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"635.435547\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"687.535156\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"715.318359\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"776.5\" xlink:href=\"#DejaVuSans-110\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p844f7c2702\">\n   <rect height=\"271.8\" width=\"558\" x=\"35.304688\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "# 系数从大到小排序 画图比较\n",
    "coefficient_sorting = np.argsort(true_coef)[::-1]\n",
    "plt.plot(true_coef[coefficient_sorting], \"o\", label=\"true\")\n",
    "plt.plot(lr.coef_[coefficient_sorting], \"+\", label=\"linear regression\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**learning curve 学习曲线**\n",
    "---\n",
    "A learning curve shows the validation and training score of an estimator for varying numbers of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 16,  52,  88, 124, 160])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, valid_scores = learning_curve(\n",
    "    LinearRegression(), X, y, train_sizes=np.linspace(.1, 1, 5), cv=5)\n",
    "train_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mSignature:\u001b[0m\n\u001b[0mlearning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtrain_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.1\u001b[0m  \u001b[0;34m,\u001b[0m \u001b[0;36m0.325\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.55\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.775\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m   \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mexploit_incremental_learning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0merror_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m\nLearning curve.\n\nDetermines cross-validated training and test scores for different training\nset sizes.\n\nA cross-validation generator splits the whole dataset k times in training\nand test data. Subsets of the training set with varying sizes will be used\nto train the estimator and a score for each training subset size and the\ntest set will be computed. Afterwards, the scores will be averaged over\nall k runs for each training subset size.\n\nRead more in the :ref:`User Guide <learning_curve>`.\n\nParameters\n----------\nestimator : object type that implements the \"fit\" and \"predict\" methods\n    An object of that type which is cloned for each validation.\n\nX : array-like, shape (n_samples, n_features)\n    Training vector, where n_samples is the number of samples and\n    n_features is the number of features.\n\ny : array-like, shape (n_samples) or (n_samples, n_features), optional\n    Target relative to X for classification or regression;\n    None for unsupervised learning.\n\ngroups : array-like, with shape (n_samples,), optional\n    Group labels for the samples used while splitting the dataset into\n    train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n    instance (e.g., :class:`GroupKFold`).\n\ntrain_sizes : array-like, shape (n_ticks,), dtype float or int\n    Relative or absolute numbers of training examples that will be used to\n    generate the learning curve. If the dtype is float, it is regarded as a\n    fraction of the maximum size of the training set (that is determined\n    by the selected validation method), i.e. it has to be within (0, 1].\n    Otherwise it is interpreted as absolute sizes of the training sets.\n    Note that for classification the number of samples usually have to\n    be big enough to contain at least one sample from each class.\n    (default: np.linspace(0.1, 1.0, 5))\n\ncv : int, cross-validation generator or an iterable, optional\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the default 5-fold cross validation,\n    - integer, to specify the number of folds in a `(Stratified)KFold`,\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if the estimator is a classifier and ``y`` is\n    either binary or multiclass, :class:`StratifiedKFold` is used. In all\n    other cases, :class:`KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\n    .. versionchanged:: 0.22\n        ``cv`` default value if None changed from 3-fold to 5-fold.\n\nscoring : string, callable or None, optional, default: None\n    A string (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``.\n\nexploit_incremental_learning : boolean, optional, default: False\n    If the estimator supports incremental learning, this will be\n    used to speed up fitting for different training set sizes.\n\nn_jobs : int or None, optional (default=None)\n    Number of jobs to run in parallel.\n    ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n    ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n    for more details.\n\npre_dispatch : integer or string, optional\n    Number of predispatched jobs for parallel execution (default is\n    all). The option can reduce the allocated memory. The string can\n    be an expression like '2*n_jobs'.\n\nverbose : integer, optional\n    Controls the verbosity: the higher, the more messages.\n\nshuffle : boolean, optional\n    Whether to shuffle training data before taking prefixes of it\n    based on``train_sizes``.\n\nrandom_state : int, RandomState instance or None, optional (default=None)\n    If int, random_state is the seed used by the random number generator;\n    If RandomState instance, random_state is the random number generator;\n    If None, the random number generator is the RandomState instance used\n    by `np.random`. Used when ``shuffle`` is True.\n\nerror_score : 'raise' or numeric\n    Value to assign to the score if an error occurs in estimator fitting.\n    If set to 'raise', the error is raised.\n    If a numeric value is given, FitFailedWarning is raised. This parameter\n    does not affect the refit step, which will always raise the error.\n\nreturn_times : boolean, optional (default: False)\n    Whether to return the fit and score times.\n\nReturns\n-------\ntrain_sizes_abs : array, shape (n_unique_ticks,), dtype int\n    Numbers of training examples that has been used to generate the\n    learning curve. Note that the number of ticks might be less\n    than n_ticks because duplicate entries will be removed.\n\ntrain_scores : array, shape (n_ticks, n_cv_folds)\n    Scores on training sets.\n\ntest_scores : array, shape (n_ticks, n_cv_folds)\n    Scores on test set.\n\nfit_times : array, shape (n_ticks, n_cv_folds)\n    Times spent for fitting in seconds. Only present if ``return_times``\n    is True.\n\nscore_times : array, shape (n_ticks, n_cv_folds)\n    Times spent for scoring in seconds. Only present if ``return_times``\n    is True.\n\nNotes\n-----\nSee :ref:`examples/model_selection/plot_learning_curve.py\n<sphx_glr_auto_examples_model_selection_plot_learning_curve.py>`\n\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\n\u001b[0;31mType:\u001b[0m      function\n"
    }
   ],
   "source": [
    "learning_curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1.    , 1.    , 1.    , 1.    , 1.    ],\n       [0.8361, 0.8313, 0.8395, 0.8395, 0.8395],\n       [0.8266, 0.7631, 0.7211, 0.7461, 0.7461],\n       [0.8401, 0.7952, 0.7754, 0.7855, 0.7735],\n       [0.8028, 0.7629, 0.7478, 0.7579, 0.7798]])"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "train_scores  # (n_ticks, n_cv_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4851,  0.2807,  0.2403, -0.1845,  0.4229],\n",
       "       [ 0.0926,  0.2826,  0.4763, -0.0515,  0.2072],\n",
       "       [ 0.1387,  0.6938,  0.7296,  0.5487,  0.5216],\n",
       "       [ 0.17  ,  0.6873,  0.7396,  0.6146,  0.6083],\n",
       "       [ 0.288 ,  0.7235,  0.749 ,  0.6684,  0.6191]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(est, X, y):\n",
    "    plt.figure()\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        LinearRegression(), X, y, train_sizes=np.linspace(.1, 1, 20), cv=5)\n",
    "    estimator_name = est.__class__.__name__\n",
    "    # 训练集的 训练集大小-socre 分数 曲线\n",
    "    line = plt.plot(train_sizes, train_scores.mean(axis=1), '--', label=f\"train scores {estimator_name}\")\n",
    "    plt.plot(train_sizes, test_scores.mean(axis=1), '-', label=f\"test scores {estimator_name}\")\n",
    "    plt.xlabel(\"Training set size\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.ylim(-0.1, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curve(LinearRegression(), X, y)\n",
    "# 不同训练集大小 训练出来的 结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**普通最小二乘法的复杂度**\n",
    "\n",
    "该方法使用 X 的奇异值分解来计算最小二乘解。如果 X 是一个形状为 (n_samples, n_features)的矩阵，设$$n_{samples} \\geq n_{features}$$, 则该方法的复杂度为$$O(n_{samples} n_{fearures}^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 岭回归（L2 惩罚）\n",
    "---\n",
    "岭估计器是普通LinearRegression的简单正则化（称为 l2 惩罚）。 特别是，它具有的优点是，在计算上不比普通的最小二乘估计更昂贵。\n",
    "$$\\underset {\\theta}{min} ||X\\theta - y||_2^2 + \\alpha ||\\theta||_2^2$$\n",
    "其中， $\\alpha \\geq 0$ 是控制系数收缩量的复杂性参数： $\\alpha$ 的值越大，收缩量越大，模型对共线性的鲁棒性也更强。\n",
    "\n",
    "最优的$\\hat \\theta = (X^TX+\\alpha I)^{-1}X^Ty$, 它是一个关于$\\alpha$的函数.  \n",
    "**岭回归的复杂度**\n",
    "这种方法与 `普通最小二乘法` 的复杂度是相同的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们加载一个不满秩（low effective rank）数据集来比较岭回归和线性回归。秩是矩阵线性无关组的数量，满秩是指一个$m \\times n$矩阵中行向量或列向量中现行无关组的数量等于$min(m,n)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-0.009 , -0.0097, -0.0006],\n       [-0.0036,  0.021 ,  0.068 ],\n       [ 0.0023, -0.0132, -0.0079],\n       ...,\n       [-0.0102,  0.034 , -0.0037],\n       [-0.02  , -0.0115,  0.0031],\n       [-0.0195,  0.0451,  0.0168]])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# 建一个有3个自变量的数据集，但是其秩为2，因此3个自变量中有两个自变量存在相关性\n",
    "X, y = make_regression(n_samples=2000, n_features=3, effective_rank=2, noise=10)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression(lr, X, y):\n",
    "    n_sample, n_feature = X.shape\n",
    "    n_bootstraps = 1000 # 1000次\n",
    "    coefs = np.zeros((n_bootstraps, n_feature))\n",
    "    scores = np.zeros((n_bootstraps, 2))\n",
    "    \n",
    "    # 重复 n_bootstraps\n",
    "    for i in range(n_bootstraps):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25)\n",
    "        lr.fit(X_train, y_train)\n",
    "        scores[i] = (lr.score(X_train, y_train), lr.score(X_test, y_test))\n",
    "        coefs[i] = lr.coef_\n",
    "    f, axes = plt.subplots(nrows=n_feature, sharex=True, sharey=True, figsize=(10, 8))\n",
    "    # 查看这n_bootstraps次 3个特征系数的分布\n",
    "    for i, ax in enumerate(axes):\n",
    "        # 频率分布直方图\n",
    "        ax.hist(coefs[:, i], alpha=.5)\n",
    "        ax.set_title(\"Coef {}\".format(i))\n",
    "    plt.show()\n",
    "    return coefs, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 720x576 with 3 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"481.07625pt\" version=\"1.1\" viewBox=\"0 0 598.4875 481.07625\" width=\"598.4875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 481.07625 \nL 598.4875 481.07625 \nL 598.4875 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 150.224007 \nL 591.2875 150.224007 \nL 591.2875 22.318125 \nL 33.2875 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#p84e8ff1408)\" d=\"M 229.877039 150.224007 \nL 263.481721 150.224007 \nL 263.481721 147.111942 \nL 229.877039 147.111942 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#p84e8ff1408)\" d=\"M 263.481721 150.224007 \nL 297.086404 150.224007 \nL 297.086404 143.999877 \nL 263.481721 143.999877 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#p84e8ff1408)\" d=\"M 297.086404 150.224007 \nL 330.691086 150.224007 \nL 330.691086 120.881678 \nL 297.086404 120.881678 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#p84e8ff1408)\" d=\"M 330.691086 150.224007 \nL 364.295769 150.224007 \nL 364.295769 99.097221 \nL 330.691086 99.097221 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#p84e8ff1408)\" d=\"M 364.295769 150.224007 \nL 397.900451 150.224007 \nL 397.900451 62.641599 \nL 364.295769 62.641599 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#p84e8ff1408)\" d=\"M 397.900451 150.224007 \nL 431.505134 150.224007 \nL 431.505134 57.751211 \nL 397.900451 57.751211 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#p84e8ff1408)\" d=\"M 431.505134 150.224007 \nL 465.109816 150.224007 \nL 465.109816 67.087407 \nL 431.505134 67.087407 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#p84e8ff1408)\" d=\"M 465.109816 150.224007 \nL 498.714499 150.224007 \nL 498.714499 93.762252 \nL 465.109816 93.762252 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#p84e8ff1408)\" d=\"M 498.714499 150.224007 \nL 532.319181 150.224007 \nL 532.319181 126.661227 \nL 498.714499 126.661227 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#p84e8ff1408)\" d=\"M 532.319181 150.224007 \nL 565.923864 150.224007 \nL 565.923864 138.664908 \nL 532.319181 138.664908 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m60d6353fa2\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.362782\" xlink:href=\"#m60d6353fa2\" y=\"150.224007\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"123.089009\" xlink:href=\"#m60d6353fa2\" y=\"150.224007\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"192.815236\" xlink:href=\"#m60d6353fa2\" y=\"150.224007\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"262.541463\" xlink:href=\"#m60d6353fa2\" y=\"150.224007\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"332.26769\" xlink:href=\"#m60d6353fa2\" y=\"150.224007\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"401.993917\" xlink:href=\"#m60d6353fa2\" y=\"150.224007\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"471.720144\" xlink:href=\"#m60d6353fa2\" y=\"150.224007\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"541.446371\" xlink:href=\"#m60d6353fa2\" y=\"150.224007\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m265bfa6166\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"150.224007\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(19.925 154.023226)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"127.99497\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 50 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(13.5625 131.794189)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"105.765932\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(7.2 109.565151)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"83.536895\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 150 -->\n      <g transform=\"translate(7.2 87.336113)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"61.307857\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 200 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(7.2 65.107076)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"39.078819\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 250 -->\n      <g transform=\"translate(7.2 42.878038)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 33.2875 150.224007 \nL 33.2875 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 591.2875 150.224007 \nL 591.2875 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 33.2875 150.224007 \nL 591.2875 150.224007 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 33.2875 22.318125 \nL 591.2875 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_7\">\n    <!-- Coef 0 -->\n    <defs>\n     <path d=\"M 64.40625 67.28125 \nL 64.40625 56.890625 \nQ 59.421875 61.53125 53.78125 63.8125 \nQ 48.140625 66.109375 41.796875 66.109375 \nQ 29.296875 66.109375 22.65625 58.46875 \nQ 16.015625 50.828125 16.015625 36.375 \nQ 16.015625 21.96875 22.65625 14.328125 \nQ 29.296875 6.6875 41.796875 6.6875 \nQ 48.140625 6.6875 53.78125 8.984375 \nQ 59.421875 11.28125 64.40625 15.921875 \nL 64.40625 5.609375 \nQ 59.234375 2.09375 53.4375 0.328125 \nQ 47.65625 -1.421875 41.21875 -1.421875 \nQ 24.65625 -1.421875 15.125 8.703125 \nQ 5.609375 18.84375 5.609375 36.375 \nQ 5.609375 53.953125 15.125 64.078125 \nQ 24.65625 74.21875 41.21875 74.21875 \nQ 47.75 74.21875 53.53125 72.484375 \nQ 59.328125 70.75 64.40625 67.28125 \nz\n\" id=\"DejaVuSans-67\"/>\n     <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n     <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n     <path id=\"DejaVuSans-32\"/>\n    </defs>\n    <g transform=\"translate(292.898125 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-67\"/>\n     <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"131.005859\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"192.529297\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"227.734375\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"259.521484\" xlink:href=\"#DejaVuSans-48\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_17\">\n    <path d=\"M 33.2875 303.711066 \nL 591.2875 303.711066 \nL 591.2875 175.805184 \nL 33.2875 175.805184 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#p22b6bd089e)\" d=\"M 58.651136 303.711066 \nL 87.662565 303.711066 \nL 87.662565 302.377324 \nL 58.651136 302.377324 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#p22b6bd089e)\" d=\"M 87.662565 303.711066 \nL 116.673993 303.711066 \nL 116.673993 298.376097 \nL 87.662565 298.376097 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#p22b6bd089e)\" d=\"M 116.673993 303.711066 \nL 145.685421 303.711066 \nL 145.685421 279.703706 \nL 116.673993 279.703706 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#p22b6bd089e)\" d=\"M 145.685421 303.711066 \nL 174.69685 303.711066 \nL 174.69685 242.358922 \nL 145.685421 242.358922 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#p22b6bd089e)\" d=\"M 174.69685 303.711066 \nL 203.708278 303.711066 \nL 203.708278 203.680397 \nL 174.69685 203.680397 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#p22b6bd089e)\" d=\"M 203.708278 303.711066 \nL 232.719706 303.711066 \nL 232.719706 204.569558 \nL 203.708278 204.569558 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#p22b6bd089e)\" d=\"M 232.719706 303.711066 \nL 261.731134 303.711066 \nL 261.731134 217.01782 \nL 232.719706 217.01782 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#p22b6bd089e)\" d=\"M 261.731134 303.711066 \nL 290.742563 303.711066 \nL 290.742563 261.920475 \nL 261.731134 261.920475 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#p22b6bd089e)\" d=\"M 290.742563 303.711066 \nL 319.753991 303.711066 \nL 319.753991 283.704932 \nL 290.742563 283.704932 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#p22b6bd089e)\" d=\"M 319.753991 303.711066 \nL 348.765419 303.711066 \nL 348.765419 298.820678 \nL 319.753991 298.820678 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_9\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.362782\" xlink:href=\"#m60d6353fa2\" y=\"303.711066\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"123.089009\" xlink:href=\"#m60d6353fa2\" y=\"303.711066\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"192.815236\" xlink:href=\"#m60d6353fa2\" y=\"303.711066\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"262.541463\" xlink:href=\"#m60d6353fa2\" y=\"303.711066\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"332.26769\" xlink:href=\"#m60d6353fa2\" y=\"303.711066\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"401.993917\" xlink:href=\"#m60d6353fa2\" y=\"303.711066\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"471.720144\" xlink:href=\"#m60d6353fa2\" y=\"303.711066\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"541.446371\" xlink:href=\"#m60d6353fa2\" y=\"303.711066\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_7\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"303.711066\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 307.510285)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"281.482029\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 285.281247)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"259.252991\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 263.05221)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"237.023953\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 150 -->\n      <g transform=\"translate(7.2 240.823172)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"214.794916\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 200 -->\n      <g transform=\"translate(7.2 218.594135)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"192.565878\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 250 -->\n      <g transform=\"translate(7.2 196.365097)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_28\">\n    <path d=\"M 33.2875 303.711066 \nL 33.2875 175.805184 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path d=\"M 591.2875 303.711066 \nL 591.2875 175.805184 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path d=\"M 33.2875 303.711066 \nL 591.2875 303.711066 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path d=\"M 33.2875 175.805184 \nL 591.2875 175.805184 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_14\">\n    <!-- Coef 1 -->\n    <g transform=\"translate(292.898125 169.805184)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-67\"/>\n     <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"131.005859\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"192.529297\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"227.734375\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"259.521484\" xlink:href=\"#DejaVuSans-49\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g id=\"patch_32\">\n    <path d=\"M 33.2875 457.198125 \nL 591.2875 457.198125 \nL 591.2875 329.292243 \nL 33.2875 329.292243 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_33\">\n    <path clip-path=\"url(#pb978dcaae2)\" d=\"M 140.188211 457.198125 \nL 170.816226 457.198125 \nL 170.816226 453.641479 \nL 140.188211 453.641479 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path clip-path=\"url(#pb978dcaae2)\" d=\"M 170.816226 457.198125 \nL 201.444241 457.198125 \nL 201.444241 438.081153 \nL 170.816226 438.081153 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path clip-path=\"url(#pb978dcaae2)\" d=\"M 201.444241 457.198125 \nL 232.072257 457.198125 \nL 232.072257 395.4014 \nL 201.444241 395.4014 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path clip-path=\"url(#pb978dcaae2)\" d=\"M 232.072257 457.198125 \nL 262.700272 457.198125 \nL 262.700272 354.944552 \nL 232.072257 354.944552 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_37\">\n    <path clip-path=\"url(#pb978dcaae2)\" d=\"M 262.700272 457.198125 \nL 293.328287 457.198125 \nL 293.328287 335.382999 \nL 262.700272 335.382999 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_38\">\n    <path clip-path=\"url(#pb978dcaae2)\" d=\"M 293.328287 457.198125 \nL 323.956302 457.198125 \nL 323.956302 382.953139 \nL 293.328287 382.953139 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_39\">\n    <path clip-path=\"url(#pb978dcaae2)\" d=\"M 323.956302 457.198125 \nL 354.584318 457.198125 \nL 354.584318 410.072565 \nL 323.956302 410.072565 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_40\">\n    <path clip-path=\"url(#pb978dcaae2)\" d=\"M 354.584318 457.198125 \nL 385.212333 457.198125 \nL 385.212333 448.751091 \nL 354.584318 448.751091 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_41\">\n    <path clip-path=\"url(#pb978dcaae2)\" d=\"M 385.212333 457.198125 \nL 415.840348 457.198125 \nL 415.840348 451.863156 \nL 385.212333 451.863156 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_42\">\n    <path clip-path=\"url(#pb978dcaae2)\" d=\"M 415.840348 457.198125 \nL 446.468363 457.198125 \nL 446.468363 456.308963 \nL 415.840348 456.308963 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"matplotlib.axis_5\">\n    <g id=\"xtick_17\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"53.362782\" xlink:href=\"#m60d6353fa2\" y=\"457.198125\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0 -->\n      <g transform=\"translate(50.181532 471.796562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"123.089009\" xlink:href=\"#m60d6353fa2\" y=\"457.198125\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 10 -->\n      <g transform=\"translate(116.726509 471.796562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_19\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"192.815236\" xlink:href=\"#m60d6353fa2\" y=\"457.198125\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 20 -->\n      <g transform=\"translate(186.452736 471.796562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_20\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"262.541463\" xlink:href=\"#m60d6353fa2\" y=\"457.198125\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 30 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(256.178963 471.796562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_21\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"332.26769\" xlink:href=\"#m60d6353fa2\" y=\"457.198125\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 40 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(325.90519 471.796562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_22\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"401.993917\" xlink:href=\"#m60d6353fa2\" y=\"457.198125\"/>\n      </g>\n     </g>\n     <g id=\"text_20\">\n      <!-- 50 -->\n      <g transform=\"translate(395.631417 471.796562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_23\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"471.720144\" xlink:href=\"#m60d6353fa2\" y=\"457.198125\"/>\n      </g>\n     </g>\n     <g id=\"text_21\">\n      <!-- 60 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(465.357644 471.796562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_24\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"541.446371\" xlink:href=\"#m60d6353fa2\" y=\"457.198125\"/>\n      </g>\n     </g>\n     <g id=\"text_22\">\n      <!-- 70 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(535.083871 471.796562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_6\">\n    <g id=\"ytick_13\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"457.198125\"/>\n      </g>\n     </g>\n     <g id=\"text_23\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 460.997344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_38\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"434.969087\"/>\n      </g>\n     </g>\n     <g id=\"text_24\">\n      <!-- 50 -->\n      <g transform=\"translate(13.5625 438.768306)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_39\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"412.74005\"/>\n      </g>\n     </g>\n     <g id=\"text_25\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 416.539269)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_40\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"390.511012\"/>\n      </g>\n     </g>\n     <g id=\"text_26\">\n      <!-- 150 -->\n      <g transform=\"translate(7.2 394.310231)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_41\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"368.281975\"/>\n      </g>\n     </g>\n     <g id=\"text_27\">\n      <!-- 200 -->\n      <g transform=\"translate(7.2 372.081193)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_42\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#m265bfa6166\" y=\"346.052937\"/>\n      </g>\n     </g>\n     <g id=\"text_28\">\n      <!-- 250 -->\n      <g transform=\"translate(7.2 349.852156)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_43\">\n    <path d=\"M 33.2875 457.198125 \nL 33.2875 329.292243 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_44\">\n    <path d=\"M 591.2875 457.198125 \nL 591.2875 329.292243 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_45\">\n    <path d=\"M 33.2875 457.198125 \nL 591.2875 457.198125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_46\">\n    <path d=\"M 33.2875 329.292243 \nL 591.2875 329.292243 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_29\">\n    <!-- Coef 2 -->\n    <g transform=\"translate(292.898125 323.292243)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-67\"/>\n     <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"131.005859\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"192.529297\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"227.734375\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"259.521484\" xlink:href=\"#DejaVuSans-50\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p84e8ff1408\">\n   <rect height=\"127.905882\" width=\"558\" x=\"33.2875\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p22b6bd089e\">\n   <rect height=\"127.905882\" width=\"558\" x=\"33.2875\" y=\"175.805184\"/>\n  </clipPath>\n  <clipPath id=\"pb978dcaae2\">\n   <rect height=\"127.905882\" width=\"558\" x=\"33.2875\" y=\"329.292243\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# 普通的线性回归\n",
    "coefs_lr, scores_lr = plot_regression(LinearRegression(), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 720x576 with 3 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"481.07625pt\" version=\"1.1\" viewBox=\"0 0 598.4875 481.07625\" width=\"598.4875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 481.07625 \nL 598.4875 481.07625 \nL 598.4875 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 33.2875 150.224007 \nL 591.2875 150.224007 \nL 591.2875 22.318125 \nL 33.2875 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path clip-path=\"url(#pffcd355447)\" d=\"M 223.251649 150.224007 \nL 254.7508 150.224007 \nL 254.7508 148.109161 \nL 223.251649 148.109161 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path clip-path=\"url(#pffcd355447)\" d=\"M 254.7508 150.224007 \nL 286.249951 150.224007 \nL 286.249951 147.263223 \nL 254.7508 147.263223 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path clip-path=\"url(#pffcd355447)\" d=\"M 286.249951 150.224007 \nL 317.749102 150.224007 \nL 317.749102 129.498517 \nL 286.249951 129.498517 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path clip-path=\"url(#pffcd355447)\" d=\"M 317.749102 150.224007 \nL 349.248253 150.224007 \nL 349.248253 85.932691 \nL 317.749102 85.932691 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path clip-path=\"url(#pffcd355447)\" d=\"M 349.248253 150.224007 \nL 380.747404 150.224007 \nL 380.747404 43.635772 \nL 349.248253 43.635772 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path clip-path=\"url(#pffcd355447)\" d=\"M 380.747404 150.224007 \nL 412.246555 150.224007 \nL 412.246555 53.364063 \nL 380.747404 53.364063 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path clip-path=\"url(#pffcd355447)\" d=\"M 412.246555 150.224007 \nL 443.745706 150.224007 \nL 443.745706 78.319245 \nL 412.246555 78.319245 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_10\">\n    <path clip-path=\"url(#pffcd355447)\" d=\"M 443.745706 150.224007 \nL 475.244857 150.224007 \nL 475.244857 109.618965 \nL 443.745706 109.618965 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_11\">\n    <path clip-path=\"url(#pffcd355447)\" d=\"M 475.244857 150.224007 \nL 506.744008 150.224007 \nL 506.744008 136.266024 \nL 475.244857 136.266024 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_12\">\n    <path clip-path=\"url(#pffcd355447)\" d=\"M 506.744008 150.224007 \nL 538.243159 150.224007 \nL 538.243159 147.263223 \nL 506.744008 147.263223 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m1e3474aa45\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"128.707616\" xlink:href=\"#m1e3474aa45\" y=\"150.224007\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"238.633444\" xlink:href=\"#m1e3474aa45\" y=\"150.224007\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"348.559273\" xlink:href=\"#m1e3474aa45\" y=\"150.224007\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"458.485102\" xlink:href=\"#m1e3474aa45\" y=\"150.224007\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"568.41093\" xlink:href=\"#m1e3474aa45\" y=\"150.224007\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_6\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mdd957e4328\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mdd957e4328\" y=\"150.224007\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(19.925 154.023226)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mdd957e4328\" y=\"107.927089\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(7.2 111.726307)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mdd957e4328\" y=\"65.63017\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(7.2 69.429389)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mdd957e4328\" y=\"23.333251\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(7.2 27.13247)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_13\">\n    <path d=\"M 33.2875 150.224007 \nL 33.2875 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_14\">\n    <path d=\"M 591.2875 150.224007 \nL 591.2875 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_15\">\n    <path d=\"M 33.2875 150.224007 \nL 591.2875 150.224007 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_16\">\n    <path d=\"M 33.2875 22.318125 \nL 591.2875 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_5\">\n    <!-- Coef 0 -->\n    <defs>\n     <path d=\"M 64.40625 67.28125 \nL 64.40625 56.890625 \nQ 59.421875 61.53125 53.78125 63.8125 \nQ 48.140625 66.109375 41.796875 66.109375 \nQ 29.296875 66.109375 22.65625 58.46875 \nQ 16.015625 50.828125 16.015625 36.375 \nQ 16.015625 21.96875 22.65625 14.328125 \nQ 29.296875 6.6875 41.796875 6.6875 \nQ 48.140625 6.6875 53.78125 8.984375 \nQ 59.421875 11.28125 64.40625 15.921875 \nL 64.40625 5.609375 \nQ 59.234375 2.09375 53.4375 0.328125 \nQ 47.65625 -1.421875 41.21875 -1.421875 \nQ 24.65625 -1.421875 15.125 8.703125 \nQ 5.609375 18.84375 5.609375 36.375 \nQ 5.609375 53.953125 15.125 64.078125 \nQ 24.65625 74.21875 41.21875 74.21875 \nQ 47.75 74.21875 53.53125 72.484375 \nQ 59.328125 70.75 64.40625 67.28125 \nz\n\" id=\"DejaVuSans-67\"/>\n     <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n     <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n     <path id=\"DejaVuSans-32\"/>\n    </defs>\n    <g transform=\"translate(292.898125 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-67\"/>\n     <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"131.005859\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"192.529297\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"227.734375\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"259.521484\" xlink:href=\"#DejaVuSans-48\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_2\">\n   <g id=\"patch_17\">\n    <path d=\"M 33.2875 303.711066 \nL 591.2875 303.711066 \nL 591.2875 175.805184 \nL 33.2875 175.805184 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_18\">\n    <path clip-path=\"url(#p82d187113a)\" d=\"M 58.651136 303.711066 \nL 96.411879 303.711066 \nL 96.411879 302.442159 \nL 58.651136 302.442159 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_19\">\n    <path clip-path=\"url(#p82d187113a)\" d=\"M 96.411879 303.711066 \nL 134.172622 303.711066 \nL 134.172622 300.750282 \nL 96.411879 300.750282 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_20\">\n    <path clip-path=\"url(#p82d187113a)\" d=\"M 134.172622 303.711066 \nL 171.933364 303.711066 \nL 171.933364 288.061206 \nL 134.172622 288.061206 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_21\">\n    <path clip-path=\"url(#p82d187113a)\" d=\"M 171.933364 303.711066 \nL 209.694107 303.711066 \nL 209.694107 253.800702 \nL 171.933364 253.800702 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_22\">\n    <path clip-path=\"url(#p82d187113a)\" d=\"M 209.694107 303.711066 \nL 247.45485 303.711066 \nL 247.45485 207.274091 \nL 209.694107 207.274091 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_23\">\n    <path clip-path=\"url(#p82d187113a)\" d=\"M 247.45485 303.711066 \nL 285.215592 303.711066 \nL 285.215592 181.89594 \nL 247.45485 181.89594 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_24\">\n    <path clip-path=\"url(#p82d187113a)\" d=\"M 285.215592 303.711066 \nL 322.976335 303.711066 \nL 322.976335 214.041598 \nL 285.215592 214.041598 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_25\">\n    <path clip-path=\"url(#p82d187113a)\" d=\"M 322.976335 303.711066 \nL 360.737078 303.711066 \nL 360.737078 271.565408 \nL 322.976335 271.565408 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_26\">\n    <path clip-path=\"url(#p82d187113a)\" d=\"M 360.737078 303.711066 \nL 398.49782 303.711066 \nL 398.49782 293.136836 \nL 360.737078 293.136836 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_27\">\n    <path clip-path=\"url(#p82d187113a)\" d=\"M 398.49782 303.711066 \nL 436.258563 303.711066 \nL 436.258563 301.173251 \nL 398.49782 301.173251 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"matplotlib.axis_3\">\n    <g id=\"xtick_6\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"128.707616\" xlink:href=\"#m1e3474aa45\" y=\"303.711066\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"238.633444\" xlink:href=\"#m1e3474aa45\" y=\"303.711066\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"348.559273\" xlink:href=\"#m1e3474aa45\" y=\"303.711066\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"458.485102\" xlink:href=\"#m1e3474aa45\" y=\"303.711066\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"568.41093\" xlink:href=\"#m1e3474aa45\" y=\"303.711066\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_4\">\n    <g id=\"ytick_5\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mdd957e4328\" y=\"303.711066\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 307.510285)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mdd957e4328\" y=\"261.414147\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 265.213366)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mdd957e4328\" y=\"219.117229\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 200 -->\n      <g transform=\"translate(7.2 222.916447)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mdd957e4328\" y=\"176.82031\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 300 -->\n      <g transform=\"translate(7.2 180.619529)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_28\">\n    <path d=\"M 33.2875 303.711066 \nL 33.2875 175.805184 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_29\">\n    <path d=\"M 591.2875 303.711066 \nL 591.2875 175.805184 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_30\">\n    <path d=\"M 33.2875 303.711066 \nL 591.2875 303.711066 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_31\">\n    <path d=\"M 33.2875 175.805184 \nL 591.2875 175.805184 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_10\">\n    <!-- Coef 1 -->\n    <g transform=\"translate(292.898125 169.805184)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-67\"/>\n     <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"131.005859\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"192.529297\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"227.734375\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"259.521484\" xlink:href=\"#DejaVuSans-49\"/>\n    </g>\n   </g>\n  </g>\n  <g id=\"axes_3\">\n   <g id=\"patch_32\">\n    <path d=\"M 33.2875 457.198125 \nL 591.2875 457.198125 \nL 591.2875 329.292243 \nL 33.2875 329.292243 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"patch_33\">\n    <path clip-path=\"url(#pd863cb124e)\" d=\"M 222.792802 457.198125 \nL 257.105908 457.198125 \nL 257.105908 455.083279 \nL 222.792802 455.083279 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_34\">\n    <path clip-path=\"url(#pd863cb124e)\" d=\"M 257.105908 457.198125 \nL 291.419014 457.198125 \nL 291.419014 452.545464 \nL 257.105908 452.545464 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_35\">\n    <path clip-path=\"url(#pd863cb124e)\" d=\"M 291.419014 457.198125 \nL 325.732121 457.198125 \nL 325.732121 434.780758 \nL 291.419014 434.780758 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_36\">\n    <path clip-path=\"url(#pd863cb124e)\" d=\"M 325.732121 457.198125 \nL 360.045227 457.198125 \nL 360.045227 406.441822 \nL 325.732121 406.441822 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_37\">\n    <path clip-path=\"url(#pd863cb124e)\" d=\"M 360.045227 457.198125 \nL 394.358333 457.198125 \nL 394.358333 367.951626 \nL 360.045227 367.951626 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_38\">\n    <path clip-path=\"url(#pd863cb124e)\" d=\"M 394.358333 457.198125 \nL 428.671439 457.198125 \nL 428.671439 349.340982 \nL 394.358333 349.340982 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_39\">\n    <path clip-path=\"url(#pd863cb124e)\" d=\"M 428.671439 457.198125 \nL 462.984545 457.198125 \nL 462.984545 380.640702 \nL 428.671439 380.640702 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_40\">\n    <path clip-path=\"url(#pd863cb124e)\" d=\"M 462.984545 457.198125 \nL 497.297651 457.198125 \nL 497.297651 406.864792 \nL 462.984545 406.864792 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_41\">\n    <path clip-path=\"url(#pd863cb124e)\" d=\"M 497.297651 457.198125 \nL 531.610757 457.198125 \nL 531.610757 441.548265 \nL 497.297651 441.548265 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"patch_42\">\n    <path clip-path=\"url(#pd863cb124e)\" d=\"M 531.610757 457.198125 \nL 565.923864 457.198125 \nL 565.923864 453.814371 \nL 531.610757 453.814371 \nz\n\" style=\"fill:#1f77b4;opacity:0.5;\"/>\n   </g>\n   <g id=\"matplotlib.axis_5\">\n    <g id=\"xtick_11\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"128.707616\" xlink:href=\"#m1e3474aa45\" y=\"457.198125\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0 -->\n      <g transform=\"translate(125.526366 471.796562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"238.633444\" xlink:href=\"#m1e3474aa45\" y=\"457.198125\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(235.452194 471.796562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"348.559273\" xlink:href=\"#m1e3474aa45\" y=\"457.198125\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 10 -->\n      <g transform=\"translate(342.196773 471.796562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"458.485102\" xlink:href=\"#m1e3474aa45\" y=\"457.198125\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 15 -->\n      <g transform=\"translate(452.122602 471.796562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"568.41093\" xlink:href=\"#m1e3474aa45\" y=\"457.198125\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 20 -->\n      <g transform=\"translate(562.04843 471.796562)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_6\">\n    <g id=\"ytick_9\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mdd957e4328\" y=\"457.198125\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- 0 -->\n      <g transform=\"translate(19.925 460.997344)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mdd957e4328\" y=\"414.901206\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- 100 -->\n      <g transform=\"translate(7.2 418.700425)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mdd957e4328\" y=\"372.604287\"/>\n      </g>\n     </g>\n     <g id=\"text_18\">\n      <!-- 200 -->\n      <g transform=\"translate(7.2 376.403506)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mdd957e4328\" y=\"330.307369\"/>\n      </g>\n     </g>\n     <g id=\"text_19\">\n      <!-- 300 -->\n      <g transform=\"translate(7.2 334.106587)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_43\">\n    <path d=\"M 33.2875 457.198125 \nL 33.2875 329.292243 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_44\">\n    <path d=\"M 591.2875 457.198125 \nL 591.2875 329.292243 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_45\">\n    <path d=\"M 33.2875 457.198125 \nL 591.2875 457.198125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_46\">\n    <path d=\"M 33.2875 329.292243 \nL 591.2875 329.292243 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"text_20\">\n    <!-- Coef 2 -->\n    <g transform=\"translate(292.898125 323.292243)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-67\"/>\n     <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"131.005859\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"192.529297\" xlink:href=\"#DejaVuSans-102\"/>\n     <use x=\"227.734375\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"259.521484\" xlink:href=\"#DejaVuSans-50\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pffcd355447\">\n   <rect height=\"127.905882\" width=\"558\" x=\"33.2875\" y=\"22.318125\"/>\n  </clipPath>\n  <clipPath id=\"p82d187113a\">\n   <rect height=\"127.905882\" width=\"558\" x=\"33.2875\" y=\"175.805184\"/>\n  </clipPath>\n  <clipPath id=\"pd863cb124e\">\n   <rect height=\"127.905882\" width=\"558\" x=\"33.2875\" y=\"329.292243\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "# 岭回归\n",
    "coefs_ridge, scores_ridge = plot_regression(Ridge(), X, y)  # 正则化系数 alpha 默认1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然, 岭回归的系数更接近0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([-39.922 , -16.9649, -18.7397])"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "np.mean(coefs_ridge - coefs_lr, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从均值上看，线性回归比岭回归的系数要大很多。均值显示的差异其实是线性回归的系数隐含的偏差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([72.9676, 45.0432, 41.9908])"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "coefs_lr.var(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([4.9412, 5.7828, 6.0879])"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "coefs_ridge.var(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0546, 0.0635],\n",
       "       [0.0486, 0.0744],\n",
       "       [0.0631, 0.0356],\n",
       "       ...,\n",
       "       [0.0598, 0.0482],\n",
       "       [0.0539, 0.059 ],\n",
       "       [0.0608, 0.0424]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0579, 0.0507]), array([0.0343, 0.0304]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_lr.mean(0), scores_ridge.mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "岭回归的系数方差也会小很多。这就是机器学习里著名的偏差-方差均衡(Bias-Variance Trade-off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化岭回归参数 \n",
    "---\n",
    "用OLS（普通最小二乘法）做回归也许可以显示两个变量之间的某些关系；但是，当alpha参数正则化之后，那些关系就会消失.\n",
    "\n",
    "在linear_models模块中，有一个对象叫RidgeCV，表示**岭回归交叉检验**（ridge cross-validation）。这个交叉检验类似于**留一交叉验证法**（leave-one-out cross-validation，LOOCV）\n",
    "\n",
    "指定cv属性的值将触发(通过GridSearchCV的)交叉验证。例如，cv=10将触发10折的交叉验证，而不是广义交叉验证(GCV)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mRidgeCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0malphas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mgcv_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mstore_cv_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nRidge regression with built-in cross-validation.\n\nSee glossary entry for :term:`cross-validation estimator`.\n\nBy default, it performs Generalized Cross-Validation, which is a form of\nefficient Leave-One-Out cross-validation.\n\nRead more in the :ref:`User Guide <ridge_regression>`.\n\nParameters\n----------\nalphas : ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)\n    Array of alpha values to try.\n    Regularization strength; must be a positive float. Regularization\n    improves the conditioning of the problem and reduces the variance of\n    the estimates. Larger values specify stronger regularization.\n    Alpha corresponds to ``C^-1`` in other linear models such as\n    LogisticRegression or LinearSVC.\n    If using generalized cross-validation, alphas must be positive.\n\nfit_intercept : bool, default=True\n    Whether to calculate the intercept for this model. If set\n    to false, no intercept will be used in calculations\n    (i.e. data is expected to be centered).\n\nnormalize : bool, default=False\n    This parameter is ignored when ``fit_intercept`` is set to False.\n    If True, the regressors X will be normalized before regression by\n    subtracting the mean and dividing by the l2-norm.\n    If you wish to standardize, please use\n    :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n    on an estimator with ``normalize=False``.\n\nscoring : string, callable, default=None\n    A string (see model evaluation documentation) or\n    a scorer callable object / function with signature\n    ``scorer(estimator, X, y)``.\n    If None, the negative mean squared error if cv is 'auto' or None\n    (i.e. when using generalized cross-validation), and r2 score otherwise.\n\ncv : int, cross-validation generator or an iterable, default=None\n    Determines the cross-validation splitting strategy.\n    Possible inputs for cv are:\n\n    - None, to use the efficient Leave-One-Out cross-validation\n      (also known as Generalized Cross-Validation).\n    - integer, to specify the number of folds.\n    - :term:`CV splitter`,\n    - An iterable yielding (train, test) splits as arrays of indices.\n\n    For integer/None inputs, if ``y`` is binary or multiclass,\n    :class:`sklearn.model_selection.StratifiedKFold` is used, else,\n    :class:`sklearn.model_selection.KFold` is used.\n\n    Refer :ref:`User Guide <cross_validation>` for the various\n    cross-validation strategies that can be used here.\n\ngcv_mode : {'auto', 'svd', eigen'}, default='auto'\n    Flag indicating which strategy to use when performing\n    Generalized Cross-Validation. Options are::\n\n        'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen'\n        'svd' : force use of singular value decomposition of X when X is\n            dense, eigenvalue decomposition of X^T.X when X is sparse.\n        'eigen' : force computation via eigendecomposition of X.X^T\n\n    The 'auto' mode is the default and is intended to pick the cheaper\n    option of the two depending on the shape of the training data.\n\nstore_cv_values : bool, default=False\n    Flag indicating if the cross-validation values corresponding to\n    each alpha should be stored in the ``cv_values_`` attribute (see\n    below). This flag is only compatible with ``cv=None`` (i.e. using\n    Generalized Cross-Validation).\n\nAttributes\n----------\ncv_values_ : ndarray of shape (n_samples, n_alphas) or         shape (n_samples, n_targets, n_alphas), optional\n    Cross-validation values for each alpha (if ``store_cv_values=True``        and ``cv=None``). After ``fit()`` has been called, this attribute         will contain the mean squared errors (by default) or the values         of the ``{loss,score}_func`` function (if provided in the constructor).\n\ncoef_ : ndarray of shape (n_features) or (n_targets, n_features)\n    Weight vector(s).\n\nintercept_ : float or ndarray of shape (n_targets,)\n    Independent term in decision function. Set to 0.0 if\n    ``fit_intercept = False``.\n\nalpha_ : float\n    Estimated regularization parameter.\n\nExamples\n--------\n>>> from sklearn.datasets import load_diabetes\n>>> from sklearn.linear_model import RidgeCV\n>>> X, y = load_diabetes(return_X_y=True)\n>>> clf = RidgeCV(alphas=[1e-3, 1e-2, 1e-1, 1]).fit(X, y)\n>>> clf.score(X, y)\n0.5166...\n\nSee also\n--------\nRidge : Ridge regression\nRidgeClassifier : Ridge classifier\nRidgeClassifierCV : Ridge classifier with built-in cross validation\n\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.7/site-packages/sklearn/linear_model/_ridge.py\n\u001b[0;31mType:\u001b[0m           ABCMeta\n\u001b[0;31mSubclasses:\u001b[0m     \n"
    }
   ],
   "source": [
    "RidgeCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RidgeCV(alphas=array([     0.    ,      0.0001,      0.001 ,      0.01  ,      0.1   ,\n            1.    ,     10.    ,    100.    ,   1000.    ,  10000.    ,\n       100000.    ]),\n        cv=None, fit_intercept=True, gcv_mode=None, normalize=False,\n        scoring=None, store_cv_values=False)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=2, effective_rank=1, noise=10, random_state=12345)\n",
    "rcv = RidgeCV(alphas=np.logspace(-5, 5, 11))\n",
    "rcv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.01"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# 拟合模型之后，alpha参数就是最优正则化系数：\n",
    "rcv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.05"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# 查看0.1附近更优的alpha值\n",
    "rcv = RidgeCV(alphas=np.linspace(.05, .2, 16))\n",
    "rcv.fit(X, y)\n",
    "rcv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RidgeCV(alphas=array([0.001, 0.002, 0.003, ..., 0.998, 0.999, 1.   ]), cv=None,\n        fit_intercept=True, gcv_mode=None, normalize=False, scoring=None,\n        store_cv_values=True)"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "alpha_list = np.linspace(0.001, 1, 1000)\n",
    "rcv = RidgeCV(alphas=alpha_list, store_cv_values=True)  # 保存交叉检验的数据\n",
    "rcv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(100, 1000)"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "rcv.cv_values_.shape  # 100次交叉验证  1000个不同alpha 的均方根误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "35"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "min_alpha_idx = rcv.cv_values_.mean(0).argmin()\n",
    "min_alpha_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.036000000000000004"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "alpha_list[min_alpha_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.036000000000000004"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "rcv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_ridgecv(alpha_list, X, y):\n",
    "    rcv = RidgeCV(alphas=alpha_list, store_cv_values=True)  # 保存交叉检验的数据\n",
    "    rcv.fit(X, y)\n",
    "    min_alpha_idx = rcv.cv_values_.mean(0).argmin()\n",
    "    f, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.set_title(r\"Various values of $\\alpha$\")\n",
    "    xy = (alpha_list[min_alpha_idx], rcv.cv_values_.mean(axis=0)[min_alpha_idx])\n",
    "    xytext = (xy[0] + .01, xy[1] + .1)\n",
    "    \n",
    "    ax.annotate(r'Chosen $\\alpha$', xy=xy, xytext=xytext,\n",
    "            arrowprops=dict(facecolor='black', shrink=0, width=0)\n",
    "            )\n",
    "    ax.plot(alpha_list, rcv.cv_values_.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 720x432 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"372.35625pt\" version=\"1.1\" viewBox=\"0 0 601.665625 372.35625\" width=\"601.665625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 372.35625 \nL 601.665625 372.35625 \nL 601.665625 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 36.465625 348.478125 \nL 594.465625 348.478125 \nL 594.465625 22.318125 \nL 36.465625 22.318125 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m3ca25c15ad\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"56.705294\" xlink:href=\"#m3ca25c15ad\" y=\"348.478125\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0.00 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g transform=\"translate(45.572482 363.076563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"159.184633\" xlink:href=\"#m3ca25c15ad\" y=\"348.478125\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 0.02 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(148.051821 363.076563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"261.663972\" xlink:href=\"#m3ca25c15ad\" y=\"348.478125\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 0.04 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(250.53116 363.076563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"364.143311\" xlink:href=\"#m3ca25c15ad\" y=\"348.478125\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 0.06 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(353.010498 363.076563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"466.62265\" xlink:href=\"#m3ca25c15ad\" y=\"348.478125\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0.08 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(455.489837 363.076563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"569.101989\" xlink:href=\"#m3ca25c15ad\" y=\"348.478125\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0.10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(557.969176 363.076563)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m790dbfc679\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m790dbfc679\" y=\"332.687738\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 98.1 -->\n      <defs>\n       <path d=\"M 10.984375 1.515625 \nL 10.984375 10.5 \nQ 14.703125 8.734375 18.5 7.8125 \nQ 22.3125 6.890625 25.984375 6.890625 \nQ 35.75 6.890625 40.890625 13.453125 \nQ 46.046875 20.015625 46.78125 33.40625 \nQ 43.953125 29.203125 39.59375 26.953125 \nQ 35.25 24.703125 29.984375 24.703125 \nQ 19.046875 24.703125 12.671875 31.3125 \nQ 6.296875 37.9375 6.296875 49.421875 \nQ 6.296875 60.640625 12.9375 67.421875 \nQ 19.578125 74.21875 30.609375 74.21875 \nQ 43.265625 74.21875 49.921875 64.515625 \nQ 56.59375 54.828125 56.59375 36.375 \nQ 56.59375 19.140625 48.40625 8.859375 \nQ 40.234375 -1.421875 26.421875 -1.421875 \nQ 22.703125 -1.421875 18.890625 -0.6875 \nQ 15.09375 0.046875 10.984375 1.515625 \nz\nM 30.609375 32.421875 \nQ 37.25 32.421875 41.125 36.953125 \nQ 45.015625 41.5 45.015625 49.421875 \nQ 45.015625 57.28125 41.125 61.84375 \nQ 37.25 66.40625 30.609375 66.40625 \nQ 23.96875 66.40625 20.09375 61.84375 \nQ 16.21875 57.28125 16.21875 49.421875 \nQ 16.21875 41.5 20.09375 36.953125 \nQ 23.96875 32.421875 30.609375 32.421875 \nz\n\" id=\"DejaVuSans-57\"/>\n      </defs>\n      <g transform=\"translate(7.2 336.486956)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m790dbfc679\" y=\"275.605506\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 98.2 -->\n      <g transform=\"translate(7.2 279.404725)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m790dbfc679\" y=\"218.523275\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 98.3 -->\n      <defs>\n       <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n      </defs>\n      <g transform=\"translate(7.2 222.322494)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m790dbfc679\" y=\"161.441044\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 98.4 -->\n      <g transform=\"translate(7.2 165.240262)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m790dbfc679\" y=\"104.358812\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 98.5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(7.2 108.158031)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m790dbfc679\" y=\"47.276581\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 98.6 -->\n      <g transform=\"translate(7.2 51.0758)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-57\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path clip-path=\"url(#p59a2bea1d6)\" d=\"M 61.829261 223.542649 \nL 66.953228 230.070286 \nL 72.077195 236.381746 \nL 77.201162 242.478715 \nL 82.325129 248.362865 \nL 87.449096 254.035853 \nL 92.573063 259.499317 \nL 97.69703 264.754883 \nL 102.820997 269.804159 \nL 107.944964 274.648741 \nL 113.068931 279.290209 \nL 118.192898 283.730127 \nL 123.316865 287.970045 \nL 128.440832 292.011502 \nL 133.564799 295.856018 \nL 138.688765 299.505102 \nL 143.812732 302.96025 \nL 148.936699 306.222941 \nL 154.060666 309.294644 \nL 159.184633 312.176814 \nL 164.3086 314.870891 \nL 169.432567 317.378303 \nL 174.556534 319.700467 \nL 179.680501 321.838786 \nL 184.804468 323.794649 \nL 189.928435 325.569435 \nL 195.052402 327.164509 \nL 200.176369 328.581226 \nL 205.300336 329.820927 \nL 210.424303 330.884943 \nL 215.54827 331.774591 \nL 220.672237 332.49118 \nL 225.796204 333.036003 \nL 230.92017 333.410346 \nL 236.044137 333.615481 \nL 241.168104 333.65267 \nL 246.292071 333.523166 \nL 251.416038 333.228208 \nL 256.540005 332.769026 \nL 261.663972 332.146839 \nL 266.787939 331.362856 \nL 271.911906 330.418276 \nL 277.035873 329.314287 \nL 282.15984 328.052068 \nL 287.283807 326.632787 \nL 292.407774 325.057601 \nL 297.531741 323.327662 \nL 302.655708 321.444106 \nL 307.779675 319.408064 \nL 312.903642 317.220657 \nL 318.027608 314.882994 \nL 323.151575 312.396179 \nL 328.275542 309.761302 \nL 333.399509 306.979448 \nL 338.523476 304.051691 \nL 343.647443 300.979098 \nL 348.77141 297.762724 \nL 353.895377 294.403618 \nL 359.019344 290.902821 \nL 364.143311 287.261363 \nL 369.267278 283.480267 \nL 374.391245 279.560548 \nL 379.515212 275.503212 \nL 384.639179 271.309258 \nL 389.763146 266.979676 \nL 394.887113 262.515448 \nL 400.01108 257.917548 \nL 405.135046 253.186943 \nL 410.259013 248.324592 \nL 415.38298 243.331447 \nL 420.506947 238.20845 \nL 425.630914 232.956539 \nL 430.754881 227.576642 \nL 435.878848 222.06968 \nL 441.002815 216.436568 \nL 446.126782 210.678212 \nL 451.250749 204.795514 \nL 456.374716 198.789364 \nL 461.498683 192.660651 \nL 466.62265 186.410251 \nL 471.746617 180.039038 \nL 476.870584 173.547877 \nL 481.994551 166.937627 \nL 487.118518 160.209139 \nL 492.242485 153.36326 \nL 497.366451 146.400827 \nL 502.490418 139.322675 \nL 507.614385 132.129628 \nL 512.738352 124.822506 \nL 517.862319 117.402124 \nL 522.986286 109.869289 \nL 528.110253 102.224801 \nL 533.23422 94.469455 \nL 538.358187 86.604042 \nL 543.482154 78.629344 \nL 548.606121 70.546138 \nL 553.730088 62.355196 \nL 558.854055 54.057284 \nL 563.978022 45.65316 \nL 569.101989 37.14358 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 36.465625 348.478125 \nL 36.465625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 594.465625 348.478125 \nL 594.465625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 36.465625 348.478125 \nL 594.465625 348.478125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 36.465625 22.318125 \nL 594.465625 22.318125 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path d=\"M 290.546958 280.647594 \nQ 269.948827 302.758385 249.350696 324.869176 \nL 253.74085 328.958986 \nQ 247.454477 331.305828 241.168104 333.65267 \nQ 243.064324 327.216018 244.960543 320.779365 \nL 249.350696 324.869176 \nQ 269.948827 302.758385 290.546958 280.647594 \nL 290.546958 280.647594 \nz\n\" style=\"stroke:#000000;stroke-linecap:round;\"/>\n   </g>\n   <g id=\"text_13\">\n    <!-- Chosen $\\alpha$ -->\n    <defs>\n     <path d=\"M 64.40625 67.28125 \nL 64.40625 56.890625 \nQ 59.421875 61.53125 53.78125 63.8125 \nQ 48.140625 66.109375 41.796875 66.109375 \nQ 29.296875 66.109375 22.65625 58.46875 \nQ 16.015625 50.828125 16.015625 36.375 \nQ 16.015625 21.96875 22.65625 14.328125 \nQ 29.296875 6.6875 41.796875 6.6875 \nQ 48.140625 6.6875 53.78125 8.984375 \nQ 59.421875 11.28125 64.40625 15.921875 \nL 64.40625 5.609375 \nQ 59.234375 2.09375 53.4375 0.328125 \nQ 47.65625 -1.421875 41.21875 -1.421875 \nQ 24.65625 -1.421875 15.125 8.703125 \nQ 5.609375 18.84375 5.609375 36.375 \nQ 5.609375 53.953125 15.125 64.078125 \nQ 24.65625 74.21875 41.21875 74.21875 \nQ 47.75 74.21875 53.53125 72.484375 \nQ 59.328125 70.75 64.40625 67.28125 \nz\n\" id=\"DejaVuSans-67\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n     <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n     <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n     <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n     <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n     <path id=\"DejaVuSans-32\"/>\n     <path d=\"M 40.921875 25.4375 \nL 40.96875 36.71875 \nQ 41.015625 48.25 32.328125 48.296875 \nQ 25.828125 48.34375 21.78125 42.921875 \nQ 16.703125 36.234375 14.984375 27.296875 \nQ 12.890625 16.546875 15.53125 11.421875 \nQ 18.265625 6.203125 24.171875 6.203125 \nQ 30.71875 6.203125 36.234375 16.609375 \nz\nM 33.84375 55.90625 \nQ 49.078125 56.15625 48.875 40.375 \nQ 48.875 40.375 56.5 54.6875 \nL 64.5 54.6875 \nL 48.734375 25.046875 \nL 48.578125 14.359375 \nQ 48.578125 11.96875 49.90625 9.96875 \nQ 51.421875 7.625 52.984375 7.625 \nL 57.328125 7.625 \nL 55.859375 0 \nL 50.4375 0 \nQ 45.84375 0 42.53125 4.109375 \nQ 40.96875 6.15625 40.921875 10.453125 \nQ 37.75 5.21875 32.28125 0.78125 \nQ 29.6875 -1.265625 22.703125 -1.21875 \nQ 11.28125 -1.125 7.125 6.203125 \nQ 2.875 13.8125 5.515625 27.296875 \nQ 8.34375 41.796875 15.765625 48.390625 \nQ 24.125 55.765625 33.84375 55.90625 \nz\n\" id=\"DejaVuSans-Oblique-945\"/>\n    </defs>\n    <g transform=\"translate(292.407774 276.570439)scale(0.1 -0.1)\">\n     <use transform=\"translate(0 0.015625)\" xlink:href=\"#DejaVuSans-67\"/>\n     <use transform=\"translate(69.824219 0.015625)\" xlink:href=\"#DejaVuSans-104\"/>\n     <use transform=\"translate(133.203125 0.015625)\" xlink:href=\"#DejaVuSans-111\"/>\n     <use transform=\"translate(194.384766 0.015625)\" xlink:href=\"#DejaVuSans-115\"/>\n     <use transform=\"translate(246.484375 0.015625)\" xlink:href=\"#DejaVuSans-101\"/>\n     <use transform=\"translate(308.007812 0.015625)\" xlink:href=\"#DejaVuSans-110\"/>\n     <use transform=\"translate(371.386719 0.015625)\" xlink:href=\"#DejaVuSans-32\"/>\n     <use transform=\"translate(403.173828 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-945\"/>\n    </g>\n   </g>\n   <g id=\"text_14\">\n    <!-- Various values of $\\alpha$ -->\n    <defs>\n     <path d=\"M 28.609375 0 \nL 0.78125 72.90625 \nL 11.078125 72.90625 \nL 34.1875 11.53125 \nL 57.328125 72.90625 \nL 67.578125 72.90625 \nL 39.796875 0 \nz\n\" id=\"DejaVuSans-86\"/>\n     <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n     <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n     <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n     <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n     <path d=\"M 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 8.796875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nL 35.6875 0 \nL 23.484375 0 \nz\n\" id=\"DejaVuSans-118\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n     <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n    </defs>\n    <g transform=\"translate(257.925625 16.318125)scale(0.12 -0.12)\">\n     <use transform=\"translate(0 0.015625)\" xlink:href=\"#DejaVuSans-86\"/>\n     <use transform=\"translate(68.408203 0.015625)\" xlink:href=\"#DejaVuSans-97\"/>\n     <use transform=\"translate(129.6875 0.015625)\" xlink:href=\"#DejaVuSans-114\"/>\n     <use transform=\"translate(170.800781 0.015625)\" xlink:href=\"#DejaVuSans-105\"/>\n     <use transform=\"translate(198.583984 0.015625)\" xlink:href=\"#DejaVuSans-111\"/>\n     <use transform=\"translate(259.765625 0.015625)\" xlink:href=\"#DejaVuSans-117\"/>\n     <use transform=\"translate(323.144531 0.015625)\" xlink:href=\"#DejaVuSans-115\"/>\n     <use transform=\"translate(375.244141 0.015625)\" xlink:href=\"#DejaVuSans-32\"/>\n     <use transform=\"translate(407.03125 0.015625)\" xlink:href=\"#DejaVuSans-118\"/>\n     <use transform=\"translate(466.210938 0.015625)\" xlink:href=\"#DejaVuSans-97\"/>\n     <use transform=\"translate(527.490234 0.015625)\" xlink:href=\"#DejaVuSans-108\"/>\n     <use transform=\"translate(555.273438 0.015625)\" xlink:href=\"#DejaVuSans-117\"/>\n     <use transform=\"translate(618.652344 0.015625)\" xlink:href=\"#DejaVuSans-101\"/>\n     <use transform=\"translate(680.175781 0.015625)\" xlink:href=\"#DejaVuSans-115\"/>\n     <use transform=\"translate(732.275391 0.015625)\" xlink:href=\"#DejaVuSans-32\"/>\n     <use transform=\"translate(764.0625 0.015625)\" xlink:href=\"#DejaVuSans-111\"/>\n     <use transform=\"translate(825.244141 0.015625)\" xlink:href=\"#DejaVuSans-102\"/>\n     <use transform=\"translate(860.449219 0.015625)\" xlink:href=\"#DejaVuSans-32\"/>\n     <use transform=\"translate(892.236328 0.015625)\" xlink:href=\"#DejaVuSans-Oblique-945\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p59a2bea1d6\">\n   <rect height=\"326.16\" width=\"558\" x=\"36.465625\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt_ridgecv(np.linspace(0.001, 0.1, 100), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "RidgeCV(alphas=array([0.01  , 0.0109, 0.0118, ..., 0.0982, 0.0991, 0.1   ]),\n        cv=None, fit_intercept=True, gcv_mode=None, normalize=False,\n        scoring=make_scorer(mad, greater_is_better=False),\n        store_cv_values=True)"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "# 使用自定义的函数(偏差的绝对值的均值)   作为评分函数\n",
    "def mad(y, y_pred):\n",
    "    return np.abs(y - y_pred).mean()\n",
    "MAD = make_scorer(mad, greater_is_better=False)  # \n",
    "rcv = RidgeCV(alphas=np.linspace(0.01, 0.1, 100), store_cv_values=True, scoring=MAD)\n",
    "rcv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.01"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "rcv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "rcv.cv_values_.mean(0).argmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO 正则化\n",
    "---\n",
    "LASSO（ least absolute shrinkage and selection operator，最小绝对值收缩和选择算子）方法与岭回归和LARS（least angle regression，最小角回归）很类似。与岭回归类似，它也是通过增加惩罚函数来判断、消除特征间的共线性。与LARS相似的是它也可以用作参数选择，通常得出一个相关系数的稀疏向量。\n",
    "\n",
    "`Lasso` 类的实现使用了 coordinate descent （坐标下降算法）来拟合系数\n",
    "\n",
    "其最小化的目标函数是\n",
    "$$\\underset {\\theta} {min} ||X\\theta-y||^2 + \\alpha||\\theta||_1$$\n",
    "最小化残差平方和的另一种表达方式是：\n",
    "\n",
    "$$ RSS(\\theta),其中 {\\begin{Vmatrix} \\theta \\end{Vmatrix}}_1 \\lt \\theta $$\n",
    "这个约束会让数据稀疏。LASSO回归的约束创建了围绕原点的超立方体（相关系数是轴），也就意味着大多数点都在各个顶点上，那里相关系数为0。而岭回归创建的是超平面，因为其约束是L2范数，少一个约束，但是即使有限制相关系数也不会变成0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 500个特征中只有5个是有用的\n",
    "X, y = make_regression(n_samples=200, n_features=500, n_informative=5, noise=5, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Lasso(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=1000,\n      normalize=False, positive=False, precompute=False, random_state=None,\n      selection='cyclic', tol=0.0001, warm_start=False)"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso()\n",
    "lasso.fit(X, y)  # 默认alpha 也是1. 设置为0时就变成普通线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "7"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "np.sum(lasso.coef_ != 0)   # 其余系数都是0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "500"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "# 如果使用线性回归, 就有500个系数\n",
    "lasso_0 = Lasso(alpha=0)\n",
    "lasso_0.fit(X, y)\n",
    "np.sum(lasso_0.coef_ != 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LASSO交叉验证**  \n",
    "操作上与岭回归的交叉验证差不多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LassoCV(alphas=array([     0.    ,      0.0001,      0.001 ,      0.01  ,      0.1   ,\n            1.    ,     10.    ,    100.    ,   1000.    ,  10000.    ,\n       100000.    ]),\n        copy_X=True, cv=None, eps=0.001, fit_intercept=True, max_iter=1000,\n        n_alphas=100, n_jobs=None, normalize=False, positive=False,\n        precompute='auto', random_state=None, selection='cyclic', tol=0.0001,\n        verbose=False)"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "alpha_list = np.logspace(-5, 5, 11)\n",
    "lasso_cv = LassoCV(alphas=alpha_list)\n",
    "lasso_cv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1.0"
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "lasso_cv.alpha_  # 通过交叉验证得到最优的正则化系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "7"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "np.sum(lasso_cv.coef_ != 0)  # 交叉验证之后的非零系数数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LASSO特征选择**\n",
    "---\n",
    "LASSO通常用来为其他方法所特征选择([L1-based feature selection](https://github.com/apachecn/sklearn-doc-zh/blob/master/docs/0.21.3/14.md#11341-%E5%9F%BA%E4%BA%8E-l1-%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E5%8F%96))。例如，你可能会用LASSO回归获取适当的特征变量，然后在其他算法中使用。\n",
    "\n",
    "要获取想要的特征，需要创建一个非零相关系数的列向量，然后再其他算法拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 11)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = lasso_cv.coef_ != 0\n",
    "X_new = X[:, mask]\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LARS (Least Angle Regression)\n",
    "---\n",
    "最小角回归 （LARS） 是对高维数据的回归算法， 由 Bradley Efron, Trevor Hastie, Iain Johnstone 和 Robert Tibshirani 开发完成。 LARS 和逐步回归很像。在每一步，它都寻找与响应最有关联的预测。当有很多预测有相同的关联时，它并不会继续利用相同的预测，而是在这些预测中找出应该等角的方向。\n",
    "\n",
    "LARS的优点:\n",
    "\n",
    "- 当 p(n_feature) >> n(n_sample)，该算法数值运算上非常有效。(例如当维度的数目远超点的个数)\n",
    "- 它在计算上和前向选择一样快，和普通最小二乘法有相同的运算复杂度。\n",
    "- 它产生了一个完整的分段线性的解决路径，在交叉验证或者其他相似的微调模型的方法上非常有用。\n",
    "- 如果两个变量对响应几乎有相等的联系，则它们的系数应该有相似的增长率。因此这个算法和我们直觉 上的判断一样，而且还更加稳定。\n",
    "- 它很容易修改并为其他估算器生成解，比如Lasso。\n",
    "\n",
    "LARS 的缺点:\n",
    "- 因为 LARS 是建立在循环拟合剩余变量上的，所以它对噪声非常敏感。\n",
    "\n",
    "[算法的具体描述](https://blog.csdn.net/u014664226/article/details/52240272/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=200, n_features=500, \n",
    "                       n_informative=10, noise=2, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于我们用了10个信息特征，因此我们还要为LARS设置10个非0的相关系数。我们事先可能不知道信息特征的准确数量，但是出于试验的目的是可行的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,\n",
       "     n_nonzero_coefs=10, normalize=True, positive=False, precompute='auto',\n",
       "     verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lars\n",
    "\n",
    "lars = Lars(n_nonzero_coefs=10)\n",
    "train_n = 100\n",
    "lars.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(lars.coef_ != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,\n",
       "     n_nonzero_coefs=12, normalize=True, positive=False, precompute='auto',\n",
       "     verbose=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lars_12 = Lars(n_nonzero_coefs=12)  # 10左右估计一个值\n",
    "lars_12.fit(X[:train_n], y[:train_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=9.508e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=9.161e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.910e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.801e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.668e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.599e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.535e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.334e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=8.110e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.936e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.918e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.839e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.838e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.764e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.580e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.732e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.559e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.552e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.479e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.431e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.401e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.397e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.312e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.306e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.273e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.220e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.219e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.163e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.162e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.011e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=7.007e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.930e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.902e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.780e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.758e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.744e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.742e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.730e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.653e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.643e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.609e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.605e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.530e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.443e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.403e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 9.246e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.391e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.388e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.330e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.320e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.308e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.295e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.278e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.271e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.266e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.229e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.206e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.198e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.170e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.165e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.155e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.133e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.132e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=6.121e-02, with an active set of 100 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.148e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.144e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.105e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.918e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.875e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.868e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.806e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.767e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.762e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.689e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.643e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.603e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.599e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.598e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.560e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.509e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.496e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.488e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.473e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.465e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.449e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.415e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.380e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.325e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.271e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.263e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.254e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.247e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.237e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.237e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.228e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.225e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.174e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.173e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.151e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.129e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.103e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.096e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.089e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.071e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.026e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.991e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.978e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.976e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.972e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.966e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.933e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.928e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.903e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.888e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.848e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.793e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.781e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.748e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.722e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.634e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.626e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.572e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.543e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.518e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.510e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.486e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.451e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.428e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.408e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.377e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.352e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.289e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.280e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.267e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.264e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.251e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.244e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.222e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.220e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.214e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.211e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.184e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.143e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.125e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.070e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.066e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.055e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.013e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.007e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.992e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.953e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.953e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.943e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.942e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.895e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.886e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.850e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.848e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.806e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.804e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.790e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.765e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.718e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.718e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.705e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.697e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.687e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.674e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.661e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.601e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.598e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.597e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.576e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.570e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.568e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.568e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.550e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.542e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.519e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.493e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.468e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.421e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.356e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.386e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.317e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.314e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.295e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.271e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.261e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.251e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.816e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.247e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.227e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.226e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.213e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.212e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.210e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.178e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.174e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.154e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.144e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.122e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.116e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.108e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.104e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.073e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 9.884e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.063e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.049e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.035e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.035e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.027e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.026e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.024e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.015e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.996e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.990e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.946e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.940e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.933e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.899e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.890e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.865e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.862e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.844e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.810e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.808e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.792e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.755e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.741e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.737e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.734e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.732e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.725e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.720e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.706e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.706e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.686e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.674e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.668e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.647e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.626e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.614e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.592e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.589e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.572e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.539e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.502e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.486e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.481e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.454e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.451e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.448e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.445e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.434e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.427e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.407e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.363e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.348e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.312e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.275e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.253e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.240e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.224e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.216e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.203e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.197e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.178e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.177e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.175e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.170e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.159e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.159e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.130e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.063e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.053e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.044e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.020e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.010e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=2.002e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.997e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.689e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.995e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.993e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.971e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.963e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.950e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.928e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.928e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.898e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.891e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.875e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.868e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.860e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.817e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.814e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.796e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.774e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.751e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.749e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.739e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.692e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.673e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.665e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.622e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.621e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.602e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.583e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.575e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.555e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.555e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.532e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.510e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.471e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.429e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.413e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.391e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.378e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.374e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.349e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.303e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.296e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.259e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.217e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.178e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.144e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.125e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.121e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.117e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.115e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.103e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.084e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.083e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.071e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.056e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.051e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.033e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=1.003e-02, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=9.877e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=9.863e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=9.506e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=9.412e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=9.285e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=9.284e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=9.186e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.833e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.779e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.711e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.638e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.435e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.370e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.283e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.272e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.257e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.232e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.215e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.159e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=8.021e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=7.939e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=7.313e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.933e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.913e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.764e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.354e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.238e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=6.187e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.865e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.806e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.795e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.677e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.651e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.366e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.140e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=5.021e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.841e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.719e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.707e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.626e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.621e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.546e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.309e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.212e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=4.210e-03, with an active set of 101 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=4.077e-03, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=4.042e-03, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=3.686e-03, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=3.599e-03, with an active set of 102 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=3.340e-03, with an active set of 102 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=3.215e-03, with an active set of 102 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n",
      "/home/ulysses/.local/lib/python3.8/site-packages/sklearn/linear_model/least_angle.py:571: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=3.189e-03, with an active set of 102 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.\n",
      "  warnings.warn('Regressors in active set degenerate. '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lars(copy_X=True, eps=2.220446049250313e-16, fit_intercept=True, fit_path=True,\n",
       "     n_nonzero_coefs=500, normalize=True, positive=False, precompute='auto',\n",
       "     verbose=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lars_500 = Lars()  # 默认所有系数非零\n",
    "lars_500.fit(X[:train_n], y[:train_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.156478326549475"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(y[:train_n]-lars_12.predict(X[:train_n]), 2).mean()  # 12 训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376.6855131789647"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(y[:train_n]-lars_500.predict(X[:train_n]), 2).mean()  # 500 训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.010395792572443"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(y[train_n:]-lars.predict(X[train_n:]), 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.26807154043368"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(y[train_n:]-lars_12.predict(X[train_n:]), 2).mean()  # 12 测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.91887311386876e+31"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power(y[train_n:]-lars_500.predict(X[train_n:]), 2).mean()  # 500 测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试集的误差明显高很多。高维数据集问题就在于此；通常面对大量的特征时，想找出一个对训练集拟合很好的模型并不难，但是拟合过度却是更大的问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 贝叶斯回归\n",
    "---\n",
    "贝叶斯回归可以用于在预估阶段的参数正则化: 正则化参数的选择不是通过人为的选择，而是通过手动调节数据值来实现。\n",
    "上述过程可以通过引入 无信息先验 到模型中的超参数来完成。 在`岭回归`中使用的 $\\ell_{2}$ 正则项相当于在 w 为高斯先验条件，且此先验的精确度为 $\\lambda^{-1}$ 时，求最大后验估计。在这里，我们没有手工调参数 lambda ，而是让他作为一个变量，通过数据中估计得到。\n",
    "\n",
    "为了得到一个全概率模型，输出y也被认为是关于 X w 的高斯分布。\n",
    "$$p(y|X,w,\\alpha) = \\mathcal{N}(y|X w,\\alpha)$$\n",
    "\n",
    "**贝叶斯岭回归**\n",
    "\n",
    "`BayesianRidge `利用概率模型估算了上述的回归问题，其先验参数$\\theta$是由以下球面高斯公式得出的\n",
    "$$p(\\theta|\\lambda) =\\mathcal{N}(\\theta|0,\\lambda^{-1}{I_{p}})$$\n",
    "\n",
    "> $I_{p}$为指示函数\n",
    "\n",
    "先验参数 $\\alpha $和 $\\lambda$ 一般是服从[Gamma](https://blog.csdn.net/chenshulong/article/details/79027103)分布 ，这个分布与高斯成共轭先验关系。 得到的模型一般称为`贝叶斯岭回归`，并且这个与传统的 Ridge 非常相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=1000, n_features=10, n_informative=2, noise=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianRidge(alpha_1=1e-06, alpha_2=1e-06, compute_score=False, copy_X=True,\n",
       "              fit_intercept=True, lambda_1=1e-06, lambda_2=1e-06, n_iter=300,\n",
       "              normalize=False, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "br = BayesianRidge()\n",
    "br.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有两组相关系数，分别是alpha_1 / alpha_2和lambda_1 / lambda_2。其中，alpha_*是先验概率分布的$\\alpha$超参数，lambda_*是先验概率分布的$\\lambda$超参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0317, -0.544 , -0.6689, 85.7597, -0.5359,  0.1003, -0.7926,\n",
       "       93.5907, -0.4476,  1.0632])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "br.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.033 , -0.5474, -0.6639, 85.7153, -0.5356,  0.1004, -0.7896,\n",
       "       93.5411, -0.4445,  1.0605])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们来调整超参数，注意观察相关系数的变化\n",
    "br_alphas = BayesianRidge(alpha_1=10, lambda_1=10)\n",
    "br_alphas.fit(X, y)\n",
    "br_alphas.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 内核岭回归\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}