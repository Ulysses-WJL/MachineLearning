{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 异常检测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "异常检测设定的类型\n",
    "- 监督AD\n",
    "    * 标签可用于正常和异常数据\n",
    "    * 类似稀有类挖掘/不平衡分类\n",
    "- 半监督AD(新奇检测)\n",
    "    * 只有正常的数据可供训练\n",
    "    * 该算法仅学习正常数据\n",
    "- 无监督AD(异常值检测)\n",
    "    * 没有标签, 训练集 = 正常 + 异常数据\n",
    "    * 假设: 异常非常罕见"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# note:\n",
    "* [covariance matrix](http://docs.scipy.org/doc/numpy/reference/generated/numpy.cov.html)\n",
    "* [multivariate_normal](http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.multivariate_normal.html)\n",
    "* [seaborn  bivariate kernel density estimate](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.kdeplot.html#seaborn.kdeplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 高斯分布\n",
    "\n",
    "通常如果我们认为变量 $x$ 符合高斯分布 $x \\sim N(\\mu, \\sigma^2)$则其概率密度函数为：\n",
    "\n",
    "$p(x,\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$\n",
    "\n",
    "我们可以利用已有的数据来预测总体中的$μ​$和$σ^2​$的计算方法如下：\n",
    "\n",
    "$\\mu=\\frac{1}{m}\\sum\\limits_{i=1}^{m}x^{(i)}$\n",
    "\n",
    "\n",
    "$\\sigma^2=\\frac{1}{m}\\sum\\limits_{i=1}^{m}(x^{(i)}-\\mu)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用高斯分布进行异常检测\n",
    "\n",
    "异常检测算法：\n",
    "\n",
    "对于给定的数据集 $x^{(1)},x^{(2)},...,x^{(m)}$，我们要针对每一个特征计算 $\\mu$ 和 $\\sigma^2$ 的估计值。\n",
    "\n",
    "$\\mu_j=\\frac{1}{m}\\sum\\limits_{i=1}^{m}x_j^{(i)}$\n",
    "\n",
    "$\\sigma_j^2=\\frac{1}{m}\\sum\\limits_{i=1}^m(x_j^{(i)}-\\mu_j)^2$\n",
    "\n",
    "一旦我们获得了平均值和方差的估计值，给定新的一个训练实例，根据模型计算 $p(x)$：\n",
    "\n",
    "$p(x)=\\prod\\limits_{j=1}^np(x_j;\\mu_j,\\sigma_j^2)=\\prod\\limits_{j=1}^1\\frac{1}{\\sqrt{2\\pi}\\sigma_j}exp(-\\frac{(x_j-\\mu_j)^2}{2\\sigma_j^2})$\n",
    "\n",
    "当$p(x) < \\varepsilon$时，为异常。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开发和评价异常检测系统\n",
    "\n",
    "异常检测算法是一个非监督学习算法，意味着我们无法根据结果变量 $ y$ 的值来告诉我们数据是否真的是异常的。我们需要另一种方法来帮助检验算法是否有效。当我们开发一个异常检测系统时，我们从带标记（异常或正常）的数据着手，我们从其中选择一部分正常数据用于构建训练集，然后用剩下的正常数据和异常数据混合的数据构成交叉检验集和测试集。\n",
    "\n",
    "例如：我们有10000台正常引擎的数据，有20台异常引擎的数据。 我们这样分配数据：\n",
    "\n",
    "6000台正常引擎的数据作为训练集\n",
    "\n",
    "2000台正常引擎和10台异常引擎的数据作为交叉检验集\n",
    "\n",
    "2000台正常引擎和10台异常引擎的数据作为测试集\n",
    "\n",
    "具体的评价方法如下：\n",
    "\n",
    "1. 根据测试集数据，我们估计特征的平均值和方差并构建$p(x)$函数\n",
    "\n",
    "2. 对交叉检验集，我们尝试使用不同的$\\varepsilon$值作为阀值，并预测数据是否异常，根据$F1$值或者查准率与查全率的比例来选择 $\\varepsilon$\n",
    "\n",
    "3. 选出 $\\varepsilon​$ 后，针对测试集进行预测，计算异常检验系统的$F1​$值，或者查准率与查全率之比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 异常检测和监督学习的比较\n",
    "\n",
    "| 异常检测                                | 监督学习                                     |\n",
    "| ----------------------------------- | ---------------------------------------- |\n",
    "| 非常少量的正向类（异常数据 $y=1$）, 大量的负向类（$y=0$） | 同时有大量的正向类和负向类                            |\n",
    "| 许多不同种类的异常，非常难。根据非常 少量的正向类数据来训练算法。   | 有足够多的正向类实例，足够用于训练 算法，未来遇到的正向类实例可能与训练集中的非常近似。 |\n",
    "| 未来遇到的异常可能与已掌握的异常、非常的不同。             |                                          |\n",
    "| 例如： 欺诈行为检测 生产（例如飞机引擎）检测数据中心的计算机运行状况 | 例如：邮件过滤器 天气预报 肿瘤分类                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多元高斯分布\n",
    "\n",
    "在一般的高斯分布模型中，我们计算$ p(x) $的方法是：\n",
    "\n",
    "通过分别计算每个特征对应的几率然后将其累乘起来，在多元高斯分布模型中，我们将构建特征的协方差矩阵，用所有的特征一起来计算$ p(x)$。\n",
    "\n",
    "我们首先计算所有特征的平均值，然后再计算协方差矩阵：\n",
    "\n",
    "$ p(x)=\\prod_{j=1}^np(x_j;\\mu,\\sigma_j^2)=\\prod_{j=1}^n\\frac{1}{\\sqrt{2\\pi}\\sigma_j}exp(-\\frac{(x_j-\\mu_j)^2}{2\\sigma_j^2})$\n",
    "\n",
    "$\\mu=\\frac{1}{m}\\sum_{i=1}^mx^{(i)}$\n",
    "\n",
    "$\\Sigma = \\frac{1}{m}\\sum_{i=1}^m(x^{(i)}-\\mu)(x^{(i)}-\\mu)^T=\\frac{1}{m}(X-\\mu)^T(X-\\mu)$\n",
    "\n",
    "注:其中$\\mu$  是一个向量，其每一个单元都是原特征矩阵中一行数据的均值。最后我们计算多元高斯分布的$p\\left( x \\right)$:\n",
    "\n",
    "$p(x)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "多元高斯分布模型与原高斯分布模型的关系：\n",
    "\n",
    "可以证明的是，原本的高斯分布模型是多元高斯分布模型的一个子集，即像上图中的第1、2、3，3个例子所示，如果协方差矩阵只在对角线的单位上有非零的值时，即为原本的高斯分布模型了。\n",
    "\n",
    "原高斯分布模型和多元高斯分布模型的比较：\n",
    "\n",
    "| 原高斯分布模型                                               | 多元高斯分布模型                                             |\n",
    "| ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| 不能捕捉特征之间的相关性 但可以通过将特征进行组合的方法来解决 | 自动捕捉特征之间的相关性                                     |\n",
    "| 计算代价低，能适应大规模的特征                               | 计算代价较高 训练集较小时也同样适用                          |\n",
    "|                                                              | 必须要有 $m>n$，不然的话协方差矩阵$\\Sigma$不可逆的，通常需要 $m>10n​$ 另外特征冗余也会导致协方差矩阵不可逆 |\n",
    "\n",
    "原高斯分布模型被广泛使用着，如果特征之间在某种程度上存在相互关联的情况，我们可以通过构造新新特征的方法来捕捉这些相关性。\n",
    "\n",
    "如果训练集不是太大，并且没有太多的特征，我们可以使用多元高斯分布模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# sns.set(context=\"notebook\", style=\"white\", palette=sns.color_palette(\"RdBu\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You want to divide data into 3 set. \n",
    "1. Training set\n",
    "2. Cross Validation set\n",
    "3. Test set.  \n",
    "\n",
    "You shouldn't be doing prediction using training data or Validation data as it does in the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = sio.loadmat('./data/ex8data1.mat')\n",
    "mat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mat.get('X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat.get('yval').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "divide original validation data into validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xval, Xtest, yval, ytest = train_test_split(mat.get('Xval'),\n",
    "                                            mat.get('yval').ravel(),\n",
    "                                            test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot('Latency', 'Throughput',\n",
    "           data=pd.DataFrame(X, columns=['Latency', 'Throughput']), \n",
    "           fit_reg=False,\n",
    "           scatter_kws={\"s\":20,\n",
    "                        \"alpha\":0.5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 协方差矩阵对多元高斯模型的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "def plt_multi_gauss(u, sigma):\n",
    "    def multi_gauss(x1, x2):\n",
    "        xx = np.stack((x1.ravel(), x2.ravel()), axis=1)\n",
    "        z = []\n",
    "        for x in xx:\n",
    "            z.append(1/(2*np.pi*np.linalg.det(sigma)**0.5) * np.exp(-0.5 * (x - u).T @ np.linalg.inv(sigma) @ (x-u) ))\n",
    "        z = np.array(z).reshape(x1.shape)\n",
    "        return z\n",
    "    x = np.linspace(-3, 3, 100)\n",
    "    y = np.linspace(-3, 3, 100)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    z = multi_gauss(xx, yy)\n",
    "    fig = plt.figure()\n",
    "#     ax = fig.gca(projection='3d')\n",
    "#     ax.plot_surface(xx, yy, z, rstride=1, cstride=1, cmap=plt.get_cmap('rainbow'))\n",
    "    plt.contourf(xx, yy, z, 10, alpha=.75, cmap=plt.cm.hot)\n",
    "    c = plt.contour(xx, yy, z, colors='black')\n",
    "    plt.clabel(c, inline=True, fontsize=10)\n",
    "    plt.axis('scaled')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = np.array([0, 0])\n",
    "sigma = np.array([[1, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_multi_gauss(u, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 令特征1拥有较小的偏差，同时保持特征2的偏差\n",
    "sigma = np.array([[0.6, 0], [0, 1]])\n",
    "plt_multi_gauss(u, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 令特征2拥有较大的偏差，同时保持特征1的偏差\n",
    "sigma = np.array([[1, 0], [0, 2]])\n",
    "plt_multi_gauss(u, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在不改变两个特征的原有偏差的基础上，增加两者之间的正相关性\n",
    "sigma = np.array([[1, 0.8], [0.8, 1]])\n",
    "plt_multi_gauss(u, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在不改变两个特征的原有偏差的基础上，增加两者之间的负相关性\n",
    "sigma = np.array([[1, -0.8], [-0.8, 1]])\n",
    "plt_multi_gauss(u, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# estimate multivariate Gaussian parameters $\\mu$ and $\\sigma^2$\n",
    "> according to data, X1, and X2 is not independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = X.mean(axis=0)\n",
    "print(mu, '\\n')\n",
    "\n",
    "cov = np.cov(X.T)  # 协方差矩阵\n",
    "print(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Sigma = \\frac{1}{m}\\sum_{i=1}^m(x^{(i)}-\\mu)(x^{(i)}-\\mu)^T=\\frac{1}{m}(X-\\mu)^T(X-\\mu)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of creating 2d grid to calculate probability density\n",
    "np.dstack(np.mgrid[0:3,0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dstack?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create multi-var Gaussian model 多元高斯模型\n",
    "multi_normal = stats.multivariate_normal(mu, cov)\n",
    "\n",
    "# create a grid\n",
    "x, y = np.mgrid[0:30:0.01, 0:30:0.01]\n",
    "pos = np.dstack((x, y))\n",
    "\n",
    "# x = np.linspace(-3, 3, 100)\n",
    "# y = np.linspace(-3, 3, 100)\n",
    "# xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot probability density\n",
    "ax.contourf(x, y, multi_normal.pdf(pos), cmap='Blues')\n",
    "\n",
    "# plot original data points\n",
    "sns.regplot('Latency', 'Throughput',\n",
    "           data=pd.DataFrame(X, columns=['Latency', 'Throughput']), \n",
    "           fit_reg=False,\n",
    "           ax=ax,\n",
    "           scatter_kws={\"s\":10,\n",
    "                        \"alpha\":0.4})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# select threshold $\\epsilon$\n",
    "1. use training set $X$ to model the multivariate Gaussian\n",
    "2. use cross validation set $(Xval, yval)$ to find the best $\\epsilon$ by finding the best `F-score`\n",
    "\n",
    "\n",
    "![a](./F1-score.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_threshold(X, Xval, yval):\n",
    "    \"\"\"use CV data to find the best epsilon\n",
    "    Returns:\n",
    "        e: best epsilon with the highest f-score\n",
    "        f-score: such best f-score\n",
    "    \"\"\"\n",
    "    # create multivariate model using training data\n",
    "    mu = X.mean(axis=0)\n",
    "    cov = np.cov(X.T)\n",
    "    multi_normal = stats.multivariate_normal(mu, cov)\n",
    "\n",
    "    # this is key, use CV data for fine tuning hyper parameters\n",
    "    pval = multi_normal.pdf(Xval)\n",
    "\n",
    "    # set up epsilon candidates\n",
    "    epsilon = np.linspace(np.min(pval), np.max(pval), num=10000)\n",
    "\n",
    "    # calculate f-score\n",
    "    fs = []\n",
    "    for e in epsilon:\n",
    "        y_pred = (pval <= e).astype('int')\n",
    "        fs.append(f1_score(yval, y_pred))\n",
    "\n",
    "    # find the best f-score\n",
    "    argmax_fs = np.argmax(fs)\n",
    "\n",
    "    return epsilon[argmax_fs], fs[argmax_fs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, fs = select_threshold(X, Xval, yval)\n",
    "print('Best epsilon: {}\\nBest F-score on validation data: {}'.format(e, fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize prediction of `Xval` using learned $\\epsilon$\n",
    "1. use CV data to find the best $\\epsilon$\n",
    "2. use all data (training + validation) to create model\n",
    "3. do the prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_threshold(X, Xval, yval):\n",
    "    \"\"\"use CV data to find the best epsilon\n",
    "    Returns:\n",
    "        e: best epsilon with the highest f-score\n",
    "        f-score: such best f-score\n",
    "    \"\"\"\n",
    "    # create multivariate model using training data\n",
    "    mu = X.mean(axis=0)\n",
    "    cov = np.cov(X.T)\n",
    "    multi_normal = stats.multivariate_normal(mu, cov)\n",
    "\n",
    "    # this is key, use CV data for fine tuning hyper parameters\n",
    "    pval = multi_normal.pdf(Xval)\n",
    "\n",
    "    # set up epsilon candidates\n",
    "    epsilon = np.linspace(np.min(pval), np.max(pval), num=10000)\n",
    "\n",
    "    # calculate f-score\n",
    "    fs = []\n",
    "    for e in epsilon:\n",
    "        y_pred = (pval <= e).astype('int')\n",
    "        fs.append(f1_score(yval, y_pred))\n",
    "\n",
    "    # find the best f-score\n",
    "    argmax_fs = np.argmax(fs)\n",
    "\n",
    "    return epsilon[argmax_fs], fs[argmax_fs]\n",
    "\n",
    "\n",
    "def predict(X, Xval, e, Xtest, ytest):\n",
    "    \"\"\"with optimal epsilon, combine X, Xval and predict Xtest\n",
    "    Returns:\n",
    "        multi_normal: multivariate normal model\n",
    "        y_pred: prediction of test data\n",
    "    \"\"\"\n",
    "    Xdata = np.concatenate((X, Xval), axis=0)\n",
    "\n",
    "    mu = Xdata.mean(axis=0)\n",
    "    cov = np.cov(Xdata.T)\n",
    "    multi_normal = stats.multivariate_normal(mu, cov)\n",
    "\n",
    "    # calculate probability of test data\n",
    "    pval = multi_normal.pdf(Xtest)\n",
    "    y_pred = (pval <= e).astype('int')\n",
    "\n",
    "    print(classification_report(ytest, y_pred))\n",
    "\n",
    "    return multi_normal, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_normal, y_pred = predict(X, Xval, e, Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct test DataFrame\n",
    "data = pd.DataFrame(Xtest, columns=['Latency', 'Throughput'])\n",
    "data['y_pred'] = y_pred\n",
    "\n",
    "# create a grid for graphing\n",
    "x, y = np.mgrid[0:30:0.01, 0:30:0.01]\n",
    "pos = np.dstack((x, y))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot probability density\n",
    "ax.contourf(x, y, multi_normal.pdf(pos), cmap='Blues')\n",
    "\n",
    "# plot original Xval points\n",
    "sns.regplot('Latency', 'Throughput',\n",
    "            data=data,\n",
    "            fit_reg=False,\n",
    "            ax=ax,\n",
    "            scatter_kws={\"s\":10,\n",
    "                         \"alpha\":0.4})\n",
    "\n",
    "# mark the predicted anamoly of CV data. We should have a test set for this...\n",
    "anamoly_data = data[data['y_pred']==1]\n",
    "ax.scatter(anamoly_data['Latency'], anamoly_data['Throughput'], marker='x', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# high dimension data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = sio.loadmat('./data/ex8data2.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mat.get('X')\n",
    "Xval, Xtest, yval, ytest = train_test_split(mat.get('Xval'),\n",
    "                                            mat.get('yval').ravel(),\n",
    "                                            test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, fs = select_threshold(X, Xval, yval)\n",
    "print('Best epsilon: {}\\nBest F-score on validation data: {}'.format(e, fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_normal, y_pred = predict(X, Xval, e, Xtest, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('find {} anamolies'.format(y_pred.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The huge difference between my result, and the official `117` anamolies in the ex8 is due to:\n",
    "1. my use of **multivariate Gaussian**\n",
    "2. I split data very differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(n_features=2, centers=3, n_samples=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用密度估计的异常检测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors.kde import KernelDensity\n",
    "np.set_printoptions(precision=4, threshold=16, suppress=True)\n",
    "\n",
    "\n",
    "# 使用高斯核密度估计\n",
    "kde = KernelDensity(kernel='gaussian')\n",
    "kde.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kde_X = kde.score_samples(X)\n",
    "kde_X  # 对数似然, 越小样本越罕见"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import mquantiles\n",
    "alpha_set = 0.95\n",
    "tau_kde = mquantiles(kde_X, 1. - alpha_set)  # 0.05分位时的score值\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "X_range = np.zeros((n_features, 2))\n",
    "X_range[:, 0] = np.min(X, axis=0) - 1.\n",
    "X_range[:, 1] = np.max(X, axis=0) + 1.\n",
    "\n",
    "h = 0.1  # step size of the mesh\n",
    "x_min, x_max = X_range[0]\n",
    "y_min, y_max = X_range[1]\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "Z_kde = kde.score_samples(grid)\n",
    "Z_kde = Z_kde.reshape(xx.shape)\n",
    "\n",
    "plt.figure()\n",
    "c_0 = plt.contour(xx, yy, Z_kde, levels=tau_kde, colors='blue', linewidths=3)\n",
    "plt.clabel(c_0, inline=1, fontsize=15, fmt={tau_kde[0]: str(alpha_set)})\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单类SVM\n",
    "---\n",
    "基于密度的估计的问题在于，当数据的维数增加时，它们往往变得低效。 这就是所谓的维度灾难，尤其会影响密度估算算法。 在这种情况下可以使用单类 SVM 算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "nu = 0.05  # 理论上说这应该是离群值的上限, 训练误差的上线\n",
    "ocsvm = OneClassSVM(kernel='rbf', gamma=0.05, nu=nu)  # Unsupervised Outlier Detection.\n",
    "ocsvm.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_outliers = X[ocsvm.predict(X) == -1]\n",
    "\n",
    "Z_ocsvm = ocsvm.decision_function(grid)\n",
    "Z_ocsvm = Z_ocsvm.reshape(xx.shape)\n",
    "\n",
    "plt.figure()\n",
    "c_0 = plt.contour(xx, yy, Z_ocsvm, levels=[0], colors='blue', linewidths=3)\n",
    "plt.clabel(c_0, inline=1, fontsize=15, fmt={0: str(alpha_set)})\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.scatter(X_outliers[:, 0], X_outliers[:, 1], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持向量 - 离群点\n",
    "---\n",
    "所谓的单类 SVM 的支持向量形成离群点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SV = X[ocsvm.support_]\n",
    "n_SV = len(X_SV)\n",
    "n_outliers = len(X_outliers)\n",
    "\n",
    "print('{0:.2f} <= {1:.2f} <= {2:.2f}?'.format(1./n_samples*n_outliers, nu, 1./n_samples*n_SV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只有支持向量涉及单类 SVM 的决策函数。\n",
    "\n",
    "- 绘制单类 SVM 决策函数的级别集，就像我们对真实密度所做的那样。\n",
    "- 突出支持向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.contourf(xx, yy, Z_ocsvm, 10, cmap=plt.cm.Blues_r)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=1.)\n",
    "plt.scatter(X_SV[:, 0], X_SV[:, 1], color='orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 隔离森林\n",
    "---\n",
    "隔离森林是一种基于树的异常检测算法。 该算法构建了许多随机树，其基本原理是，如果样本被隔离，在非常少量的随机分割之后，它应该单独存在于叶子中。 隔离森林根据样本最终所在的树的深度建立异常得分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = digits.images\n",
    "labels = digits.target\n",
    "images.shape  # 8*8图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 102\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.title('{0}'.format(labels[i]))\n",
    "plt.axis('off')\n",
    "plt.imshow(images[i], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将图片展开 \n",
    "n_samples = len(images)\n",
    "data = images.reshape((n_samples, -1))\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data\n",
    "y = labels\n",
    "# 数字5\n",
    "X_5 = X[y == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(10, 4))\n",
    "for ax, x in zip(axes, X_5[:5]):\n",
    "    img = x.reshape(8, 8)\n",
    "    ax.imshow(img, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用IsolationForest来查找前 5% 最异常的图像。\n",
    "iforest = IsolationForest(contamination=0.05, behaviour=\"new\")\n",
    "iforest.fit(X_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用iforest.decision_function计算“异常”的级别。越低就越异常。\n",
    "iforest_X = iforest.decision_function(X_5)  \n",
    "plt.hist(iforest_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最正常的10个值\n",
    "X_strong_inliers = X_5[np.argsort(iforest_X)[-10:]]\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(X_strong_inliers[i].reshape((8, 8)),\n",
    "               cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最异常的值\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "\n",
    "X_outliers = X_5[iforest.predict(X_5) == -1]\n",
    "\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.imshow(X_outliers[i].reshape((8, 8)),\n",
    "               cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.axis('off')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
