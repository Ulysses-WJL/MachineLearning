{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# np.set_printoptions(precision=4, threshold=15,suppress=True)\n",
    "pd.options.display.max_rows = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**多项式朴素贝叶斯分类器**  \n",
    "多项式分布(Multinomial Distribution)是二项式分布的推广。二项式做n次伯努利实验，规定了每次试验的结果只有两个，如果现在还是做n次试验，只不过每次试验的结果可以有多m个，且m个结果发生的概率互斥且和为1，则发生其中一个结果X次的概率就是多项式分布。  \n",
    "\n",
    "多项式朴素贝叶斯的工作方式类似于高斯朴素贝叶斯，但假设这些特征是多项式分布的。 在实践中，这意味着当我们具有离散数据（例如，电影评级范围为 1 到 5）时，通常使用该分类器。\n",
    "\n",
    "分布参数由每类 y 的 $\\theta_y = (\\theta_{y_1},\\ldots,\\theta_{y_n})$ 向量决定， 式中 n 是特征的数量(对于文本分类，是词汇量的大小) $\\theta_{y_i}$ 是样本中属于类 y 中特征 i 概率 $P(x_i \\mid y)$ 。\n",
    "\n",
    "参数 $\\theta_y$ 使用平滑过的最大似然估计法来估计，即相对频率计数:\n",
    "\n",
    "$$\\hat{\\theta}{y_i} = \\frac{ N{y_i} + \\alpha}{N_y + \\alpha n}$$\n",
    "\n",
    "式中$N_{y_i} = \\sum_{x \\in T} x_i$是 训练集T中特征i在类y中出现的次数，$N_{y} = \\sum_{i=1}^{|T|} N_{y_i}$ 是类 y 中出现所有特征的计数总和。\n",
    "\n",
    "先验平滑因子 $\\alpha \\ge 0$ 为在学习样本中没有出现的特征而设计，以防在将来的计算中出现0概率输出。 把 $\\alpha = 1$ 被称为拉普拉斯平滑(Lapalce smoothing)，而 $\\alpha < 1$ 被称为Lidstone平滑方法(Lidstone smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯的文本分类\n",
    "---\n",
    "使用朴素贝叶斯进行文本分类；我们将有一组带有相应类别的文本文档，我们将训练一个朴素贝叶斯算法，来学习预测新的没见过的实例的类别。这项简单的任务有许多实际应用；可能是最知名和广泛使用的**垃圾邮件过滤**。在本节中，我们将尝试使用可以从 scikit-learn 中检索的数据集，对新闻组消息进行分类。该数据集包括来自 20 个不同主题的大约 19,000 条新闻组信息，从政治和宗教到体育和科学。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer  # 文本特征提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fetch_20newsgroups(data_home=None, subset=’train’, categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True)\n",
    "\n",
    "- data_home指的是数据集的地址，如果默认的话，所有的数据都会在'~/scikit_learn_data'文件夹下.   \n",
    "    cd \\site-packages\\sklearn\\datasets打开twenty_newsgroups.py文件 修改`archive_path`\n",
    "- subset就是train,test,all三种可选，分别对应训练集、测试集和所有样本。 \n",
    "- categories:是指类别，如果指定类别，就会只提取出目标类，如果是默认，则是提取所有类别出来。 \n",
    "- shuffle:是否打乱样本顺序，如果是相互独立的话。 \n",
    "- random_state:打乱顺序的随机种子 \n",
    "- remove:是一个元组，用来去除一些停用词的，例如标题引用之类的。 \n",
    "- download_if_missing: 如果数据缺失，是否去下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = fetch_20newsgroups(subset='all')  # 导入所有实例 [‘train’ or ‘test’, ‘all’, optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回的数据bunch  \n",
    "* bunch.data: list, length [n_samples]\n",
    "* bunch.target: array, shape [n_samples]\n",
    "* bunch.filenames: list, length [n_samples]\n",
    "* bunch.DESCR: a description of the dataset.\n",
    "* bunch.target_names: a list of categories of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, numpy.ndarray, list)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(news.data), type(news.target), type(news.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target_names  # 新闻的类别名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\\nSubject: Pens fans reactions\\nOrganization: Post Office, Carnegie Mellon, Pittsburgh, PA\\nLines: 12\\nNNTP-Posting-Host: po4.andrew.cmu.edu\\n\\n\\n\\nI am sure some bashers of Pens fans are pretty confused about the lack\\nof any kind of posts about the recent Pens massacre of the Devils. Actually,\\nI am  bit puzzled too and a bit relieved. However, I am going to put an end\\nto non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\\nare killing those Devils worse than I thought. Jagr just showed you why\\nhe is much better than his regular season stats. He is also a lot\\nfo fun to watch in the playoffs. Bowman should let JAgr have a lot of\\nfun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\\nregular season game.          PENS RULE!!!\\n\\n\",\n",
       " 'rec.sport.hockey')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.data[0], news.target_names[news.target[0]]  # 查看第一个实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理数据\n",
    "---\n",
    "我们的机器学习算法只能用于数字数据，因此我们的下一步是将基于文本的数据集转换为数字数据集。目前我们只有一个特征，即消息的文本内容；我们需要一些函数，将文本转换为一组有意义的数字特征。直观地，我们可以尝试查看每个文本类别中使用的单词（或更确切地说，标记，包括数字或标点符号），并尝试表示每个类别中每个单词的频率分布。\n",
    "\n",
    "scikit-learn提供了从文本内容中提取数字特征的最常见方法，即：\n",
    "\n",
    "*   **令牌化（tokenizing）** 对每个可能的词令牌分成字符串并赋予整数形的id，例如通过使用空格和标点符号作为令牌分隔符。\n",
    "*   **统计（counting）** 每个词令牌在文档中的出现次数。\n",
    "*   **标准化（normalizing）** 在大多数的文档 / 样本中，可以减少重要的次令牌的出现次数的权重。。\n",
    "\n",
    "在该方案中，特征和样本定义如下：\n",
    "\n",
    "*   每个**单独的令牌发生频率**（标准化或不标准化）被视为一个**特征**。\n",
    "*   给定**文档**中所有的令牌频率向量被看做一个多元sample**样本**。\n",
    "\n",
    "因此，文本的集合可被表示为矩阵形式，每行对应一条文本，每列对应每个文本中出现的词令牌(如单个词)。\n",
    "\n",
    "我们称**向量化**是将文本文档集合转换为数字集合特征向量的普通方法。 这种特殊思想（令牌化，计数和归一化）被称为 **Bag of Words** 或 “Bag of n-grams” 模型。 文档由单词出现来描述，同时完全忽略文档中单词的**相对位置信息**。\n",
    "\n",
    "`sklearn.feature_extraction.text`模块具有一些有用的工具，可以从文本文档构建数字特征向量:\n",
    "`CountVectorizer`，`HashingVectorizer`和`TfidfVectorizer`。它们之间的区别在于它们为获得数字特征而执行的计算。\n",
    "- `CountVectorizer`基本上从文本语料库中创建单词词典。然后，将每个实例转换为数字特征的向量，其中每个元素将是特定单词在文档中出现的次数的计数。\n",
    "\n",
    "- `HashingVectorizer`，则是在内存中限制并维护字典，实现了 将标记映射到特征索引的散列函数，然后计算`CountVectorizer`中的计数。\n",
    "\n",
    "- `TfidfVectorizer`的工作方式与`CountVectorizer`类似，但更高级的计算称为**词语频率逆文档频率（TF-IDF）**。这是用于测量在文档或语料库中单词的重要性的统计量。直观地说，它在当前文档中查找中更频繁的单词，与它们在整个文档集中的频率的比值。您可以将此视为一种方法，标准化结果并避免单词过于频繁而无法用于表征实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train, news_test, target_train, traget_test = train_test_split(news.data, news.target, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对简约的文本语料库进行 tokenize（分词）和统计单词出现频数\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This is the second second document.',\n",
    "     'And the third one.',\n",
    "     'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认配置通过提取至少 2 个字母的单词来对 string 进行分词。做这一步的函数可以显式地被调用:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.feature_extraction.text.VectorizerMixin.build_analyzer.<locals>.<lambda>(doc)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'text', 'document', 'to', 'analyze']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze(\"This is a text document to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyzer 在拟合过程中找到的每个 term（项）都会被分配一个唯一的整数索引，对应于 resulting matrix（结果矩阵）中的一列。此列的一些说明可以被检索如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())  # 每行为一个向量, 每一列代表这个feature是否出现和次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 feature 名称到 column index（列索引） 的逆映射存储在 vocabulary_ 属性中:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 8,\n",
       " 'is': 3,\n",
       " 'the': 6,\n",
       " 'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 5,\n",
       " 'and': 0,\n",
       " 'third': 7,\n",
       " 'one': 4}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，在未来对 transform 方法的调用中，在 training corpus （训练语料库）中没有看到的单词将被完全忽略:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform([\"what does't kill you makes you stronger\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，在前面的 corpus（语料库）中，第一个和最后一个文档具有完全相同的词，因为被编码成相同的向量。 特别是我们丢失了最后一个文件是一个疑问的形式的信息。为了防止词组顺序颠倒，除了提取一元模型 1-grams（个别词）之外，我们还可以提取 2-grams 的单词:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)  # \\b 匹配空字符串但只在单词开始或结尾的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('Bi-grams are cool!')  # 除了一元模型, 还有2个词连着取的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由 vectorizer（向量化器）提取的 vocabulary（词汇）因此会变得更大，同时可以在定位模式时消除歧义:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_index = bigram_vectorizer.vocabulary_.get('is this')\n",
    "feature_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2[:, feature_index]  # 只在最后一行出现"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
