{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# np.set_printoptions(precision=4, threshold=15,suppress=True)\n",
    "pd.options.display.max_rows = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**多项式朴素贝叶斯分类器**  \n",
    "多项式分布(Multinomial Distribution)是二项式分布的推广。二项式做n次伯努利实验，规定了每次试验的结果只有两个，如果现在还是做n次试验，只不过每次试验的结果可以有多m个，且m个结果发生的概率互斥且和为1，则发生其中一个结果X次的概率就是多项式分布。  \n",
    "\n",
    "多项式朴素贝叶斯的工作方式类似于高斯朴素贝叶斯，但假设这些特征是多项式分布的。 在实践中，这意味着当我们具有离散数据（例如，电影评级范围为 1 到 5）时，通常使用该分类器。\n",
    "\n",
    "分布参数由每类 y 的 $\\theta_y = (\\theta_{y_1},\\ldots,\\theta_{y_n})$ 向量决定， 式中 n 是特征的数量(对于文本分类，是词汇量的大小) $\\theta_{y_i}$ 是样本中属于类 y 中特征 i 概率 $P(x_i \\mid y)$ 。\n",
    "\n",
    "参数 $\\theta_y$ 使用平滑过的最大似然估计法来估计，即相对频率计数:\n",
    "\n",
    "$$\\hat{\\theta}{y_i} = \\frac{ N{y_i} + \\alpha}{N_y + \\alpha n}$$\n",
    "\n",
    "式中$N_{y_i} = \\sum_{x \\in T} x_i$是 训练集T中特征i在类y中出现的次数，$N_{y} = \\sum_{i=1}^{|T|} N_{y_i}$ 是类 y 中出现所有特征的计数总和。\n",
    "\n",
    "先验平滑因子 $\\alpha \\ge 0$ 为在学习样本中没有出现的特征而设计，以防在将来的计算中出现0概率输出。 把 $\\alpha = 1$ 被称为拉普拉斯平滑(Lapalce smoothing)，而 $\\alpha < 1$ 被称为Lidstone平滑方法(Lidstone smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯的文本分类\n",
    "---\n",
    "使用朴素贝叶斯进行文本分类；我们将有一组带有相应类别的文本文档，我们将训练一个朴素贝叶斯算法，来学习预测新的没见过的实例的类别。这项简单的任务有许多实际应用；可能是最知名和广泛使用的**垃圾邮件过滤**。在本节中，我们将尝试使用可以从 scikit-learn 中检索的数据集，对新闻组消息进行分类。该数据集包括来自 20 个不同主题的大约 19,000 条新闻组信息，从政治和宗教到体育和科学。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfTransformer # 文本特征提取\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fetch_20newsgroups(data_home=None, subset=’train’, categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True)\n",
    "\n",
    "- data_home指的是数据集的地址，如果默认的话，所有的数据都会在'~/scikit_learn_data'文件夹下.   \n",
    "    cd \\site-packages\\sklearn\\datasets打开twenty_newsgroups.py文件 修改`archive_path`\n",
    "- subset就是train,test,all三种可选，分别对应训练集、测试集和所有样本。 \n",
    "- categories:是指类别，如果指定类别，就会只提取出目标类，如果是默认，则是提取所有类别出来。 \n",
    "- shuffle:是否打乱样本顺序，如果是相互独立的话。 \n",
    "- random_state:打乱顺序的随机种子 \n",
    "- remove:是一个元组，用来去除一些停用词的，例如标题引用之类的。 \n",
    "- download_if_missing: 如果数据缺失，是否去下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "news_train = fetch_20newsgroups(subset='train')  #  [‘train’ or ‘test’, ‘all’, optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(list, numpy.ndarray, list)"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "type(news_train.data), type(news_train.target), type(news_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "11314"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "len(news_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(11314, 7532)"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "len(news_train.target), len(news_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回的数据bunch  \n",
    "* bunch.data: list, length [n_samples]\n",
    "* bunch.target: array, shape [n_samples]\n",
    "* bunch.filenames: list, length [n_samples]\n",
    "* bunch.DESCR: a description of the dataset.\n",
    "* bunch.target_names: a list of categories of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "news_train.target_names  # 新闻的类别名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\",\n 'rec.autos')"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "news_train.data[0], news_train.target_names[news_train.target[0]]  # 查看第一个实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理数据\n",
    "---\n",
    "我们的机器学习算法只能用于数字数据，因此我们的下一步是将基于文本的数据集转换为数字数据集。目前我们只有一个特征，即消息的文本内容；我们需要一些函数，将文本转换为一组有意义的数字特征。直观地，我们可以尝试查看每个文本类别中使用的单词（或更确切地说，标记，包括数字或标点符号），并尝试表示每个类别中每个单词的频率分布。\n",
    "\n",
    "scikit-learn提供了从文本内容中提取数字特征的最常见方法，即：\n",
    "\n",
    "*   **令牌化（tokenizing）** 对每个可能的词令牌分成字符串并赋予整数形的id，例如通过使用空格和标点符号作为令牌分隔符。\n",
    "*   **统计（counting）** 每个词令牌在文档中的出现次数。\n",
    "*   **标准化（normalizing）** 在大多数的文档 / 样本中，可以减少重要的次令牌的出现次数的权重。。\n",
    "\n",
    "在该方案中，特征和样本定义如下：\n",
    "\n",
    "*   每个**单独的令牌发生频率**（标准化或不标准化）被视为一个**特征**。\n",
    "*   给定**文档**中所有的令牌频率向量被看做一个多元sample**样本**。\n",
    "\n",
    "因此，文本的集合可被表示为矩阵形式，每行对应一条文本，每列对应每个文本中出现的词令牌(如单个词)。\n",
    "\n",
    "我们称**向量化**是将文本文档集合转换为数字集合特征向量的普通方法。 这种特殊思想（令牌化，计数和归一化）被称为 **Bag of Words** 词袋 或 “Bag of n-grams” 模型。 文档由单词出现来描述，同时完全忽略文档中单词的**相对位置信息**。词袋模型是典型的 high-dimensional sparse datasets（**高维稀疏数据集**） 。 我们可以通过只在内存中保存特征向量中非 0 的部分以节省大量内存。\n",
    "\n",
    "scipy.sparse 矩阵正是能完成上述操作的数据结构，同时 scikit-learn 有对这样的数据结构的内置支持。\n",
    "\n",
    "\n",
    "`sklearn.feature_extraction.text`模块具有一些有用的工具，可以从文本文档构建数字特征向量:\n",
    "`CountVectorizer`，`HashingVectorizer`和`TfidfVectorizer`。它们之间的区别在于它们为获得数字特征而执行的计算。\n",
    "- `CountVectorizer`基本上从文本语料库中创建单词词典。然后，将每个实例转换为数字特征的向量，其中每个元素将是特定单词在文档中出现的次数的计数。\n",
    "\n",
    "- `HashingVectorizer`，则是在内存中限制并维护字典，实现了 将标记映射到特征索引的散列函数，然后计算`CountVectorizer`中的计数。\n",
    "\n",
    "- `TfidfVectorizer`的工作方式与`CountVectorizer`类似，但更高级的计算称为**词语频率逆文档频率（TF-IDF）**。这是用于测量在文档或语料库中单词的重要性的统计量。直观地说，它在当前文档中查找中更频繁的单词，与它们在整个文档集中的频率的比值。您可以将此视为一种方法，标准化结果并避免单词过于频繁而无法用于表征实例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdecode_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'strict'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpreprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtoken_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'(?u)\\\\b\\\\w\\\\w+\\\\b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'numpy.int64'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nConvert a collection of text documents to a matrix of token counts\n\nThis implementation produces a sparse representation of the counts using\nscipy.sparse.csr_matrix.\n\nIf you do not provide an a-priori dictionary and you do not use an analyzer\nthat does some kind of feature selection then the number of features will\nbe equal to the vocabulary size found by analyzing the data.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n\nParameters\n----------\ninput : string {'filename', 'file', 'content'}, default='content'\n    If 'filename', the sequence passed as an argument to fit is\n    expected to be a list of filenames that need reading to fetch\n    the raw content to analyze.\n\n    If 'file', the sequence items must have a 'read' method (file-like\n    object) that is called to fetch the bytes in memory.\n\n    Otherwise the input is expected to be a sequence of items that\n    can be of type string or byte.\n\nencoding : string, default='utf-8'\n    If bytes or files are given to analyze, this encoding is used to\n    decode.\n\ndecode_error : {'strict', 'ignore', 'replace'}, default='strict'\n    Instruction on what to do if a byte sequence is given to analyze that\n    contains characters not of the given `encoding`. By default, it is\n    'strict', meaning that a UnicodeDecodeError will be raised. Other\n    values are 'ignore' and 'replace'.\n\nstrip_accents : {'ascii', 'unicode'}, default=None\n    Remove accents and perform other character normalization\n    during the preprocessing step.\n    'ascii' is a fast method that only works on characters that have\n    an direct ASCII mapping.\n    'unicode' is a slightly slower method that works on any characters.\n    None (default) does nothing.\n\n    Both 'ascii' and 'unicode' use NFKD normalization from\n    :func:`unicodedata.normalize`.\n\nlowercase : bool, default=True\n    Convert all characters to lowercase before tokenizing.\n\npreprocessor : callable, default=None\n    Override the preprocessing (string transformation) stage while\n    preserving the tokenizing and n-grams generation steps.\n    Only applies if ``analyzer is not callable``.\n\ntokenizer : callable, default=None\n    Override the string tokenization step while preserving the\n    preprocessing and n-grams generation steps.\n    Only applies if ``analyzer == 'word'``.\n\nstop_words : string {'english'}, list, default=None\n    If 'english', a built-in stop word list for English is used.\n    There are several known issues with 'english' and you should\n    consider an alternative (see :ref:`stop_words`).\n\n    If a list, that list is assumed to contain stop words, all of which\n    will be removed from the resulting tokens.\n    Only applies if ``analyzer == 'word'``.\n\n    If None, no stop words will be used. max_df can be set to a value\n    in the range [0.7, 1.0) to automatically detect and filter stop\n    words based on intra corpus document frequency of terms.\n\ntoken_pattern : string\n    Regular expression denoting what constitutes a \"token\", only used\n    if ``analyzer == 'word'``. The default regexp select tokens of 2\n    or more alphanumeric characters (punctuation is completely ignored\n    and always treated as a token separator).\n\nngram_range : tuple (min_n, max_n), default=(1, 1)\n    The lower and upper boundary of the range of n-values for different\n    word n-grams or char n-grams to be extracted. All values of n such\n    such that min_n <= n <= max_n will be used. For example an\n    ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n    unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n    Only applies if ``analyzer is not callable``.\n\nanalyzer : string, {'word', 'char', 'char_wb'} or callable,             default='word'\n    Whether the feature should be made of word n-gram or character\n    n-grams.\n    Option 'char_wb' creates character n-grams only from text inside\n    word boundaries; n-grams at the edges of words are padded with space.\n\n    If a callable is passed it is used to extract the sequence of features\n    out of the raw, unprocessed input.\n\n    .. versionchanged:: 0.21\n\n    Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n    first read from the file and then passed to the given callable\n    analyzer.\n\nmax_df : float in range [0.0, 1.0] or int, default=1.0\n    When building the vocabulary ignore terms that have a document\n    frequency strictly higher than the given threshold (corpus-specific\n    stop words).\n    If float, the parameter represents a proportion of documents, integer\n    absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmin_df : float in range [0.0, 1.0] or int, default=1\n    When building the vocabulary ignore terms that have a document\n    frequency strictly lower than the given threshold. This value is also\n    called cut-off in the literature.\n    If float, the parameter represents a proportion of documents, integer\n    absolute counts.\n    This parameter is ignored if vocabulary is not None.\n\nmax_features : int, default=None\n    If not None, build a vocabulary that only consider the top\n    max_features ordered by term frequency across the corpus.\n\n    This parameter is ignored if vocabulary is not None.\n\nvocabulary : Mapping or iterable, default=None\n    Either a Mapping (e.g., a dict) where keys are terms and values are\n    indices in the feature matrix, or an iterable over terms. If not\n    given, a vocabulary is determined from the input documents. Indices\n    in the mapping should not be repeated and should not have any gap\n    between 0 and the largest index.\n\nbinary : bool, default=False\n    If True, all non zero counts are set to 1. This is useful for discrete\n    probabilistic models that model binary events rather than integer\n    counts.\n\ndtype : type, default=np.int64\n    Type of the matrix returned by fit_transform() or transform().\n\nAttributes\n----------\nvocabulary_ : dict\n    A mapping of terms to feature indices.\n\nfixed_vocabulary_: boolean\n    True if a fixed vocabulary of term to indices mapping\n    is provided by the user\n\nstop_words_ : set\n    Terms that were ignored because they either:\n\n      - occurred in too many documents (`max_df`)\n      - occurred in too few documents (`min_df`)\n      - were cut off by feature selection (`max_features`).\n\n    This is only available if no vocabulary was given.\n\nExamples\n--------\n>>> from sklearn.feature_extraction.text import CountVectorizer\n>>> corpus = [\n...     'This is the first document.',\n...     'This document is the second document.',\n...     'And this is the third one.',\n...     'Is this the first document?',\n... ]\n>>> vectorizer = CountVectorizer()\n>>> X = vectorizer.fit_transform(corpus)\n>>> print(vectorizer.get_feature_names())\n['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n>>> print(X.toarray())\n[[0 1 1 1 0 0 1 0 1]\n [0 2 0 1 0 1 1 0 1]\n [1 0 0 1 1 0 1 1 1]\n [0 1 1 1 0 0 1 0 1]]\n>>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n>>> X2 = vectorizer2.fit_transform(corpus)\n>>> print(vectorizer2.get_feature_names())\n['and this', 'document is', 'first document', 'is the', 'is this',\n'second document', 'the first', 'the second', 'the third', 'third one',\n 'this document', 'this is', 'this the']\n >>> print(X2.toarray())\n [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n [0 1 0 1 0 1 0 1 0 0 1 0 0]\n [1 0 0 1 0 0 0 0 1 1 0 1 0]\n [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n\nSee Also\n--------\nHashingVectorizer, TfidfVectorizer\n\nNotes\n-----\nThe ``stop_words_`` attribute can get large and increase the model size\nwhen pickling. This attribute is provided only for introspection and can\nbe safely removed using delattr or set to None before pickling.\n\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     TfidfVectorizer\n"
    }
   ],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mInit signature:\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nNaive Bayes classifier for multinomial models\n\nThe multinomial Naive Bayes classifier is suitable for classification with\ndiscrete features (e.g., word counts for text classification). The\nmultinomial distribution normally requires integer feature counts. However,\nin practice, fractional counts such as tf-idf may also work.\n\nRead more in the :ref:`User Guide <multinomial_naive_bayes>`.\n\nParameters\n----------\nalpha : float, default=1.0\n    Additive (Laplace/Lidstone) smoothing parameter\n    (0 for no smoothing).\n\nfit_prior : bool, default=True\n    Whether to learn class prior probabilities or not.\n    If false, a uniform prior will be used.\n\nclass_prior : array-like of shape (n_classes,), default=None\n    Prior probabilities of the classes. If specified the priors are not\n    adjusted according to the data.\n\nAttributes\n----------\nclass_count_ : ndarray of shape (n_classes,)\n    Number of samples encountered for each class during fitting. This\n    value is weighted by the sample weight when provided.\n\nclass_log_prior_ : ndarray of shape (n_classes, )\n    Smoothed empirical log probability for each class.\n\nclasses_ : ndarray of shape (n_classes,)\n    Class labels known to the classifier\n\ncoef_ : ndarray of shape (n_classes, n_features)\n    Mirrors ``feature_log_prob_`` for interpreting MultinomialNB\n    as a linear model.\n\nfeature_count_ : ndarray of shape (n_classes, n_features)\n    Number of samples encountered for each (class, feature)\n    during fitting. This value is weighted by the sample weight when\n    provided.\n\nfeature_log_prob_ : ndarray of shape (n_classes, n_features)\n    Empirical log probability of features\n    given a class, ``P(x_i|y)``.\n\nintercept_ : ndarray of shape (n_classes, )\n    Mirrors ``class_log_prior_`` for interpreting MultinomialNB\n    as a linear model.\n\nn_features_ : int\n    Number of features of each sample.\n\nExamples\n--------\n>>> import numpy as np\n>>> rng = np.random.RandomState(1)\n>>> X = rng.randint(5, size=(6, 100))\n>>> y = np.array([1, 2, 3, 4, 5, 6])\n>>> from sklearn.naive_bayes import MultinomialNB\n>>> clf = MultinomialNB()\n>>> clf.fit(X, y)\nMultinomialNB()\n>>> print(clf.predict(X[2:3]))\n[3]\n\nNotes\n-----\nFor the rationale behind the names `coef_` and `intercept_`, i.e.\nnaive Bayes as a linear classifier, see J. Rennie et al. (2003),\nTackling the poor assumptions of naive Bayes text classifiers, ICML.\n\nReferences\n----------\nC.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to\nInformation Retrieval. Cambridge University Press, pp. 234-265.\nhttps://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html\n\u001b[0;31mFile:\u001b[0m           ~/.local/lib/python3.7/site-packages/sklearn/naive_bayes.py\n\u001b[0;31mType:\u001b[0m           ABCMeta\n\u001b[0;31mSubclasses:\u001b[0m     \n"
    }
   ],
   "source": [
    "MultinomialNB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CountVectorizer()"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# 对简约的文本语料库进行 tokenize（分词）和统计单词出现频数\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This is the second second document.',\n",
    "     'And the third one.',\n",
    "     'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n\twith 19 stored elements in Compressed Sparse Row format>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(corpus)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认配置通过提取至少 2 个字母的单词来对 string 进行分词。做这一步的函数可以显式地被调用:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "functools.partial(<function _analyze at 0x7fdc78088dd0>, ngrams=<bound method _VectorizerMixin._word_ngrams of CountVectorizer()>, tokenizer=<built-in method findall of re.Pattern object at 0x7fdc748ce6b0>, preprocessor=functools.partial(<function _preprocess at 0x7fdc78072dd0>, accent_function=None, lower=True), decoder=<bound method _VectorizerMixin.decode of CountVectorizer()>, stop_words=None)"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "analyzer = vectorizer.build_analyzer()\n",
    "analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['this', 'is', 'text', 'document', 'to', 'analyze']"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "analyzer(\"This is a text document to analyze.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyzer 在拟合过程中找到的每个 term（项）都会被分配一个唯一的整数索引，对应于 resulting matrix（结果矩阵）中的一列。此列的一些说明可以被检索如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0 1 1 1 0 0 1 0 1]\n [0 1 0 1 0 2 1 0 1]\n [1 0 0 0 1 0 1 1 0]\n [0 1 1 1 0 0 1 0 1]]\n"
    }
   ],
   "source": [
    "print(X.toarray())  # 每行为一个向量, 每一列代表这个feature是否出现和次数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从 feature 名称到 column index（列索引） 的逆映射存储在 vocabulary_ 属性中:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'this': 8,\n 'is': 3,\n 'the': 6,\n 'first': 2,\n 'document': 1,\n 'second': 5,\n 'and': 0,\n 'third': 7,\n 'one': 4}"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，在未来对 transform 方法的调用中，在 training corpus （训练语料库）中没有看到的单词将被完全忽略:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "vectorizer.transform([\"what does't kill you makes you stronger\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，在前面的 corpus（语料库）中，第一个和最后一个文档具有完全相同的词，因为被编码成相同的向量。 特别是我们丢失了最后一个文件是一个疑问的形式的信息。为了防止词组顺序颠倒，除了提取一元模型 1-grams（个别词）之外，我们还可以提取 2-grams 的单词:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)  # \\b 匹配空字符串但只在单词开始或结尾的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool']"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "analyzer = bigram_vectorizer.build_analyzer()\n",
    "analyzer('Bi-grams are cool!')  # 除了一元模型, 还有2个词连着取的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由 vectorizer（向量化器）提取的 vocabulary（词汇）因此会变得更大，同时可以在定位模式时消除歧义:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n       [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n       [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]])"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "X_2 = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "X_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['and',\n 'and the',\n 'document',\n 'first',\n 'first document',\n 'is',\n 'is the',\n 'is this',\n 'one',\n 'second',\n 'second document',\n 'second second',\n 'the',\n 'the first',\n 'the second',\n 'the third',\n 'third',\n 'third one',\n 'this',\n 'this is',\n 'this the']"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "bigram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'this': 18,\n 'is': 5,\n 'the': 12,\n 'first': 3,\n 'document': 2,\n 'this is': 19,\n 'is the': 6,\n 'the first': 13,\n 'first document': 4,\n 'second': 9,\n 'the second': 14,\n 'second second': 11,\n 'second document': 10,\n 'and': 0,\n 'third': 16,\n 'one': 8,\n 'and the': 1,\n 'the third': 15,\n 'third one': 17,\n 'is this': 7,\n 'this the': 20}"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "bigram_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "7"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "feature_index = bigram_vectorizer.vocabulary_.get('is this')\n",
    "feature_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 0, 0, 1])"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "X_2[:, feature_index]  # 只在最后一行出现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从出现次数到出现频率\n",
    "出现次数的统计是非常好的开始，但是有个问题：长的文本相对于短的文本有更高的单词平均出现次数，尽管他们可能在描述同一个主题。\n",
    "\n",
    "为了避免这些潜在的差异，只需将各文档中每个单词的出现次数除以该文档中所有单词的总数：这些新的特征称之为词频 tf (Term Frequencies)。\n",
    "\n",
    "另一个在词频的基础上改良是，降低在该训练文集中的很多文档中均出现的单词的权重，从而突出那些仅在该训练文集中在一小部分文档中出现的单词的信息量。\n",
    "\n",
    "这种方法称为 tf–idf ，全称为 “Term Frequency times Inverse Document Frequency” 。\n",
    "\n",
    "tf 和 tf–idf 都可以按照下面的方式计算:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<4x9 sparse matrix of type '<class 'numpy.float64'>'\n\twith 19 stored elements in Compressed Sparse Row format>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "X_tfidf  = TfidfTransformer().fit_transform(X)  # 通过tf-idf转换器(transformer), 将向量化（vectorizer）的矩阵转为tfidf矩阵\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,\n        0.        , 0.35872874, 0.        , 0.43877674],\n       [0.        , 0.27230147, 0.        , 0.27230147, 0.        ,\n        0.85322574, 0.22262429, 0.        , 0.27230147],\n       [0.55280532, 0.        , 0.        , 0.        , 0.55280532,\n        0.        , 0.28847675, 0.55280532, 0.        ],\n       [0.        , 0.43877674, 0.54197657, 0.43877674, 0.        ,\n        0.        , 0.35872874, 0.        , 0.43877674]])"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "X_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练分类器\n",
    "###  构建Pipeline (管道)\n",
    "为了使得 向量化（vectorizer） => 转换器（transformer） => 分类器（classifier） 过程更加简单,scikit-learn 提供了一个 Pipeline 类，操作起来像一个复合分类器.\n",
    "> Pipeline: 链式评估器<br></br>\n",
    "> Pipeline 可以把多个评估器链接成一个。这个是很有用的，因为处理数据的步骤一般都是固定的，例如特征选择、标准化和分类。Pipeline 在这里有多种用途:\n",
    "> * **便捷性和封装性** 你只要对数据调用 fit和 predict 一次来适配所有的一系列评估器。\n",
    "> * **联合的参数选择** 你可以一次grid search管道中所有评估器的参数。\n",
    "> * **安全性** 训练转换器和预测器使用的是相同样本，管道有助于防止来自测试数据的统计数据泄露到交叉验证的训练模型中。<br></br>\n",
    "> 管道中的所有评估器，除了最后一个评估器，管道的所有评估器必须是转换器。 (例如，必须有 `transform` 方法). 最后一个评估器的类型不限（转换器、分类器等等）\n",
    "\n",
    "Pipeline 使用一系列 (key, value) 键值对来构建,其中 key 是你给这个步骤起的名字， value 是一个评估器对象:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HashingVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_clf = Pipeline([('count', CountVectorizer()), ('clf', MultinomialNB())])\n",
    "hash_clf = Pipeline([('hash', HashingVectorizer(binary=True)), ('clf', MultinomialNB())])\n",
    "tfidf_clf = Pipeline([('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])\n",
    "#  TfidfVectorizer ，它将 CountVectorizer 和 TfidfTransformer 的所有选项组合在一个单例模型中:\n",
    "count_tfidf_clf = Pipeline([('count', CountVectorizer()), ('transformer', TfidfTransformer()), ('clf', MultinomialNB())]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7728359001593202"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "count_clf.fit(news_train.data,  news_train.target)\n",
    "count_clf.score(news_test.data, news_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.6942379182156134"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "hash_clf.fit(news_train.data,  news_train.target)\n",
    "hash_clf.score(news_test.data, news_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7738980350504514"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "tfidf_clf.fit(news_train.data,  news_train.target)\n",
    "tfidf_clf.score(news_test.data, news_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7738980350504514"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "count_tfidf_clf.fit(news_train.data, news_train.target)\n",
    "count_tfidf_clf.score(news_test.data, news_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold cross validation(k折交叉验证)\n",
    "将数据集分为k个大小相同, 互不相交的子集, 每次使用k-1个子集训练模型, 用剩下的子集测试模型. 对k个不同的选择进行测试, 选取最佳模型.\n",
    "使用交叉验证最简单的方法是在估计器和数据集上调用` cross_val_score `辅助函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "def evaluate_cross_validation(clf, X, y, k=5):\n",
    "    scores = cross_val_score(clf, X, y, cv=k)\n",
    "    print(scores)\n",
    "    print(f'mean socre: {np.mean(scores):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.84091913 0.84047724 0.82810429 0.83208131 0.83996463]\nmean socre: 0.836\n"
    }
   ],
   "source": [
    "evaluate_cross_validation(count_clf, news_train.data, news_train.target, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.75430844 0.7410517  0.75872735 0.75475033 0.7617153 ]\nmean socre: 0.754\n"
    }
   ],
   "source": [
    "evaluate_cross_validation(hash_clf, news_train.data, news_train.target, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.84887318 0.84180292 0.84401237 0.84136103 0.84394341]\nmean socre: 0.844\n"
    }
   ],
   "source": [
    "evaluate_cross_validation(tfidf_clf, news_train.data, news_train.target, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CountVectorizer`和`TfidfVectorizer`具有相似的表现，并且比`HashingVectorizer`好得多。  \n",
    "继续使用`TfidfVectorizer`, 可以尝试通过尝试将文本文档 解析为具有不同**正则表达式**的标记来改进结果.\n",
    "\n",
    "默认正则表达式选择2个或更多字母数字字符的标记（标点符号被完全忽略，始终视为标记分隔符）:`r\"\\b\\w\\w+\\b\"`,也许还考虑斜线和点可以改善分词, 如:`wi-Fi`, `site.com`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_regex = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(token_pattern=r'\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b')),\n",
    "    ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.86212992 0.85550155 0.86257181 0.85550155 0.86958444]\nmean socre: 0.861\n"
    }
   ],
   "source": [
    "evaluate_cross_validation(clf_regex, news_train.data, news_train.target, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用**stop words**\n",
    "内置的英文停止词列表: sklearn\\feature_extraction\\stop_words.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english\n",
    "clf_stop_words = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(token_pattern=r'\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b', stop_words='english')),\n",
    "    ('clf', MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.8882015  0.88908529 0.89085285 0.8833407  0.88815208]\nmean socre: 0.888\n"
    }
   ],
   "source": [
    "evaluate_cross_validation(clf_stop_words, news_train.data, news_train.target, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_alpha = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(token_pattern=r'\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b', stop_words='english')),\n",
    "    ('clf', MultinomialNB(alpha=0.01))])  # 调整 alpha  1.0 ->0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.91559876 0.92001768 0.9138312  0.91515687 0.91865606]\nmean socre: 0.917\n"
    }
   ],
   "source": [
    "evaluate_cross_validation(clf_alpha, news_train.data, news_train.target, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def train_and_evaluate(clf, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f'训练集score: {clf.score(X_train, y_train)}')\n",
    "    print(f'测试集score: {clf.score(X_test, y_test)}')\n",
    "    # 查看主要的分类报告（包含每个类的精确率和召回率）\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    # 混淆矩阵\n",
    "    print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "训练集score: 0.9977019621707619\n测试集score: 0.8469198088157196\n              precision    recall  f1-score   support\n\n           0       0.84      0.78      0.81       319\n           1       0.76      0.76      0.76       389\n           2       0.80      0.63      0.70       394\n           3       0.66      0.77      0.71       392\n           4       0.83      0.86      0.84       385\n           5       0.86      0.83      0.85       395\n           6       0.83      0.79      0.81       390\n           7       0.90      0.90      0.90       396\n           8       0.95      0.96      0.96       398\n           9       0.96      0.95      0.96       397\n          10       0.95      0.98      0.96       399\n          11       0.87      0.94      0.90       396\n          12       0.80      0.78      0.79       393\n          13       0.90      0.86      0.88       396\n          14       0.88      0.92      0.90       394\n          15       0.82      0.95      0.88       398\n          16       0.76      0.92      0.83       364\n          17       0.97      0.95      0.96       376\n          18       0.81      0.65      0.72       310\n          19       0.76      0.62      0.69       251\n\n    accuracy                           0.85      7532\n   macro avg       0.85      0.84      0.84      7532\nweighted avg       0.85      0.85      0.85      7532\n\n[[250   0   0   3   0   1   0   0   1   2   0   2   0   5   3  24   4   4\n    1  19]\n [  1 297  11  15  10  17   6   2   1   3   0   7   9   0   5   1   1   2\n    1   0]\n [  0  26 247  48   4  23   4   0   0   1   1  14   4   4   4   5   2   0\n    4   3]\n [  0   7  21 301  21   2  12   0   1   0   1   2  23   0   1   0   0   0\n    0   0]\n [  0   4   7  15 330   3   8   3   0   1   1   1  10   0   2   0   0   0\n    0   0]\n [  0  27   7   9   7 328   2   0   1   2   0   5   0   0   4   0   2   1\n    0   0]\n [  0   3   4  29   8   0 309  15   4   0   4   0   8   2   2   1   1   0\n    0   0]\n [  0   1   1   5   1   0  10 358   7   1   0   0   7   1   1   0   1   0\n    2   0]\n [  0   0   0   1   1   0   3   8 382   0   0   0   2   1   0   0   0   0\n    0   0]\n [  0   0   0   0   1   0   2   1   1 379  10   0   0   1   1   1   0   0\n    0   0]\n [  0   0   0   0   1   0   0   0   0   3 390   1   0   1   0   1   1   0\n    1   0]\n [  1   2   0   1   2   1   2   2   0   0   1 371   3   4   0   0   6   0\n    0   0]\n [  1   5  10  25   8   0   8   4   3   0   0  10 306   6   5   0   0   1\n    1   0]\n [  1   6   0   2   0   2   5   4   0   2   0   2   8 339   3   8   3   2\n    7   2]\n [  1   6   0   1   1   3   1   0   0   0   0   2   4   4 363   2   0   1\n    5   0]\n [  3   2   1   1   0   0   0   0   0   1   1   0   0   2   2 378   0   0\n    1   6]\n [  0   0   0   1   0   0   1   0   1   0   0   4   0   2   1   1 336   0\n   11   6]\n [  4   1   0   0   1   0   0   0   0   1   0   0   0   0   0   3   0 359\n    7   0]\n [  3   1   0   0   2   0   0   1   0   0   1   4   0   2   9   1  74   0\n  200  12]\n [ 31   1   0   0   0   0   0   0   0   0   0   0   0   2   5  35  14   2\n    5 156]]\n"
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(news.data, news.target, test_size=0.2)\n",
    "\n",
    "train_and_evaluate(clf_alpha, news_train.data, news_test.data, news_train.target, news_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "133348"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "# 查看向量化器，我们可以看到哪些标记已用于创建我们的字典\n",
    "len(clf_alpha.named_steps['tfidf'].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['zzr1100',\n 'zzrk',\n 'zzt',\n 'zztop.dps.co.uk',\n 'zzy_3w',\n 'zzz',\n 'zzzoh',\n 'zzzz',\n 'zzzzzz',\n 'zzzzzzt']"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "clf_alpha.named_steps['tfidf'].get_feature_names()[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}