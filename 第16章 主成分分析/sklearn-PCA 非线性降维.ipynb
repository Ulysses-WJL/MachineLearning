{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iris 数据集上的PCA降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = [(\"standard\", StandardScaler()), ( 'reduce_dim', PCA(n_components=2))]\n",
    "pipe = Pipeline(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new = pipe.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "for color , i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n",
    "    plt.scatter(data_new[y==i, 0], data_new[y==i, 1], color=color, label=target_name)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "np.random.seed(5)\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "fig = plt.figure(1, figsize=(12, 9))\n",
    "plt.clf()\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
    "\n",
    "plt.cla()\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X)\n",
    "X = pca.transform(X)\n",
    "for name, label in [('Setosa', 0), ('Versicolour', 1), ('Virginica', 2)]:\n",
    "    ax.text3D(X[y == label, 0].mean(),\n",
    "                X[y == label, 1].mean() + 1.5,\n",
    "                X[y == label, 2].mean(), name,\n",
    "                horizontalalignment='center',\n",
    "                bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
    "# Reorder the labels to have colors matching the cluster results\n",
    "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap=plt.cm.nipy_spectral,\n",
    "            edgecolor='k')\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 手写数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印图片信息\n",
    "def print_digits(images, y, max_n=10):\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1,\n",
    "                       hspace=0.05, wspace=0.005)\n",
    "    i = 0\n",
    "    while i < max_n and i < images.shape[0]:\n",
    "        p = fig.add_subplot(20, 20, i+1, xticks=[], yticks=[])\n",
    "        p.imshow(images[i], cmap=plt.cm.bone)\n",
    "        p.text(0, 14, str(y[i]))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_digits(digits.images, digits.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()  # 64维降到2维\n",
    "X_pca = pca.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一、二主成分 在特征空间的表示\n",
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 个数字10种颜色, 不同类别是否明显可分 \n",
    "def plot_pca_scatter(X_pca):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    colors = ['black', 'blue', 'purple', 'yellow', 'green',\n",
    "              'red', 'lime', 'cyan', 'orange', 'gray']\n",
    "    for i in range(10):\n",
    "        plt.scatter(X_pca[digits.target==i, 0], X_pca[digits.target==i, 1],\n",
    "                    color=colors[i], edgecolors='k')\n",
    "    plt.legend(digits.target_names)\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Compoent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_scatter(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分析结果:\n",
    "- 对于大多数类，它们的实例根据其目标类清楚地分组，并且簇相对不同。例外是对应于数字 5 的类，其中实例非常稀疏地分布在平面上与其他类重叠\n",
    "- 在另一个极端，对应于数字 0 的类是最可分离的簇。直观地说，这个类可能是最容易与其他类分开的类；也就是说，如果我们训练一个分类器，它应该是具有最佳评估数字的类。\n",
    "- 此外，对于拓扑分布，我们可以预测相邻类对应于相似的数字，这意味着它们将是最难分离的。例如，对应于数字 9 和 3 的簇看起来是相邻的（由于它们的图形表示是相似的，因此可以预计），因此，从 3 分离 9 可能比从 3 分离 4 更难，它位于左侧，远离这些簇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pca_components(images, n_cols, n_rows):\n",
    "    plt.figure(figsize=(2 * n_cols, 2.26 * n_rows))\n",
    "    for i, comp in enumerate(images):\n",
    "        plt.subplot(n_rows, n_cols, i+1)\n",
    "        plt.imshow(comp.reshape(8, 8), interpolation='nearest')\n",
    "        plt.text(0, -1, str(i+1) + '-component')\n",
    "        plt.xticks()\n",
    "        plt.yticks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_pca_components(pca.components_[:10], 5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过查看上图中的前两个成分，我们可以得出一些有趣的观察结果：\n",
    "\n",
    "- 如果查看第二个成分，可以看到它主要突出显示图像的中心区域。受此模式影响最大的digit类是 0，因为其中心区域为空。通过查看我们之前的散点图来证实这种直觉。如果查看对应于数字 0 的簇，您可以看到它是第二个成分具有较低值的簇。\n",
    "- 关于第一个成分，正如我们在散点图中看到的那样，可用于分离对应于数字 4（极右，高值）和 3 （极左，低值）数字的簇。如果你看到第一个成分图，它证实了这个观察结果。您可以看到区域非常相似于数字 3 ，而在数字 4 的特征区域中，它也拥有颜色。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 增量PCA(Incremental PCA)\n",
    "\n",
    "PCA 对象非常有用, 但 针对大型数据集的应用, 仍然具有一定的限制。 最大的限制是 PCA 仅支持批处理，这意味着所有要处理的数据必须放在主内存。 IncrementalPCA 对象使用不同的处理形式, 即允许部分计算以小型批处理方式处理数据的方法进行, 而得到和 PCA 算法差不多的结果。 IncrementalPCA 可以通过以下方式实现核外（out-of-core）主成分分析：\n",
    "\n",
    "- 基于从本地硬盘或网络数据库中连续获取的数据块之上, 使用 partial_fit 方法。\n",
    "- 在 memory mapped file (通过 numpy.memmap 创建)上使用 fit 方法。\n",
    "\n",
    "IncrementalPCA 类为了增量式的更新 explained_variance_ratio_ ，仅需要存储估计出的分量和噪声方差。 这就是为什么内存使用量依赖于每个批次的样本数量，而不是数据集中需要处理的样本总量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "# Authors: Kyle Kastner\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "n_components = 2\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=10)\n",
    "X_ipca = ipca.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "\n",
    "for X_transformed, title in [(X_ipca, \"Incremental PCA\"), (X_pca, \"PCA\")]:\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n",
    "        plt.scatter(X_transformed[y == i, 0], X_transformed[y == i, 1],\n",
    "                    color=color, lw=2, label=target_name)\n",
    "        if \"Incremental\" in title:\n",
    "            err = np.abs(np.abs(X_pca) - np.abs(X_ipca)).mean()\n",
    "            plt.title(title + \" of iris dataset\\nMean absolute unsigned error \"\n",
    "            \"%.6f\" % err)\n",
    "        else:\n",
    "            plt.title(title + \" of iris dataset\")\n",
    "plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "plt.axis([-4, 4, -1.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于随机化SVD的PCA\n",
    "通过丢弃具有较低奇异值的奇异向量的分量，将数据降维到低维空间并保留大部分方差信息是非常有意义的。\n",
    "\n",
    "例如，如果我们使用64x64像素的灰度级图像进行人脸识别，数据的维数为4096，在这样大的数据上训练含RBF内核的支持向量机是很慢的。此外我们知道这些数据固有维度远低于4096，因为人脸的所有照片都看起来有点相似。样本位于较低维度的流体上（例如约200维）。 PCA算法可以用于线性变换数据，同时降低维数并同时保留大部分可描述的方差信息。\n",
    "\n",
    "在这种情况下，使用可选参数 svd_solver='randomized' 的 PCA 是非常有用的。既然我们将要丢弃大部分奇异值，那么仅仅就实际转换中所需的奇异向量进行计算就可以使得 PCA 计算过程变得异常有效\n",
    "\n",
    "---\n",
    "**Faces dataset decompositions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel PCA\n",
    "KernelPCA 是 PCA 的扩展，通过使用核方法实现`非线性降维`（dimensionality reduction） (参阅 成对的矩阵, 类别和核函数)。 它具有许多应用，包括去噪, 压缩和结构化预测（ structured prediction ） (kernel dependency estimation（内核依赖估计）)。 KernelPCA 支持 transform 和 inverse_transform 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "# Authors: Mathieu Blondel\n",
    "# Andreas Mueller\n",
    "# License: BSD 3 clause\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X, y = make_circles(n_samples=400, factor=.3, noise=.05)\n",
    "\n",
    "# y = \\phi(x) 高斯核 不再是线性变换\n",
    "kpca = KernelPCA(kernel=\"rbf\", fit_inverse_transform=True, gamma=10)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "# 降维后的数据重新变换回来\n",
    "X_back = kpca.inverse_transform(X_kpca)\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.subplot(2, 2, 1, aspect='equal')\n",
    "\n",
    "plt.title(\"Original space\")\n",
    "reds = y == 0  # 外圆\n",
    "blues = y == 1  # 内圆\n",
    "plt.scatter(X[reds, 0], X[reds, 1], c=\"red\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.scatter(X[blues, 0], X[blues, 1], c=\"blue\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "X1, X2 = np.meshgrid(np.linspace(-1.5, 1.5, 50), np.linspace(-1.5, 1.5, 50))\n",
    "X_grid = np.array([np.ravel(X1), np.ravel(X2)]).T\n",
    "# projection on the first principal component (in the phi space)\n",
    "Z_grid = kpca.transform(X_grid)[:, 0].reshape(X1.shape)\n",
    "plt.contour(X1, X2, Z_grid, colors='grey', linewidths=1, origin='lower')\n",
    "\n",
    "plt.subplot(2, 2, 2, aspect='equal')\n",
    "plt.scatter(X_pca[reds, 0], X_pca[reds, 1], c=\"red\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.scatter(X_pca[blues, 0], X_pca[blues, 1], c=\"blue\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.title(\"Projection by PCA\")\n",
    "plt.xlabel(\"1st principal component\")\n",
    "plt.ylabel(\"2nd component\")\n",
    "\n",
    "plt.subplot(2, 2, 3, aspect='equal')\n",
    "plt.scatter(X_kpca[reds, 0], X_kpca[reds, 1], c=\"red\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.scatter(X_kpca[blues, 0], X_kpca[blues, 1], c=\"blue\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.title(\"Projection by KPCA\")\n",
    "plt.xlabel(r\"1st principal component in space induced by $\\phi$\")\n",
    "plt.ylabel(\"2nd component\")\n",
    "\n",
    "plt.subplot(2, 2, 4, aspect='equal')\n",
    "plt.scatter(X_back[reds, 0], X_back[reds, 1], c=\"red\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.scatter(X_back[blues, 0], X_back[blues, 1], c=\"blue\",\n",
    "            s=20, edgecolor='k')\n",
    "plt.title(\"Original space after inverse transform\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
