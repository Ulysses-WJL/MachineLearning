{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经元(neuron)模型\n",
    "\n",
    "\n",
    "![MP](../img/M-P神经元模型.png)\n",
    "\n",
    "在这个模型中, 神经元接收到来自n个其它神经元传递过来的输入信号, 这些输入信号通过带权重的连接(connection)进行传递, 神经元接受到的总输入值将与神经元的阈值进行比较, 然后通过`激活函数`(activation function)或`响应函数`处理以产生神经元的输出.\n",
    "\n",
    "![激活](../img/激活函数.png)\n",
    "\n",
    "把这样多个神经元按一定的层次结构连接起来, 就得到了`神经网络`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前向传播算法Forward Propagation\n",
    "\n",
    "下图为一个 3 层的神经网络，第一层成为输入层（**Input Layer**），最后一\n",
    "层称为输出层（**Output Layer**），中间一层成为隐藏层（**Hidden Layers**）。我们为每一层都增\n",
    "加一个偏差单位（**bias unit**）：\n",
    "![神经网络模型](../img/神经网络模型.png)\n",
    "\n",
    "$a_{i}^{(j)}$代表第𝑗 层的第 𝑖 个激活单元。$\\theta^{(j)}$代表从第 𝑗 层映射到第𝑗 + 1 层时的权重的矩阵\n",
    "对于上图所示的模型，激活单元和输出分别表达为:\n",
    "$$a_{1}^{(2)}=g(\\Theta _{10}^{(1)}{{x}_{0}}+\\Theta _{11}^{(1)}{{x}_{1}}+\\Theta _{12}^{(1)}{{x}_{2}}+\\Theta _{13}^{(1)}{{x}_{3}})$$\n",
    "$$a_{2}^{(2)}=g(\\Theta _{20}^{(1)}{{x}_{0}}+\\Theta _{21}^{(1)}{{x}_{1}}+\\Theta _{22}^{(1)}{{x}_{2}}+\\Theta _{23}^{(1)}{{x}_{3}})$$\n",
    "$$a_{3}^{(2)}=g(\\Theta _{30}^{(1)}{{x}_{0}}+\\Theta _{31}^{(1)}{{x}_{1}}+\\Theta _{32}^{(1)}{{x}_{2}}+\\Theta _{33}^{(1)}{{x}_{3}})$$\n",
    "$${{h}_{\\Theta }}(x)=g(\\Theta _{10}^{(2)}a_{0}^{(2)}+\\Theta _{11}^{(2)}a_{1}^{(2)}+\\Theta _{12}^{(2)}a_{2}^{(2)}+\\Theta _{13}^{(2)}a_{3}^{(2)})$$\n",
    "\n",
    "每一个$a$都是由上一层所有的$x$和每一个$x$所对应的决定的\n",
    "\n",
    "我们把这样从左到右的算法称为前向传播算法( **FORWARD PROPAGATION** )\n",
    "把$x$, $\\theta$, $a$ 分别用矩阵表示：\n",
    "$$X = \\begin{bmatrix}x_0 \\\\ x_1\\\\x_2\\\\x_3\\end{bmatrix}, \n",
    "\\theta = \\begin{bmatrix}\\theta_{10} & ... & ... &... \\\\ \n",
    "... & ... & ... &... \\\\\n",
    "... & ... & ...& \\theta_{33}\n",
    "\\end{bmatrix},\n",
    "a = \\begin{bmatrix}a_1 \\\\ a_2\\\\a_3\\end{bmatrix}\n",
    "$$\n",
    "可以得到\n",
    "$\\theta \\cdot X = a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上面的神经网络为例，试着计算第二层的值:\n",
    "$$\n",
    "X = \\begin{bmatrix}x_0 \\\\ x_1\\\\x_2\\\\x_3\\end{bmatrix},\n",
    "z^{(2)}=\\begin{bmatrix}z_1\\\\z_2\\\\z_3\\end{bmatrix} \\\\\n",
    "z^{(2)} = \\Theta^{(1)}x\\\\\n",
    "a^{(2)} = g(z^{(2)})\n",
    "$$\n",
    "$$g\\left(\\begin{bmatrix}\\theta_{10}^{(1)} & \\theta_{11}^{(1)} & \\theta_{12}^{(1)}& \\theta_{13}^{(1)} \\\\ \n",
    "\\theta_{20}^{(1)} & \\theta_{21}^{(1)} & \\theta_{22}^{(1)}& \\theta_{23}^{(1)} \\\\ \n",
    "\\theta_{30}^{(1)} & \\theta_{31}^{(1)} & \\theta_{32}^{(1)}& \\theta_{33}^{(1)} \\\\ \n",
    "\\end{bmatrix} \n",
    "\\times  \\begin{bmatrix}x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\right)  = g\\left(\\begin{bmatrix} \\theta_{10}^{(1)}{{x}_{0}}+\\theta_{11}^{(1)}{{x}_{1}}+\\theta_{12}^{(1)}{{x}_{2}}+\\theta_{13}^{(1)}{{x}_{3}}\\end{bmatrix}\\right)\n",
    " = \\begin{bmatrix}a_1^{(2)} \\\\ a_2^{(2)} \\\\ a_3^{(2)} \\end{bmatrix}$$\n",
    " \n",
    "  我们令 ${{z}^{\\left( 2 \\right)}}={{\\theta }^{\\left( 1 \\right)}}x$，则 ${{a}^{\\left( 2 \\right)}}=g({{z}^{\\left( 2 \\right)}})$ ，计算后添加 $a_{0}^{\\left( 2 \\right)}=1$。 计算输出的值为：\n",
    "$$\n",
    "g\\left(\\begin{bmatrix}\\theta_{10}^{(2)}& \\theta_{10}^{(2)} & \\theta_{10}^{(2)} & \\theta_{10}^{(2)} \\end{bmatrix} \n",
    "\\times \\begin{bmatrix} a_0^{(2)}\\\\ a_1^{(2)} \\\\ a_2^{(2)} \\\\ a_3^{(2)}\\end{bmatrix}\n",
    "\\right) = g(\\theta^{(2)}_{10}a^{(2)}_{0} + \\theta^{(2)}_{11}a^{(2)}_{1} + \\theta^{(2)}_{12}a^{(2)}_{2} + \\theta^{(2)}_{13}a^{(2)}_{3} ) = h_{\\theta}(x)\n",
    "$$\n",
    "\n",
    "我们令 ${{z}^{\\left( 3 \\right)}}={{\\theta }^{\\left( 2 \\right)}}{{a}^{\\left( 2 \\right)}}$，则 $h_\\theta(x)={{a}^{\\left( 3 \\right)}}=g({{z}^{\\left( 3 \\right)}})$。\n",
    "\n",
    "这只是针对训练集中一个训练实例所进行的计算。如果我们要对整个训练集进行计算，我们需要将训练集特征矩阵进行转置，使得同一个实例的特征都在同一列里。即：\n",
    "${{z}^{\\left( 2 \\right)}}={{\\Theta }^{\\left( 1 \\right)}}\\times {{X}^{T}} $\n",
    "\n",
    " ${{a}^{\\left( 2 \\right)}}=g({{z}^{\\left( 2 \\right)}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知机与多层网络\n",
    "\n",
    "感知机(Perceptron)由两层神经元组成, 输入层接受外界输入信号后传递给输出层, 输出层是M-P神经元, 亦称\"阈值逻辑单元\"(threshold logic unit).\n",
    "\n",
    "![感知机](../img/感知机.png)\n",
    "感知机能容易地实现逻辑与、或、非运算. 令$y = f(\\sum_i w_ix_i - \\theta)$, f为阶跃函数\n",
    "- \"与\", 令$w_1=w_2=1, \\theta=0.5$, 则$y=f(1\\cdot x_1 + 1\\cdot x_2 -0.5)在x_1=x_2=1时, y=1;$\n",
    "- \"或\", 令$w_1=w_2=1, \\theta=-0.5$, 则$y=f(1\\cdot x_1+ 1 \\cdot x_2 + 0.5), 当x_1=1或x_2=1时, y=1;$\n",
    "- \"非\", 令$w_1=-0.6, w_2=0,  \\theta=-0.5, 则y=f(-0.6\\cdot x_1 + 0 \\cdot x_2 +0.5), 当x_1=1时, y=0;当x_1=0时, y=1$\n",
    "\n",
    "感知机只有输出层神经单元进行激活函数处理, 即只有一层功能神经元(functional neuron), 其学习能力非常有限. 上述与、或、非问题都是线性可分(linearly separable)的.要解决非线性可分问题需要使用多层功能神经元.\n",
    "\n",
    "更一般地, 常见神经网络是层级结构,每层神经元与下一层神经元全互连, 神经元之间不存在同层连接, 也不存在跨层连接. 这样的神经网络结构通常称为\"多层前馈神经网络\"(multi-layer feedforward neural networks). 输入层只接受输入, 隐层与输出层神经元对信号进行处理.\n",
    "![多层](../img/多层神经网络.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络结构的代价函数\n",
    "\n",
    "假设神经网络的训练样本有$m$个，每个包含一组输入$x$和一组输出信号$y$，$L$表示神经网络层数，$S_I$表示每层的**neuron**个数($S_l$表示输出层神经元个数)，$S_L$代表最后一层中处理单元的个数。\n",
    "\n",
    "将神经网络的分类定义为两种情况：二类分类和多类分类，\n",
    "\n",
    "二类分类：$S_L=0, y=0\\, or\\, 1$表示哪一类；\n",
    "\n",
    "$K$类分类：$S_L=k, y_i = 1$表示分到第$i$类；$(k>2)$\n",
    "\n",
    "![](../img/神经网络多类分类.jpg)\n",
    "\n",
    "我们回顾逻辑回归问题中我们的代价函数为：\n",
    "\n",
    "$  J\\left(\\theta \\right)=-\\frac{1}{m}\\left[\\sum_\\limits{i=1}^{m}{y}^{(i)}\\log{h_\\theta({x}^{(i)})}+\\left(1-{y}^{(i)}\\right)log\\left(1-h_\\theta\\left({x}^{(i)}\\right)\\right)\\right]+\\frac{\\lambda}{2m}\\sum_\\limits{j=1}^{n}{\\theta_j}^{2}  $\n",
    "\n",
    "在逻辑回归中，我们只有一个输出变量，又称标量（**scalar**），也只有一个因变量$y$，但是在神经网络中，我们可以有很多输出变量，我们的$h_\\theta(x)$是一个维度为$K$的向量，并且我们训练集中的因变量也是同样维度的一个向量，因此我们的代价函数会比逻辑回归更加复杂一些，为：$\\newcommand{\\subk}[1]{ #1_k }$\n",
    "$$h_\\theta\\left(x\\right)\\in \\mathbb{R}^{K}$$ $${\\left({h_\\theta}\\left(x\\right)\\right)}_{i}={i}^{th} \\text{output}$$\n",
    "\n",
    "$J(\\Theta) = -\\frac{1}{m} \\left[ \\sum\\limits_{i=1}^{m} \\sum\\limits_{k=1}^{k} {y_k}^{(i)} \\log \\subk{(h_\\Theta(x^{(i)}))} + \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1- \\subk{\\left( h_\\Theta \\left( x^{(i)} \\right) \\right)} \\right) \\right] + \\frac{\\lambda}{2m} \\sum\\limits_{l=1}^{L-1} \\sum\\limits_{i=1}^{s_l} \\sum\\limits_{j=1}^{s_{l+1}} \\left( \\Theta_{ji}^{(l)} \\right)^2$\n",
    "\n",
    "这个看起来复杂很多的代价函数背后的思想还是一样的，我们希望通过代价函数来观察算法预测的结果与真实情况的误差有多大，唯一不同的是，对于每一行特征，我们都会给出$K$个预测，基本上我们可以利用循环，对每一行特征都预测$K$个不同结果，然后在利用循环在$K$个预测中选择可能性最高的一个，将其与$y$中的实际数据进行比较。\n",
    "\n",
    "正则化的那一项只是排除了每一层$\\theta_0$后，每一层的$\\theta$ 矩阵的和。最里层的循环$j$循环所有的行（由$s_{l+1}$  层的激活单元数决定），循环$i$则循环所有的列，由该层（$s_l$层）的激活单元数所决定。即：$h_\\theta(x)$与真实值之间的距离为每个样本-每个类输出的加和，对参数进行**regularization**的**bias**项处理所有参数的平方和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播算法(逆误差传播算法)Backpropagation Algorithm \n",
    "\n",
    "反向传播(BP)算法是迄今最成功的神经网络学习算法. 现实任务中使用神经网络时, 大多是在使用BP算法进行训练.\n",
    "\n",
    "之前我们在计算神经网络预测结果的时候我们采用了一种正向传播方法，我们从第一层开始正向一层一层进行计算，直到最后一层的$h_{\\theta}\\left(x\\right)$。\n",
    "\n",
    "现在，为了计算代价函数的偏导数$\\frac{\\partial}{\\partial\\Theta^{(l)}_{ij}}J\\left(\\Theta\\right)$，我们需要采用一种反向传播算法，也就是首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。\n",
    "以一个例子来说明反向传播算法。\n",
    "\n",
    "假设我们的训练集只有一个样本$\\left({x}^{(1)},{y}^{(1)}\\right)$，我们的神经网络是一个四层的神经网络，其中$K=4，S_{L}=4，L=4$：\n",
    "\n",
    "前向传播算法：\n",
    "![](../img/前向传播.png)\n",
    "\n",
    "我们从最后一层的误差开始计算，误差是激活单元的预测（${a^{(4)}}$）与实际值（$y^k$）之间的误差，（$k=1:k$）。  \n",
    "我们用$\\delta$来表示误差(该残差表明了该节点对最终输出值的残差产生了多少影响)，则：$\\delta^{(4)}=a^{(4)}-y$  \n",
    "我们利用这个误差值来计算前一层的误差：$\\delta^{(3)}=\\left({\\Theta^{(3)}}\\right)^{T}\\delta^{(4)}\\ast g'\\left(z^{(3)}\\right)$  \n",
    "其中 $g'(z^{(3)})$是 $S$ 形函数的导数，$g'(z^{(3)})=a^{(3)}\\ast(1-a^{(3)})$。而$(θ^{(3)})^{T}\\delta^{(4)}$则是权重导致的误差的和。下一步是继续计算第二层的误差：  \n",
    "$ \\delta^{(2)}=(\\Theta^{(2)})^{T}\\delta^{(3)}\\ast g'(z^{(2)})$  \n",
    "因为第一层是输入变量，不存在误差。我们有了所有的误差的表达式后，便可以计算代价函数的偏导数了，假设$λ=0$，即我们不做任何正则化处理时有：  \n",
    "$\\frac{\\partial}{\\partial\\Theta_{ij}^{(l)}}J(\\Theta)=a_{j}^{(l)} \\delta_{i}^{l+1}$\n",
    "\n",
    "重要的是清楚地知道上面式子中上下标的含义：\n",
    "\n",
    "- $l$ 代表目前所计算的是第几层。\n",
    "\n",
    "- $j$ 代表目前计算层中的激活单元的下标，也将是下一层的第$j$个输入变量的下标。\n",
    "\n",
    "- $i$ 代表下一层中误差单元的下标，是受到权重矩阵中第$i$行影响的下一层中的误差单元的下标。\n",
    "\n",
    "如果我们考虑正则化处理，并且我们的训练集是一个特征矩阵而非向量。在上面的特殊情况中，我们需要计算每一层的误差单元来计算代价函数的偏导数。在更为一般的情况中，我们同样需要计算每一层的误差单元，但是我们需要为整个训练集计算误差单元，此时的误差单元也是一个矩阵，我们用$\\Delta^{(l)}_{ij}$来表示这个误差矩阵。第 $l$  层的第 $i$ 个激活单元受到第 $j$ 个参数影响而导致的误差。\n",
    "\n",
    "我们的算法表示为：\n",
    "\n",
    "![](../img/BP算法代码.jpg)\n",
    "\n",
    "即首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。\n",
    "\n",
    "在求出了$\\Delta_{ij}^{(l)}$之后，我们便可以计算代价函数的偏导数了，计算方法如下：\n",
    "\n",
    "$ D_{ij}^{(l)} :=\\frac{1}{m}\\Delta_{ij}^{(l)}+\\lambda\\Theta_{ij}^{(l)}$              ${if}\\; j \\neq  0$\n",
    "\n",
    "$ D_{ij}^{(l)} :=\\frac{1}{m}\\Delta_{ij}^{(l)}$                             ${if}\\; j = 0$\n",
    "\n",
    "详细推导过程: [神经网络--反向传播详细推导过程](https://blog.csdn.net/qq_29762941/article/details/80343185)\n",
    "\n",
    "![](../img/反向传播推导.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "训练神经网络：\n",
    "1. 参数的随机初始化\n",
    "2. 利用正向传播方法计算所有的$h_{\\Theta}(𝑥)$\n",
    "3. 编写计算代价函数$J(\\Theta)$的代码\n",
    "4. 利用反向传播方法计算所有偏导数$\\frac {\\partial}{\\partial \\Theta_{jk}^{(l)}}J(\\Theta)$\n",
    "5. 利用数值检验方法检验这些偏导数\n",
    "6. 使用优化算法来最小化代价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}